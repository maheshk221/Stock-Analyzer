{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network - Classification\n",
    "Using the top 10 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"top10_df.pkl\", \"rb\") as fp:\n",
    "    df = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "features_df = df.drop([\"Decision\"], 1)\n",
    "\n",
    "scaled_df = pd.DataFrame(scaler.fit_transform(features_df), \n",
    "                               index=features_df.index, \n",
    "                               columns=features_df.columns)\n",
    "\n",
    "df = scaled_df.join(df.Decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, test, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"Decision\"], 1)\n",
    "y = df.Decision\n",
    "\n",
    "# Train, test, split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_results(results):\n",
    "    \"\"\"\n",
    "    Plots the loss and accuracy for the training and testing data\n",
    "    \"\"\"\n",
    "    history = results.history\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.plot(history['loss'])\n",
    "    plt.legend(['val_loss', 'loss'])\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.legend(['val_accuracy', 'accuracy'])\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling - NN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               1100      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 142,601\n",
      "Trainable params: 142,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instatiating the model\n",
    "model = Sequential()\n",
    "\n",
    "activ = \"relu\"\n",
    "# Input Layer\n",
    "model.add(Dense(100, activation=activ,input_shape=(X.shape[1],)))\n",
    "\n",
    "# Hidden Layers (11)\n",
    "model.add(Dense(100, activation=activ))\n",
    "model.add(Dense(100, activation=activ))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(100, activation=activ))\n",
    "model.add(Dense(100, activation=activ))\n",
    "model.add(Dense(100, activation=activ))\n",
    "model.add(Dense(100, activation=activ))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(100, activation=activ))\n",
    "model.add(Dense(100, activation=activ))\n",
    "model.add(Dense(100, activation=activ))\n",
    "model.add(Dense(100, activation=activ))\n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(100, activation=activ))\n",
    "model.add(Dense(100, activation=activ))\n",
    "model.add(Dense(100, activation=activ))\n",
    "model.add(Dense(100, activation=activ))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 20056 samples, validate on 6686 samples\n",
      "Epoch 1/2500\n",
      "20056/20056 [==============================] - 2s 110us/step - loss: 0.7457 - accuracy: 0.3337 - val_loss: 0.6453 - val_accuracy: 0.3358\n",
      "Epoch 2/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.6587 - accuracy: 0.3325 - val_loss: 0.6468 - val_accuracy: 0.3358\n",
      "Epoch 3/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.6525 - accuracy: 0.3325 - val_loss: 0.6580 - val_accuracy: 0.3358\n",
      "Epoch 4/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.6521 - accuracy: 0.3327 - val_loss: 0.6557 - val_accuracy: 0.3358\n",
      "Epoch 5/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.6479 - accuracy: 0.3326 - val_loss: 0.6401 - val_accuracy: 0.3358\n",
      "Epoch 6/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.6509 - accuracy: 0.3325 - val_loss: 0.6406 - val_accuracy: 0.3358\n",
      "Epoch 7/2500\n",
      "20056/20056 [==============================] - 1s 75us/step - loss: 0.6517 - accuracy: 0.3325 - val_loss: 0.6413 - val_accuracy: 0.3358\n",
      "Epoch 8/2500\n",
      "20056/20056 [==============================] - 2s 77us/step - loss: 0.6463 - accuracy: 0.3325 - val_loss: 0.6436 - val_accuracy: 0.3358\n",
      "Epoch 9/2500\n",
      "20056/20056 [==============================] - 2s 93us/step - loss: 0.6467 - accuracy: 0.3325 - val_loss: 0.6398 - val_accuracy: 0.3358\n",
      "Epoch 10/2500\n",
      "20056/20056 [==============================] - 2s 97us/step - loss: 0.6460 - accuracy: 0.3325 - val_loss: 0.6401 - val_accuracy: 0.3356\n",
      "Epoch 11/2500\n",
      "20056/20056 [==============================] - 2s 93us/step - loss: 0.6467 - accuracy: 0.3326 - val_loss: 0.6413 - val_accuracy: 0.3358\n",
      "Epoch 12/2500\n",
      "20056/20056 [==============================] - 2s 85us/step - loss: 0.6463 - accuracy: 0.3325 - val_loss: 0.6421 - val_accuracy: 0.3356\n",
      "Epoch 13/2500\n",
      "20056/20056 [==============================] - 2s 83us/step - loss: 0.6459 - accuracy: 0.3325 - val_loss: 0.6422 - val_accuracy: 0.3356\n",
      "Epoch 14/2500\n",
      "20056/20056 [==============================] - 2s 82us/step - loss: 0.6469 - accuracy: 0.3326 - val_loss: 0.6422 - val_accuracy: 0.3356\n",
      "Epoch 15/2500\n",
      "20056/20056 [==============================] - 2s 86us/step - loss: 0.6441 - accuracy: 0.3325 - val_loss: 0.6450 - val_accuracy: 0.3356\n",
      "Epoch 16/2500\n",
      "20056/20056 [==============================] - 2s 80us/step - loss: 0.6458 - accuracy: 0.3325 - val_loss: 0.6438 - val_accuracy: 0.3358\n",
      "Epoch 17/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.6433 - accuracy: 0.3326 - val_loss: 0.6388 - val_accuracy: 0.3356\n",
      "Epoch 18/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.6432 - accuracy: 0.3325 - val_loss: 0.6404 - val_accuracy: 0.3356\n",
      "Epoch 19/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.6448 - accuracy: 0.3325 - val_loss: 0.6447 - val_accuracy: 0.3358\n",
      "Epoch 20/2500\n",
      "20056/20056 [==============================] - 2s 77us/step - loss: 0.6437 - accuracy: 0.3325 - val_loss: 0.6409 - val_accuracy: 0.3358\n",
      "Epoch 21/2500\n",
      "20056/20056 [==============================] - 2s 79us/step - loss: 0.6435 - accuracy: 0.3325 - val_loss: 0.6431 - val_accuracy: 0.3358\n",
      "Epoch 22/2500\n",
      "20056/20056 [==============================] - ETA: 0s - loss: 0.6430 - accuracy: 0.33 - 2s 82us/step - loss: 0.6437 - accuracy: 0.3325 - val_loss: 0.6393 - val_accuracy: 0.3358\n",
      "Epoch 23/2500\n",
      "20056/20056 [==============================] - 2s 83us/step - loss: 0.6422 - accuracy: 0.3326 - val_loss: 0.6401 - val_accuracy: 0.3356\n",
      "Epoch 24/2500\n",
      "20056/20056 [==============================] - 2s 82us/step - loss: 0.6441 - accuracy: 0.3326 - val_loss: 0.6422 - val_accuracy: 0.3356\n",
      "Epoch 25/2500\n",
      "20056/20056 [==============================] - 2s 82us/step - loss: 0.6420 - accuracy: 0.3327 - val_loss: 0.6410 - val_accuracy: 0.3358\n",
      "Epoch 26/2500\n",
      "20056/20056 [==============================] - 2s 83us/step - loss: 0.6468 - accuracy: 0.3327 - val_loss: 0.6458 - val_accuracy: 0.3358\n",
      "Epoch 27/2500\n",
      "20056/20056 [==============================] - 2s 77us/step - loss: 0.6442 - accuracy: 0.3325 - val_loss: 0.6415 - val_accuracy: 0.3358\n",
      "Epoch 28/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.6423 - accuracy: 0.3326 - val_loss: 0.6402 - val_accuracy: 0.3358\n",
      "Epoch 29/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.6421 - accuracy: 0.3325 - val_loss: 0.6395 - val_accuracy: 0.3358\n",
      "Epoch 30/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.6414 - accuracy: 0.3325 - val_loss: 0.6397 - val_accuracy: 0.3358\n",
      "Epoch 31/2500\n",
      "20056/20056 [==============================] - 2s 75us/step - loss: 0.6408 - accuracy: 0.3325 - val_loss: 0.6401 - val_accuracy: 0.3358\n",
      "Epoch 32/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.6420 - accuracy: 0.3325 - val_loss: 0.6405 - val_accuracy: 0.3356\n",
      "Epoch 33/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.6420 - accuracy: 0.3326 - val_loss: 0.6388 - val_accuracy: 0.3358\n",
      "Epoch 34/2500\n",
      "20056/20056 [==============================] - 2s 77us/step - loss: 0.6404 - accuracy: 0.3326 - val_loss: 0.6398 - val_accuracy: 0.3356\n",
      "Epoch 35/2500\n",
      "20056/20056 [==============================] - 2s 82us/step - loss: 0.6403 - accuracy: 0.3327 - val_loss: 0.6381 - val_accuracy: 0.3356\n",
      "Epoch 36/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.6409 - accuracy: 0.3326 - val_loss: 0.6413 - val_accuracy: 0.3356\n",
      "Epoch 37/2500\n",
      "20056/20056 [==============================] - 2s 78us/step - loss: 0.6400 - accuracy: 0.3326 - val_loss: 0.6435 - val_accuracy: 0.3356\n",
      "Epoch 38/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.6427 - accuracy: 0.3325 - val_loss: 0.6395 - val_accuracy: 0.3358\n",
      "Epoch 39/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.6408 - accuracy: 0.3326 - val_loss: 0.6383 - val_accuracy: 0.3356\n",
      "Epoch 40/2500\n",
      "20056/20056 [==============================] - 2s 77us/step - loss: 0.6398 - accuracy: 0.3325 - val_loss: 0.6400 - val_accuracy: 0.3356\n",
      "Epoch 41/2500\n",
      "20056/20056 [==============================] - 2s 79us/step - loss: 0.6408 - accuracy: 0.3325 - val_loss: 0.6429 - val_accuracy: 0.3358\n",
      "Epoch 42/2500\n",
      "20056/20056 [==============================] - 2s 79us/step - loss: 0.6395 - accuracy: 0.3326 - val_loss: 0.6381 - val_accuracy: 0.3358\n",
      "Epoch 43/2500\n",
      "20056/20056 [==============================] - 2s 81us/step - loss: 0.6396 - accuracy: 0.3326 - val_loss: 0.6391 - val_accuracy: 0.3356\n",
      "Epoch 44/2500\n",
      "20056/20056 [==============================] - 2s 85us/step - loss: 0.6378 - accuracy: 0.3325 - val_loss: 0.6392 - val_accuracy: 0.3356\n",
      "Epoch 45/2500\n",
      "20056/20056 [==============================] - 2s 77us/step - loss: 0.6403 - accuracy: 0.3325 - val_loss: 0.6379 - val_accuracy: 0.3358\n",
      "Epoch 46/2500\n",
      "20056/20056 [==============================] - 2s 78us/step - loss: 0.6387 - accuracy: 0.3325 - val_loss: 0.6380 - val_accuracy: 0.3358\n",
      "Epoch 47/2500\n",
      "20056/20056 [==============================] - 2s 79us/step - loss: 0.6386 - accuracy: 0.3325 - val_loss: 0.6389 - val_accuracy: 0.3358\n",
      "Epoch 48/2500\n",
      "20056/20056 [==============================] - 2s 97us/step - loss: 0.6395 - accuracy: 0.3325 - val_loss: 0.6395 - val_accuracy: 0.3358\n",
      "Epoch 49/2500\n",
      "20056/20056 [==============================] - 2s 83us/step - loss: 0.6375 - accuracy: 0.3327 - val_loss: 0.6389 - val_accuracy: 0.3358\n",
      "Epoch 50/2500\n",
      "20056/20056 [==============================] - 2s 80us/step - loss: 0.6393 - accuracy: 0.3326 - val_loss: 0.6381 - val_accuracy: 0.3358\n",
      "Epoch 51/2500\n",
      "20056/20056 [==============================] - 2s 79us/step - loss: 0.6381 - accuracy: 0.3326 - val_loss: 0.6409 - val_accuracy: 0.3356\n",
      "Epoch 52/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.6384 - accuracy: 0.3326 - val_loss: 0.6380 - val_accuracy: 0.3358\n",
      "Epoch 53/2500\n",
      "20056/20056 [==============================] - 2s 79us/step - loss: 0.6390 - accuracy: 0.3325 - val_loss: 0.6404 - val_accuracy: 0.3358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.6374 - accuracy: 0.3326 - val_loss: 0.6412 - val_accuracy: 0.3356\n",
      "Epoch 55/2500\n",
      "20056/20056 [==============================] - 2s 75us/step - loss: 0.6373 - accuracy: 0.3325 - val_loss: 0.6384 - val_accuracy: 0.3358\n",
      "Epoch 56/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.6377 - accuracy: 0.3327 - val_loss: 0.6371 - val_accuracy: 0.3356\n",
      "Epoch 57/2500\n",
      "20056/20056 [==============================] - 2s 81us/step - loss: 0.6356 - accuracy: 0.3326 - val_loss: 0.6422 - val_accuracy: 0.3356\n",
      "Epoch 58/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.6356 - accuracy: 0.3325 - val_loss: 0.6403 - val_accuracy: 0.3356\n",
      "Epoch 59/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.6365 - accuracy: 0.3325 - val_loss: 0.6388 - val_accuracy: 0.3358\n",
      "Epoch 60/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.6346 - accuracy: 0.3325 - val_loss: 0.6382 - val_accuracy: 0.3356\n",
      "Epoch 61/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.6368 - accuracy: 0.3327 - val_loss: 0.6397 - val_accuracy: 0.3356\n",
      "Epoch 62/2500\n",
      "20056/20056 [==============================] - 2s 82us/step - loss: 0.6343 - accuracy: 0.3325 - val_loss: 0.6431 - val_accuracy: 0.3356\n",
      "Epoch 63/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.6367 - accuracy: 0.3326 - val_loss: 0.6394 - val_accuracy: 0.3358\n",
      "Epoch 64/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.6355 - accuracy: 0.3325 - val_loss: 0.6412 - val_accuracy: 0.3358\n",
      "Epoch 65/2500\n",
      "20056/20056 [==============================] - 1s 75us/step - loss: 0.6347 - accuracy: 0.3327 - val_loss: 0.6405 - val_accuracy: 0.3353\n",
      "Epoch 66/2500\n",
      "20056/20056 [==============================] - 2s 75us/step - loss: 0.6363 - accuracy: 0.3325 - val_loss: 0.6394 - val_accuracy: 0.3358\n",
      "Epoch 67/2500\n",
      "20056/20056 [==============================] - 2s 86us/step - loss: 0.6346 - accuracy: 0.3326 - val_loss: 0.6395 - val_accuracy: 0.3358\n",
      "Epoch 68/2500\n",
      "20056/20056 [==============================] - 2s 81us/step - loss: 0.6346 - accuracy: 0.3325 - val_loss: 0.6430 - val_accuracy: 0.3358\n",
      "Epoch 69/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.6345 - accuracy: 0.3325 - val_loss: 0.6410 - val_accuracy: 0.3358\n",
      "Epoch 70/2500\n",
      "20056/20056 [==============================] - 2s 95us/step - loss: 0.6341 - accuracy: 0.3325 - val_loss: 0.6454 - val_accuracy: 0.3358\n",
      "Epoch 71/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.6348 - accuracy: 0.3324 - val_loss: 0.6385 - val_accuracy: 0.3358\n",
      "Epoch 72/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.6335 - accuracy: 0.3326 - val_loss: 0.6405 - val_accuracy: 0.3358\n",
      "Epoch 73/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.6316 - accuracy: 0.3326 - val_loss: 0.6384 - val_accuracy: 0.3356\n",
      "Epoch 74/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.6321 - accuracy: 0.3329 - val_loss: 0.6409 - val_accuracy: 0.3356\n",
      "Epoch 75/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.6354 - accuracy: 0.3326 - val_loss: 0.6394 - val_accuracy: 0.3356\n",
      "Epoch 76/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.6338 - accuracy: 0.3326 - val_loss: 0.6386 - val_accuracy: 0.3358\n",
      "Epoch 77/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.6328 - accuracy: 0.3325 - val_loss: 0.6434 - val_accuracy: 0.3356\n",
      "Epoch 78/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.6326 - accuracy: 0.3325 - val_loss: 0.6391 - val_accuracy: 0.3358\n",
      "Epoch 79/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.6339 - accuracy: 0.3327 - val_loss: 0.6453 - val_accuracy: 0.3356\n",
      "Epoch 80/2500\n",
      "20056/20056 [==============================] - 2s 77us/step - loss: 0.6317 - accuracy: 0.3326 - val_loss: 0.6395 - val_accuracy: 0.3358\n",
      "Epoch 81/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.6306 - accuracy: 0.3326 - val_loss: 0.6393 - val_accuracy: 0.3358\n",
      "Epoch 82/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.6293 - accuracy: 0.3325 - val_loss: 0.6380 - val_accuracy: 0.3355\n",
      "Epoch 83/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.6302 - accuracy: 0.3327 - val_loss: 0.6408 - val_accuracy: 0.3356\n",
      "Epoch 84/2500\n",
      "20056/20056 [==============================] - 2s 75us/step - loss: 0.6299 - accuracy: 0.3326 - val_loss: 0.6394 - val_accuracy: 0.3358\n",
      "Epoch 85/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.6294 - accuracy: 0.3326 - val_loss: 0.6411 - val_accuracy: 0.3355\n",
      "Epoch 86/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.6306 - accuracy: 0.3326 - val_loss: 0.6411 - val_accuracy: 0.3356\n",
      "Epoch 87/2500\n",
      "20056/20056 [==============================] - 2s 80us/step - loss: 0.6292 - accuracy: 0.3326 - val_loss: 0.6414 - val_accuracy: 0.3358\n",
      "Epoch 88/2500\n",
      "20056/20056 [==============================] - 2s 77us/step - loss: 0.6296 - accuracy: 0.3325 - val_loss: 0.6399 - val_accuracy: 0.3356\n",
      "Epoch 89/2500\n",
      "20056/20056 [==============================] - 2s 80us/step - loss: 0.6291 - accuracy: 0.3325 - val_loss: 0.6432 - val_accuracy: 0.3356\n",
      "Epoch 90/2500\n",
      "20056/20056 [==============================] - 2s 84us/step - loss: 0.6286 - accuracy: 0.3326 - val_loss: 0.6486 - val_accuracy: 0.3356\n",
      "Epoch 91/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.6284 - accuracy: 0.3326 - val_loss: 0.6408 - val_accuracy: 0.3356\n",
      "Epoch 92/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.6284 - accuracy: 0.3325 - val_loss: 0.6415 - val_accuracy: 0.3356\n",
      "Epoch 93/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.6273 - accuracy: 0.3325 - val_loss: 0.6418 - val_accuracy: 0.3356\n",
      "Epoch 94/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.6283 - accuracy: 0.3328 - val_loss: 0.6411 - val_accuracy: 0.3358\n",
      "Epoch 95/2500\n",
      "20056/20056 [==============================] - 2s 81us/step - loss: 0.6278 - accuracy: 0.3326 - val_loss: 0.6480 - val_accuracy: 0.3356\n",
      "Epoch 96/2500\n",
      "20056/20056 [==============================] - 2s 75us/step - loss: 0.6276 - accuracy: 0.3326 - val_loss: 0.6411 - val_accuracy: 0.3358\n",
      "Epoch 97/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.6265 - accuracy: 0.3325 - val_loss: 0.6430 - val_accuracy: 0.3358\n",
      "Epoch 98/2500\n",
      "20056/20056 [==============================] - 2s 79us/step - loss: 0.6256 - accuracy: 0.3326 - val_loss: 0.6398 - val_accuracy: 0.3356\n",
      "Epoch 99/2500\n",
      "20056/20056 [==============================] - 2s 85us/step - loss: 0.6243 - accuracy: 0.3327 - val_loss: 0.6404 - val_accuracy: 0.3358\n",
      "Epoch 100/2500\n",
      "20056/20056 [==============================] - 2s 84us/step - loss: 0.6250 - accuracy: 0.3326 - val_loss: 0.6485 - val_accuracy: 0.3356\n",
      "Epoch 101/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.6258 - accuracy: 0.3326 - val_loss: 0.6440 - val_accuracy: 0.3356\n",
      "Epoch 102/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.6235 - accuracy: 0.3326 - val_loss: 0.6415 - val_accuracy: 0.3358\n",
      "Epoch 103/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.6257 - accuracy: 0.3325 - val_loss: 0.6437 - val_accuracy: 0.3356\n",
      "Epoch 104/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.6241 - accuracy: 0.3328 - val_loss: 0.6432 - val_accuracy: 0.3356\n",
      "Epoch 105/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.6247 - accuracy: 0.3327 - val_loss: 0.6416 - val_accuracy: 0.3358\n",
      "Epoch 106/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.6238 - accuracy: 0.3325 - val_loss: 0.6425 - val_accuracy: 0.3356\n",
      "Epoch 107/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.6229 - accuracy: 0.3325 - val_loss: 0.6406 - val_accuracy: 0.3358\n",
      "Epoch 108/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.6218 - accuracy: 0.3324 - val_loss: 0.6443 - val_accuracy: 0.3358\n",
      "Epoch 109/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.6228 - accuracy: 0.3326 - val_loss: 0.6472 - val_accuracy: 0.3356\n",
      "Epoch 110/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.6211 - accuracy: 0.3326 - val_loss: 0.6466 - val_accuracy: 0.3356\n",
      "Epoch 111/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.6224 - accuracy: 0.3325 - val_loss: 0.6419 - val_accuracy: 0.3358\n",
      "Epoch 112/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.6247 - accuracy: 0.3416 - val_loss: 0.6425 - val_accuracy: 0.3356\n",
      "Epoch 113/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.6239 - accuracy: 0.3326 - val_loss: 0.6446 - val_accuracy: 0.3358\n",
      "Epoch 114/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.6216 - accuracy: 0.3325 - val_loss: 0.6416 - val_accuracy: 0.3356\n",
      "Epoch 115/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.6198 - accuracy: 0.3326 - val_loss: 0.6426 - val_accuracy: 0.3356\n",
      "Epoch 116/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.6204 - accuracy: 0.3330 - val_loss: 0.6427 - val_accuracy: 0.3356\n",
      "Epoch 117/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.6183 - accuracy: 0.3336 - val_loss: 0.6462 - val_accuracy: 0.3356\n",
      "Epoch 118/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.6217 - accuracy: 0.3336 - val_loss: 0.6448 - val_accuracy: 0.3358\n",
      "Epoch 119/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.6219 - accuracy: 0.3325 - val_loss: 0.6472 - val_accuracy: 0.3358\n",
      "Epoch 120/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.6189 - accuracy: 0.3326 - val_loss: 0.6468 - val_accuracy: 0.3356\n",
      "Epoch 121/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.6194 - accuracy: 0.3327 - val_loss: 0.6504 - val_accuracy: 0.3356\n",
      "Epoch 122/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.6196 - accuracy: 0.3326 - val_loss: 0.6469 - val_accuracy: 0.3356\n",
      "Epoch 123/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.6202 - accuracy: 0.3325 - val_loss: 0.6496 - val_accuracy: 0.3358\n",
      "Epoch 124/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.6184 - accuracy: 0.3328 - val_loss: 0.6505 - val_accuracy: 0.3356\n",
      "Epoch 125/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.6185 - accuracy: 0.3350 - val_loss: 0.6477 - val_accuracy: 0.3356\n",
      "Epoch 126/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.6180 - accuracy: 0.3349 - val_loss: 0.6473 - val_accuracy: 0.3356\n",
      "Epoch 127/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.6158 - accuracy: 0.3388 - val_loss: 0.6458 - val_accuracy: 0.3358\n",
      "Epoch 128/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.6155 - accuracy: 0.3467 - val_loss: 0.6468 - val_accuracy: 0.3358\n",
      "Epoch 129/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.6158 - accuracy: 0.3449 - val_loss: 0.6496 - val_accuracy: 0.3588\n",
      "Epoch 130/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.6159 - accuracy: 0.3496 - val_loss: 0.6473 - val_accuracy: 0.3566\n",
      "Epoch 131/2500\n",
      "20056/20056 [==============================] - 2s 80us/step - loss: 0.6147 - accuracy: 0.3586 - val_loss: 0.6561 - val_accuracy: 0.3356\n",
      "Epoch 132/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.6137 - accuracy: 0.3512 - val_loss: 0.6473 - val_accuracy: 0.3551\n",
      "Epoch 133/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.6143 - accuracy: 0.3580 - val_loss: 0.6450 - val_accuracy: 0.3552\n",
      "Epoch 134/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.6133 - accuracy: 0.3565 - val_loss: 0.6484 - val_accuracy: 0.3648\n",
      "Epoch 135/2500\n",
      "20056/20056 [==============================] - 2s 75us/step - loss: 0.6143 - accuracy: 0.3606 - val_loss: 0.6485 - val_accuracy: 0.3663\n",
      "Epoch 136/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.6121 - accuracy: 0.3603 - val_loss: 0.6473 - val_accuracy: 0.3551\n",
      "Epoch 137/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.6108 - accuracy: 0.3636 - val_loss: 0.6454 - val_accuracy: 0.3609\n",
      "Epoch 138/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.6112 - accuracy: 0.3628 - val_loss: 0.6467 - val_accuracy: 0.3675\n",
      "Epoch 139/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.6101 - accuracy: 0.3633 - val_loss: 0.6501 - val_accuracy: 0.3621\n",
      "Epoch 140/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.6111 - accuracy: 0.3650 - val_loss: 0.6485 - val_accuracy: 0.3738\n",
      "Epoch 141/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.6099 - accuracy: 0.3697 - val_loss: 0.6492 - val_accuracy: 0.3594\n",
      "Epoch 142/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.6075 - accuracy: 0.3681 - val_loss: 0.6482 - val_accuracy: 0.3676\n",
      "Epoch 143/2500\n",
      "20056/20056 [==============================] - 2s 78us/step - loss: 0.6087 - accuracy: 0.3671 - val_loss: 0.6499 - val_accuracy: 0.3620\n",
      "Epoch 144/2500\n",
      "20056/20056 [==============================] - 2s 79us/step - loss: 0.6088 - accuracy: 0.3623 - val_loss: 0.6504 - val_accuracy: 0.3646\n",
      "Epoch 145/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.6093 - accuracy: 0.3676 - val_loss: 0.6511 - val_accuracy: 0.3555\n",
      "Epoch 146/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.6081 - accuracy: 0.3671 - val_loss: 0.6505 - val_accuracy: 0.3667\n",
      "Epoch 147/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.6063 - accuracy: 0.3688 - val_loss: 0.6531 - val_accuracy: 0.3600\n",
      "Epoch 148/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.6087 - accuracy: 0.3664 - val_loss: 0.6512 - val_accuracy: 0.3640\n",
      "Epoch 149/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.6053 - accuracy: 0.3697 - val_loss: 0.6519 - val_accuracy: 0.3666\n",
      "Epoch 150/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.6052 - accuracy: 0.3738 - val_loss: 0.6520 - val_accuracy: 0.3669\n",
      "Epoch 151/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.6094 - accuracy: 0.3692 - val_loss: 0.6550 - val_accuracy: 0.3670\n",
      "Epoch 152/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.6069 - accuracy: 0.3724 - val_loss: 0.6555 - val_accuracy: 0.3672\n",
      "Epoch 153/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.6052 - accuracy: 0.3727 - val_loss: 0.6562 - val_accuracy: 0.3646\n",
      "Epoch 154/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.6029 - accuracy: 0.3745 - val_loss: 0.6570 - val_accuracy: 0.3685\n",
      "Epoch 155/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.6053 - accuracy: 0.3731 - val_loss: 0.6560 - val_accuracy: 0.3724\n",
      "Epoch 156/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.6040 - accuracy: 0.3705 - val_loss: 0.6569 - val_accuracy: 0.3682\n",
      "Epoch 157/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.6003 - accuracy: 0.3747 - val_loss: 0.6566 - val_accuracy: 0.3685\n",
      "Epoch 158/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.6031 - accuracy: 0.3720 - val_loss: 0.6546 - val_accuracy: 0.3645\n",
      "Epoch 159/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.6008 - accuracy: 0.3748 - val_loss: 0.6560 - val_accuracy: 0.3655\n",
      "Epoch 160/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.6004 - accuracy: 0.3763 - val_loss: 0.6537 - val_accuracy: 0.3611\n",
      "Epoch 161/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.6012 - accuracy: 0.3736 - val_loss: 0.6514 - val_accuracy: 0.3648\n",
      "Epoch 162/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.5988 - accuracy: 0.3763 - val_loss: 0.6527 - val_accuracy: 0.3673\n",
      "Epoch 163/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.6003 - accuracy: 0.3760 - val_loss: 0.6529 - val_accuracy: 0.3596\n",
      "Epoch 164/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.5990 - accuracy: 0.3750 - val_loss: 0.6534 - val_accuracy: 0.3657\n",
      "Epoch 165/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.6015 - accuracy: 0.3749 - val_loss: 0.6542 - val_accuracy: 0.3621\n",
      "Epoch 166/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.5975 - accuracy: 0.3790 - val_loss: 0.6514 - val_accuracy: 0.3654\n",
      "Epoch 167/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.5971 - accuracy: 0.3769 - val_loss: 0.6639 - val_accuracy: 0.3646\n",
      "Epoch 168/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.5966 - accuracy: 0.3805 - val_loss: 0.6564 - val_accuracy: 0.3682\n",
      "Epoch 169/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.5968 - accuracy: 0.3779 - val_loss: 0.6549 - val_accuracy: 0.3663\n",
      "Epoch 170/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.5984 - accuracy: 0.3776 - val_loss: 0.6550 - val_accuracy: 0.3633\n",
      "Epoch 171/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.5951 - accuracy: 0.3789 - val_loss: 0.6526 - val_accuracy: 0.3606\n",
      "Epoch 172/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.5955 - accuracy: 0.3774 - val_loss: 0.6567 - val_accuracy: 0.3712\n",
      "Epoch 173/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.5941 - accuracy: 0.3815 - val_loss: 0.6582 - val_accuracy: 0.3657\n",
      "Epoch 174/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.5920 - accuracy: 0.3848 - val_loss: 0.6578 - val_accuracy: 0.3657\n",
      "Epoch 175/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.5925 - accuracy: 0.3819 - val_loss: 0.6624 - val_accuracy: 0.3652\n",
      "Epoch 176/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.5909 - accuracy: 0.3850 - val_loss: 0.6665 - val_accuracy: 0.3684\n",
      "Epoch 177/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.5913 - accuracy: 0.3842 - val_loss: 0.6598 - val_accuracy: 0.3679\n",
      "Epoch 178/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.5931 - accuracy: 0.3839 - val_loss: 0.6604 - val_accuracy: 0.3603\n",
      "Epoch 179/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.5919 - accuracy: 0.3856 - val_loss: 0.6578 - val_accuracy: 0.3664\n",
      "Epoch 180/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.5917 - accuracy: 0.3839 - val_loss: 0.6663 - val_accuracy: 0.3735\n",
      "Epoch 181/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.5911 - accuracy: 0.3861 - val_loss: 0.6665 - val_accuracy: 0.3666\n",
      "Epoch 182/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.5912 - accuracy: 0.3829 - val_loss: 0.6542 - val_accuracy: 0.3687\n",
      "Epoch 183/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.5893 - accuracy: 0.3828 - val_loss: 0.6569 - val_accuracy: 0.3652\n",
      "Epoch 184/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.5892 - accuracy: 0.3867 - val_loss: 0.6599 - val_accuracy: 0.3615\n",
      "Epoch 185/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.5868 - accuracy: 0.3868 - val_loss: 0.6599 - val_accuracy: 0.3685\n",
      "Epoch 186/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.5888 - accuracy: 0.3854 - val_loss: 0.6621 - val_accuracy: 0.3648\n",
      "Epoch 187/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5874 - accuracy: 0.3877 - val_loss: 0.6591 - val_accuracy: 0.3700\n",
      "Epoch 188/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5869 - accuracy: 0.3881 - val_loss: 0.6602 - val_accuracy: 0.3631\n",
      "Epoch 189/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5876 - accuracy: 0.3887 - val_loss: 0.6626 - val_accuracy: 0.3585\n",
      "Epoch 190/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5902 - accuracy: 0.3853 - val_loss: 0.6659 - val_accuracy: 0.3715\n",
      "Epoch 191/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5915 - accuracy: 0.3845 - val_loss: 0.6577 - val_accuracy: 0.3594\n",
      "Epoch 192/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5903 - accuracy: 0.3819 - val_loss: 0.6587 - val_accuracy: 0.3633\n",
      "Epoch 193/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5859 - accuracy: 0.3886 - val_loss: 0.6609 - val_accuracy: 0.3642\n",
      "Epoch 194/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5873 - accuracy: 0.3860 - val_loss: 0.6688 - val_accuracy: 0.3566\n",
      "Epoch 195/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5903 - accuracy: 0.3856 - val_loss: 0.6646 - val_accuracy: 0.3732\n",
      "Epoch 196/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5837 - accuracy: 0.3920 - val_loss: 0.6592 - val_accuracy: 0.3685\n",
      "Epoch 197/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5846 - accuracy: 0.3936 - val_loss: 0.6586 - val_accuracy: 0.3642\n",
      "Epoch 198/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5862 - accuracy: 0.3866 - val_loss: 0.6604 - val_accuracy: 0.3675\n",
      "Epoch 199/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.5815 - accuracy: 0.3919 - val_loss: 0.6633 - val_accuracy: 0.3696\n",
      "Epoch 200/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5848 - accuracy: 0.3892 - val_loss: 0.6613 - val_accuracy: 0.3697\n",
      "Epoch 201/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5820 - accuracy: 0.3925 - val_loss: 0.6605 - val_accuracy: 0.3648\n",
      "Epoch 202/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5824 - accuracy: 0.3917 - val_loss: 0.6644 - val_accuracy: 0.3675\n",
      "Epoch 203/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5820 - accuracy: 0.3941 - val_loss: 0.6595 - val_accuracy: 0.3643\n",
      "Epoch 204/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5815 - accuracy: 0.3923 - val_loss: 0.6560 - val_accuracy: 0.3640\n",
      "Epoch 205/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5801 - accuracy: 0.3924 - val_loss: 0.6643 - val_accuracy: 0.3664\n",
      "Epoch 206/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5804 - accuracy: 0.3951 - val_loss: 0.6624 - val_accuracy: 0.3654\n",
      "Epoch 207/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5803 - accuracy: 0.3940 - val_loss: 0.6592 - val_accuracy: 0.3573\n",
      "Epoch 208/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5790 - accuracy: 0.3933 - val_loss: 0.6641 - val_accuracy: 0.3612\n",
      "Epoch 209/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.5777 - accuracy: 0.3943 - val_loss: 0.6637 - val_accuracy: 0.3673\n",
      "Epoch 210/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5777 - accuracy: 0.3931 - val_loss: 0.6650 - val_accuracy: 0.3699\n",
      "Epoch 211/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.5769 - accuracy: 0.3960 - val_loss: 0.6699 - val_accuracy: 0.3600\n",
      "Epoch 212/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5784 - accuracy: 0.3940 - val_loss: 0.6704 - val_accuracy: 0.3676\n",
      "Epoch 213/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5798 - accuracy: 0.3942 - val_loss: 0.6752 - val_accuracy: 0.3649\n",
      "Epoch 214/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5844 - accuracy: 0.3909 - val_loss: 0.6616 - val_accuracy: 0.3625\n",
      "Epoch 215/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5786 - accuracy: 0.3938 - val_loss: 0.6663 - val_accuracy: 0.3643\n",
      "Epoch 216/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5786 - accuracy: 0.3954 - val_loss: 0.6706 - val_accuracy: 0.3712\n",
      "Epoch 217/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5748 - accuracy: 0.3979 - val_loss: 0.6689 - val_accuracy: 0.3624\n",
      "Epoch 218/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5746 - accuracy: 0.3963 - val_loss: 0.6729 - val_accuracy: 0.3666\n",
      "Epoch 219/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5757 - accuracy: 0.3961 - val_loss: 0.6656 - val_accuracy: 0.3657\n",
      "Epoch 220/2500\n",
      "20056/20056 [==============================] - 1s 51us/step - loss: 0.5701 - accuracy: 0.4032 - val_loss: 0.6657 - val_accuracy: 0.3696\n",
      "Epoch 221/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5704 - accuracy: 0.4045 - val_loss: 0.6674 - val_accuracy: 0.3609\n",
      "Epoch 222/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5723 - accuracy: 0.4028 - val_loss: 0.6729 - val_accuracy: 0.3649\n",
      "Epoch 223/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5732 - accuracy: 0.4011 - val_loss: 0.6667 - val_accuracy: 0.3669\n",
      "Epoch 224/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5710 - accuracy: 0.4047 - val_loss: 0.6775 - val_accuracy: 0.3694\n",
      "Epoch 225/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5690 - accuracy: 0.4055 - val_loss: 0.6659 - val_accuracy: 0.3587\n",
      "Epoch 226/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5678 - accuracy: 0.4065 - val_loss: 0.6748 - val_accuracy: 0.3649\n",
      "Epoch 227/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5691 - accuracy: 0.4053 - val_loss: 0.6657 - val_accuracy: 0.3617\n",
      "Epoch 228/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5662 - accuracy: 0.4064 - val_loss: 0.6752 - val_accuracy: 0.3769\n",
      "Epoch 229/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5654 - accuracy: 0.4100 - val_loss: 0.6713 - val_accuracy: 0.3673\n",
      "Epoch 230/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5703 - accuracy: 0.4060 - val_loss: 0.6729 - val_accuracy: 0.3652\n",
      "Epoch 231/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5698 - accuracy: 0.4044 - val_loss: 0.6753 - val_accuracy: 0.3694\n",
      "Epoch 232/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5670 - accuracy: 0.4059 - val_loss: 0.6771 - val_accuracy: 0.3642\n",
      "Epoch 233/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5687 - accuracy: 0.4071 - val_loss: 0.6750 - val_accuracy: 0.3615\n",
      "Epoch 234/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5647 - accuracy: 0.4105 - val_loss: 0.6738 - val_accuracy: 0.3693\n",
      "Epoch 235/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5672 - accuracy: 0.4076 - val_loss: 0.6706 - val_accuracy: 0.3628\n",
      "Epoch 236/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5666 - accuracy: 0.4113 - val_loss: 0.6712 - val_accuracy: 0.3690\n",
      "Epoch 237/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5671 - accuracy: 0.4033 - val_loss: 0.6663 - val_accuracy: 0.3720\n",
      "Epoch 238/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5660 - accuracy: 0.4091 - val_loss: 0.6666 - val_accuracy: 0.3643\n",
      "Epoch 239/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5702 - accuracy: 0.4018 - val_loss: 0.6686 - val_accuracy: 0.3669\n",
      "Epoch 240/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5647 - accuracy: 0.4110 - val_loss: 0.6655 - val_accuracy: 0.3676\n",
      "Epoch 241/2500\n",
      "20056/20056 [==============================] - 1s 51us/step - loss: 0.5620 - accuracy: 0.4108 - val_loss: 0.6723 - val_accuracy: 0.3652\n",
      "Epoch 242/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5656 - accuracy: 0.4071 - val_loss: 0.6763 - val_accuracy: 0.3727\n",
      "Epoch 243/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5647 - accuracy: 0.4132 - val_loss: 0.6698 - val_accuracy: 0.3703\n",
      "Epoch 244/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.5616 - accuracy: 0.4129 - val_loss: 0.6824 - val_accuracy: 0.3753\n",
      "Epoch 245/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5656 - accuracy: 0.4071 - val_loss: 0.6796 - val_accuracy: 0.3655\n",
      "Epoch 246/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.5608 - accuracy: 0.4126 - val_loss: 0.6787 - val_accuracy: 0.3760\n",
      "Epoch 247/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5641 - accuracy: 0.4126 - val_loss: 0.6774 - val_accuracy: 0.3637\n",
      "Epoch 248/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5642 - accuracy: 0.4108 - val_loss: 0.6735 - val_accuracy: 0.3646\n",
      "Epoch 249/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5617 - accuracy: 0.4097 - val_loss: 0.6820 - val_accuracy: 0.3672\n",
      "Epoch 250/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.5614 - accuracy: 0.4129 - val_loss: 0.6707 - val_accuracy: 0.3694\n",
      "Epoch 251/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5597 - accuracy: 0.4124 - val_loss: 0.6787 - val_accuracy: 0.3691\n",
      "Epoch 252/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5582 - accuracy: 0.4182 - val_loss: 0.6786 - val_accuracy: 0.3651\n",
      "Epoch 253/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5597 - accuracy: 0.4171 - val_loss: 0.6775 - val_accuracy: 0.3649\n",
      "Epoch 254/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5583 - accuracy: 0.4164 - val_loss: 0.6716 - val_accuracy: 0.3697\n",
      "Epoch 255/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5598 - accuracy: 0.4149 - val_loss: 0.6680 - val_accuracy: 0.3675\n",
      "Epoch 256/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5576 - accuracy: 0.4143 - val_loss: 0.6790 - val_accuracy: 0.3696\n",
      "Epoch 257/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5611 - accuracy: 0.4154 - val_loss: 0.6798 - val_accuracy: 0.3685\n",
      "Epoch 258/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5611 - accuracy: 0.4130 - val_loss: 0.6777 - val_accuracy: 0.3678\n",
      "Epoch 259/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5582 - accuracy: 0.4187 - val_loss: 0.6695 - val_accuracy: 0.3646\n",
      "Epoch 260/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5591 - accuracy: 0.4134 - val_loss: 0.6843 - val_accuracy: 0.3708\n",
      "Epoch 261/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5557 - accuracy: 0.4180 - val_loss: 0.6773 - val_accuracy: 0.3654\n",
      "Epoch 262/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5565 - accuracy: 0.4206 - val_loss: 0.6808 - val_accuracy: 0.3651\n",
      "Epoch 263/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5564 - accuracy: 0.4166 - val_loss: 0.6731 - val_accuracy: 0.3645\n",
      "Epoch 264/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5601 - accuracy: 0.4159 - val_loss: 0.6817 - val_accuracy: 0.3681\n",
      "Epoch 265/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5570 - accuracy: 0.4171 - val_loss: 0.6810 - val_accuracy: 0.3759\n",
      "Epoch 266/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5514 - accuracy: 0.4249 - val_loss: 0.6777 - val_accuracy: 0.3705\n",
      "Epoch 267/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5551 - accuracy: 0.4198 - val_loss: 0.6772 - val_accuracy: 0.3682\n",
      "Epoch 268/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5556 - accuracy: 0.4181 - val_loss: 0.6833 - val_accuracy: 0.3738\n",
      "Epoch 269/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5558 - accuracy: 0.4185 - val_loss: 0.6742 - val_accuracy: 0.3720\n",
      "Epoch 270/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5553 - accuracy: 0.4180 - val_loss: 0.6849 - val_accuracy: 0.3655\n",
      "Epoch 271/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5540 - accuracy: 0.4207 - val_loss: 0.6811 - val_accuracy: 0.3660\n",
      "Epoch 272/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.5544 - accuracy: 0.4226 - val_loss: 0.6763 - val_accuracy: 0.3681\n",
      "Epoch 273/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5533 - accuracy: 0.4223 - val_loss: 0.6759 - val_accuracy: 0.3667\n",
      "Epoch 274/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5545 - accuracy: 0.4196 - val_loss: 0.6752 - val_accuracy: 0.3649\n",
      "Epoch 275/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5521 - accuracy: 0.4191 - val_loss: 0.6742 - val_accuracy: 0.3652\n",
      "Epoch 276/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5536 - accuracy: 0.4188 - val_loss: 0.6817 - val_accuracy: 0.3676\n",
      "Epoch 277/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5525 - accuracy: 0.4213 - val_loss: 0.6779 - val_accuracy: 0.3699\n",
      "Epoch 278/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5506 - accuracy: 0.4229 - val_loss: 0.6870 - val_accuracy: 0.3688\n",
      "Epoch 279/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5517 - accuracy: 0.4187 - val_loss: 0.6758 - val_accuracy: 0.3703\n",
      "Epoch 280/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5510 - accuracy: 0.4243 - val_loss: 0.6830 - val_accuracy: 0.3678\n",
      "Epoch 281/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5492 - accuracy: 0.4233 - val_loss: 0.6844 - val_accuracy: 0.3696\n",
      "Epoch 282/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5559 - accuracy: 0.4188 - val_loss: 0.6724 - val_accuracy: 0.3658\n",
      "Epoch 283/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5480 - accuracy: 0.4286 - val_loss: 0.6889 - val_accuracy: 0.3690\n",
      "Epoch 284/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5586 - accuracy: 0.4170 - val_loss: 0.6805 - val_accuracy: 0.3742\n",
      "Epoch 285/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5482 - accuracy: 0.4265 - val_loss: 0.6789 - val_accuracy: 0.3718\n",
      "Epoch 286/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5501 - accuracy: 0.4236 - val_loss: 0.6809 - val_accuracy: 0.3666\n",
      "Epoch 287/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5482 - accuracy: 0.4261 - val_loss: 0.6756 - val_accuracy: 0.3702\n",
      "Epoch 288/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5470 - accuracy: 0.4253 - val_loss: 0.6865 - val_accuracy: 0.3694\n",
      "Epoch 289/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5463 - accuracy: 0.4282 - val_loss: 0.6820 - val_accuracy: 0.3715\n",
      "Epoch 290/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5481 - accuracy: 0.4259 - val_loss: 0.6804 - val_accuracy: 0.3709\n",
      "Epoch 291/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.5449 - accuracy: 0.4282 - val_loss: 0.6850 - val_accuracy: 0.3700\n",
      "Epoch 292/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5420 - accuracy: 0.4297 - val_loss: 0.6799 - val_accuracy: 0.3684\n",
      "Epoch 293/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5417 - accuracy: 0.4324 - val_loss: 0.6792 - val_accuracy: 0.3712\n",
      "Epoch 294/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5417 - accuracy: 0.4309 - val_loss: 0.6805 - val_accuracy: 0.3663\n",
      "Epoch 295/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5463 - accuracy: 0.4244 - val_loss: 0.6803 - val_accuracy: 0.3681\n",
      "Epoch 296/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5434 - accuracy: 0.4309 - val_loss: 0.6873 - val_accuracy: 0.3667\n",
      "Epoch 297/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5457 - accuracy: 0.4306 - val_loss: 0.6789 - val_accuracy: 0.3669\n",
      "Epoch 298/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5452 - accuracy: 0.4309 - val_loss: 0.6770 - val_accuracy: 0.3696\n",
      "Epoch 299/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5421 - accuracy: 0.4337 - val_loss: 0.6760 - val_accuracy: 0.3663\n",
      "Epoch 300/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5436 - accuracy: 0.4302 - val_loss: 0.6755 - val_accuracy: 0.3669\n",
      "Epoch 301/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5447 - accuracy: 0.4276 - val_loss: 0.6762 - val_accuracy: 0.3700\n",
      "Epoch 302/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5426 - accuracy: 0.4332 - val_loss: 0.6765 - val_accuracy: 0.3687\n",
      "Epoch 303/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5455 - accuracy: 0.4290 - val_loss: 0.6804 - val_accuracy: 0.3669\n",
      "Epoch 304/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5400 - accuracy: 0.4311 - val_loss: 0.6882 - val_accuracy: 0.3682\n",
      "Epoch 305/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5398 - accuracy: 0.4326 - val_loss: 0.6880 - val_accuracy: 0.3584\n",
      "Epoch 306/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5419 - accuracy: 0.4312 - val_loss: 0.6875 - val_accuracy: 0.3687\n",
      "Epoch 307/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5431 - accuracy: 0.4312 - val_loss: 0.6869 - val_accuracy: 0.3718\n",
      "Epoch 308/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5447 - accuracy: 0.4298 - val_loss: 0.6850 - val_accuracy: 0.3727\n",
      "Epoch 309/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5443 - accuracy: 0.4297 - val_loss: 0.6717 - val_accuracy: 0.3639\n",
      "Epoch 310/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5383 - accuracy: 0.4339 - val_loss: 0.6837 - val_accuracy: 0.3736\n",
      "Epoch 311/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5407 - accuracy: 0.4322 - val_loss: 0.6854 - val_accuracy: 0.3655\n",
      "Epoch 312/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5419 - accuracy: 0.4326 - val_loss: 0.6841 - val_accuracy: 0.3649\n",
      "Epoch 313/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5405 - accuracy: 0.4342 - val_loss: 0.6780 - val_accuracy: 0.3708\n",
      "Epoch 314/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5392 - accuracy: 0.4344 - val_loss: 0.6881 - val_accuracy: 0.3645\n",
      "Epoch 315/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5403 - accuracy: 0.4343 - val_loss: 0.6872 - val_accuracy: 0.3720\n",
      "Epoch 316/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5375 - accuracy: 0.4366 - val_loss: 0.6896 - val_accuracy: 0.3715\n",
      "Epoch 317/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5378 - accuracy: 0.4364 - val_loss: 0.6834 - val_accuracy: 0.3703\n",
      "Epoch 318/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5397 - accuracy: 0.4353 - val_loss: 0.6778 - val_accuracy: 0.3637\n",
      "Epoch 319/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5394 - accuracy: 0.4329 - val_loss: 0.6826 - val_accuracy: 0.3712\n",
      "Epoch 320/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5343 - accuracy: 0.4381 - val_loss: 0.6827 - val_accuracy: 0.3672\n",
      "Epoch 321/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5388 - accuracy: 0.4343 - val_loss: 0.6870 - val_accuracy: 0.3694\n",
      "Epoch 322/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5380 - accuracy: 0.4366 - val_loss: 0.6801 - val_accuracy: 0.3711\n",
      "Epoch 323/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5365 - accuracy: 0.4385 - val_loss: 0.6888 - val_accuracy: 0.3687\n",
      "Epoch 324/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5346 - accuracy: 0.4371 - val_loss: 0.6859 - val_accuracy: 0.3708\n",
      "Epoch 325/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5345 - accuracy: 0.4398 - val_loss: 0.6926 - val_accuracy: 0.3679\n",
      "Epoch 326/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5349 - accuracy: 0.4411 - val_loss: 0.6862 - val_accuracy: 0.3634\n",
      "Epoch 327/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5410 - accuracy: 0.4338 - val_loss: 0.6892 - val_accuracy: 0.3664\n",
      "Epoch 328/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5431 - accuracy: 0.4329 - val_loss: 0.6808 - val_accuracy: 0.3649\n",
      "Epoch 329/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5383 - accuracy: 0.4335 - val_loss: 0.6805 - val_accuracy: 0.3706\n",
      "Epoch 330/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.5334 - accuracy: 0.4410 - val_loss: 0.6878 - val_accuracy: 0.3742\n",
      "Epoch 331/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5345 - accuracy: 0.4370 - val_loss: 0.6881 - val_accuracy: 0.3670\n",
      "Epoch 332/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5297 - accuracy: 0.4424 - val_loss: 0.6863 - val_accuracy: 0.3697\n",
      "Epoch 333/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5360 - accuracy: 0.4352 - val_loss: 0.6829 - val_accuracy: 0.3682\n",
      "Epoch 334/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5307 - accuracy: 0.4424 - val_loss: 0.6855 - val_accuracy: 0.3690\n",
      "Epoch 335/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.5314 - accuracy: 0.4424 - val_loss: 0.6809 - val_accuracy: 0.3605\n",
      "Epoch 336/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5361 - accuracy: 0.4334 - val_loss: 0.6843 - val_accuracy: 0.3715\n",
      "Epoch 337/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.5349 - accuracy: 0.4392 - val_loss: 0.6887 - val_accuracy: 0.3652\n",
      "Epoch 338/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5359 - accuracy: 0.4396 - val_loss: 0.6779 - val_accuracy: 0.3676\n",
      "Epoch 339/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5301 - accuracy: 0.4441 - val_loss: 0.6877 - val_accuracy: 0.3676\n",
      "Epoch 340/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5294 - accuracy: 0.4411 - val_loss: 0.6877 - val_accuracy: 0.3599\n",
      "Epoch 341/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5313 - accuracy: 0.4431 - val_loss: 0.6848 - val_accuracy: 0.3684\n",
      "Epoch 342/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5329 - accuracy: 0.4432 - val_loss: 0.6921 - val_accuracy: 0.3643\n",
      "Epoch 343/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5300 - accuracy: 0.4452 - val_loss: 0.6822 - val_accuracy: 0.3639\n",
      "Epoch 344/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5282 - accuracy: 0.4442 - val_loss: 0.6839 - val_accuracy: 0.3663\n",
      "Epoch 345/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5275 - accuracy: 0.4464 - val_loss: 0.6998 - val_accuracy: 0.3687\n",
      "Epoch 346/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5286 - accuracy: 0.4453 - val_loss: 0.6977 - val_accuracy: 0.3658\n",
      "Epoch 347/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5270 - accuracy: 0.4443 - val_loss: 0.6849 - val_accuracy: 0.3673\n",
      "Epoch 348/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5309 - accuracy: 0.4417 - val_loss: 0.6846 - val_accuracy: 0.3693\n",
      "Epoch 349/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5309 - accuracy: 0.4467 - val_loss: 0.6888 - val_accuracy: 0.3682\n",
      "Epoch 350/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5293 - accuracy: 0.4449 - val_loss: 0.6862 - val_accuracy: 0.3628\n",
      "Epoch 351/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5268 - accuracy: 0.4490 - val_loss: 0.6936 - val_accuracy: 0.3714\n",
      "Epoch 352/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5305 - accuracy: 0.4435 - val_loss: 0.6867 - val_accuracy: 0.3643\n",
      "Epoch 353/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5301 - accuracy: 0.4478 - val_loss: 0.6922 - val_accuracy: 0.3660\n",
      "Epoch 354/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.5371 - accuracy: 0.4392 - val_loss: 0.6846 - val_accuracy: 0.3703\n",
      "Epoch 355/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5284 - accuracy: 0.4446 - val_loss: 0.6925 - val_accuracy: 0.3685\n",
      "Epoch 356/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5299 - accuracy: 0.4412 - val_loss: 0.6901 - val_accuracy: 0.3673\n",
      "Epoch 357/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.5236 - accuracy: 0.4491 - val_loss: 0.7037 - val_accuracy: 0.3637\n",
      "Epoch 358/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5281 - accuracy: 0.4488 - val_loss: 0.6815 - val_accuracy: 0.3625\n",
      "Epoch 359/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5257 - accuracy: 0.4450 - val_loss: 0.6974 - val_accuracy: 0.3639\n",
      "Epoch 360/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5236 - accuracy: 0.4455 - val_loss: 0.6879 - val_accuracy: 0.3639\n",
      "Epoch 361/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5244 - accuracy: 0.4489 - val_loss: 0.6873 - val_accuracy: 0.3652\n",
      "Epoch 362/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5236 - accuracy: 0.4481 - val_loss: 0.6969 - val_accuracy: 0.3646\n",
      "Epoch 363/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5287 - accuracy: 0.4470 - val_loss: 0.6918 - val_accuracy: 0.3724\n",
      "Epoch 364/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5289 - accuracy: 0.4470 - val_loss: 0.6933 - val_accuracy: 0.3682\n",
      "Epoch 365/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5329 - accuracy: 0.4439 - val_loss: 0.6878 - val_accuracy: 0.3673\n",
      "Epoch 366/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.5259 - accuracy: 0.4470 - val_loss: 0.6904 - val_accuracy: 0.3685\n",
      "Epoch 367/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5263 - accuracy: 0.4527 - val_loss: 0.6853 - val_accuracy: 0.3687\n",
      "Epoch 368/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5304 - accuracy: 0.4437 - val_loss: 0.6845 - val_accuracy: 0.3694\n",
      "Epoch 369/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5273 - accuracy: 0.4472 - val_loss: 0.6930 - val_accuracy: 0.3693\n",
      "Epoch 370/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5325 - accuracy: 0.4428 - val_loss: 0.6780 - val_accuracy: 0.3622\n",
      "Epoch 371/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5229 - accuracy: 0.4512 - val_loss: 0.6933 - val_accuracy: 0.3705\n",
      "Epoch 372/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.5269 - accuracy: 0.4466 - val_loss: 0.6841 - val_accuracy: 0.3693\n",
      "Epoch 373/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5259 - accuracy: 0.4460 - val_loss: 0.6833 - val_accuracy: 0.3709\n",
      "Epoch 374/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5252 - accuracy: 0.4471 - val_loss: 0.6868 - val_accuracy: 0.3742\n",
      "Epoch 375/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.5232 - accuracy: 0.4494 - val_loss: 0.6922 - val_accuracy: 0.3703\n",
      "Epoch 376/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5260 - accuracy: 0.4482 - val_loss: 0.6886 - val_accuracy: 0.3709\n",
      "Epoch 377/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5207 - accuracy: 0.4534 - val_loss: 0.6895 - val_accuracy: 0.3631\n",
      "Epoch 378/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5229 - accuracy: 0.4479 - val_loss: 0.6921 - val_accuracy: 0.3648\n",
      "Epoch 379/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5188 - accuracy: 0.4521 - val_loss: 0.7095 - val_accuracy: 0.3717\n",
      "Epoch 380/2500\n",
      "20056/20056 [==============================] - 1s 51us/step - loss: 0.5244 - accuracy: 0.4517 - val_loss: 0.6839 - val_accuracy: 0.3588\n",
      "Epoch 381/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5259 - accuracy: 0.4483 - val_loss: 0.6930 - val_accuracy: 0.3661\n",
      "Epoch 382/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5310 - accuracy: 0.4472 - val_loss: 0.6879 - val_accuracy: 0.3670\n",
      "Epoch 383/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5262 - accuracy: 0.4492 - val_loss: 0.6879 - val_accuracy: 0.3636\n",
      "Epoch 384/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5199 - accuracy: 0.4530 - val_loss: 0.6967 - val_accuracy: 0.3715\n",
      "Epoch 385/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5297 - accuracy: 0.4461 - val_loss: 0.6803 - val_accuracy: 0.3690\n",
      "Epoch 386/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5294 - accuracy: 0.4453 - val_loss: 0.6788 - val_accuracy: 0.3602\n",
      "Epoch 387/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5258 - accuracy: 0.4471 - val_loss: 0.6885 - val_accuracy: 0.3658\n",
      "Epoch 388/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5186 - accuracy: 0.4596 - val_loss: 0.6994 - val_accuracy: 0.3655\n",
      "Epoch 389/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5213 - accuracy: 0.4515 - val_loss: 0.6825 - val_accuracy: 0.3645\n",
      "Epoch 390/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5178 - accuracy: 0.4550 - val_loss: 0.6903 - val_accuracy: 0.3645\n",
      "Epoch 391/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5169 - accuracy: 0.4543 - val_loss: 0.6900 - val_accuracy: 0.3708\n",
      "Epoch 392/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5306 - accuracy: 0.4455 - val_loss: 0.6874 - val_accuracy: 0.3696\n",
      "Epoch 393/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5175 - accuracy: 0.4558 - val_loss: 0.6973 - val_accuracy: 0.3688\n",
      "Epoch 394/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5186 - accuracy: 0.4566 - val_loss: 0.6897 - val_accuracy: 0.3724\n",
      "Epoch 395/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5173 - accuracy: 0.4573 - val_loss: 0.6874 - val_accuracy: 0.3694\n",
      "Epoch 396/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5125 - accuracy: 0.4597 - val_loss: 0.7012 - val_accuracy: 0.3697\n",
      "Epoch 397/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5168 - accuracy: 0.4603 - val_loss: 0.6927 - val_accuracy: 0.3678\n",
      "Epoch 398/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5194 - accuracy: 0.4535 - val_loss: 0.6922 - val_accuracy: 0.3723\n",
      "Epoch 399/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5118 - accuracy: 0.4626 - val_loss: 0.7027 - val_accuracy: 0.3669\n",
      "Epoch 400/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5143 - accuracy: 0.4600 - val_loss: 0.6937 - val_accuracy: 0.3739\n",
      "Epoch 401/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5130 - accuracy: 0.4600 - val_loss: 0.6985 - val_accuracy: 0.3717\n",
      "Epoch 402/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5205 - accuracy: 0.4563 - val_loss: 0.6890 - val_accuracy: 0.3703\n",
      "Epoch 403/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5132 - accuracy: 0.4596 - val_loss: 0.7007 - val_accuracy: 0.3742\n",
      "Epoch 404/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5152 - accuracy: 0.4597 - val_loss: 0.6840 - val_accuracy: 0.3676\n",
      "Epoch 405/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5200 - accuracy: 0.4533 - val_loss: 0.6921 - val_accuracy: 0.3688\n",
      "Epoch 406/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5151 - accuracy: 0.4582 - val_loss: 0.7009 - val_accuracy: 0.3673\n",
      "Epoch 407/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5148 - accuracy: 0.4559 - val_loss: 0.6922 - val_accuracy: 0.3744\n",
      "Epoch 408/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5155 - accuracy: 0.4605 - val_loss: 0.6936 - val_accuracy: 0.3705\n",
      "Epoch 409/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5148 - accuracy: 0.4624 - val_loss: 0.6916 - val_accuracy: 0.3733\n",
      "Epoch 410/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5143 - accuracy: 0.4581 - val_loss: 0.6859 - val_accuracy: 0.3678\n",
      "Epoch 411/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5138 - accuracy: 0.4588 - val_loss: 0.6953 - val_accuracy: 0.3700\n",
      "Epoch 412/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5116 - accuracy: 0.4616 - val_loss: 0.6866 - val_accuracy: 0.3667\n",
      "Epoch 413/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5136 - accuracy: 0.4596 - val_loss: 0.6938 - val_accuracy: 0.3733\n",
      "Epoch 414/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5163 - accuracy: 0.4554 - val_loss: 0.6973 - val_accuracy: 0.3748\n",
      "Epoch 415/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.5144 - accuracy: 0.4591 - val_loss: 0.6957 - val_accuracy: 0.3633\n",
      "Epoch 416/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.5124 - accuracy: 0.4594 - val_loss: 0.6967 - val_accuracy: 0.3705\n",
      "Epoch 417/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.5105 - accuracy: 0.4615 - val_loss: 0.6992 - val_accuracy: 0.3691\n",
      "Epoch 418/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.5152 - accuracy: 0.4601 - val_loss: 0.6914 - val_accuracy: 0.3733\n",
      "Epoch 419/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.5148 - accuracy: 0.4580 - val_loss: 0.6884 - val_accuracy: 0.3691\n",
      "Epoch 420/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5145 - accuracy: 0.4573 - val_loss: 0.6953 - val_accuracy: 0.3696\n",
      "Epoch 421/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5148 - accuracy: 0.4588 - val_loss: 0.6904 - val_accuracy: 0.3742\n",
      "Epoch 422/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5146 - accuracy: 0.4579 - val_loss: 0.6904 - val_accuracy: 0.3708\n",
      "Epoch 423/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5167 - accuracy: 0.4588 - val_loss: 0.6971 - val_accuracy: 0.3620\n",
      "Epoch 424/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5121 - accuracy: 0.4583 - val_loss: 0.6982 - val_accuracy: 0.3709\n",
      "Epoch 425/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5092 - accuracy: 0.4646 - val_loss: 0.6954 - val_accuracy: 0.3658\n",
      "Epoch 426/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5100 - accuracy: 0.4649 - val_loss: 0.6942 - val_accuracy: 0.3751\n",
      "Epoch 427/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.5115 - accuracy: 0.4615 - val_loss: 0.6969 - val_accuracy: 0.3706\n",
      "Epoch 428/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5174 - accuracy: 0.4535 - val_loss: 0.6902 - val_accuracy: 0.3699\n",
      "Epoch 429/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.5152 - accuracy: 0.4563 - val_loss: 0.6896 - val_accuracy: 0.3658\n",
      "Epoch 430/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5114 - accuracy: 0.4620 - val_loss: 0.6892 - val_accuracy: 0.3669\n",
      "Epoch 431/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5109 - accuracy: 0.4597 - val_loss: 0.6903 - val_accuracy: 0.3742\n",
      "Epoch 432/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5154 - accuracy: 0.4579 - val_loss: 0.6930 - val_accuracy: 0.3714\n",
      "Epoch 433/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5105 - accuracy: 0.4635 - val_loss: 0.6971 - val_accuracy: 0.3709\n",
      "Epoch 434/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5071 - accuracy: 0.4625 - val_loss: 0.6926 - val_accuracy: 0.3721\n",
      "Epoch 435/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5131 - accuracy: 0.4625 - val_loss: 0.6994 - val_accuracy: 0.3711\n",
      "Epoch 436/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5042 - accuracy: 0.4670 - val_loss: 0.7049 - val_accuracy: 0.3717\n",
      "Epoch 437/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.5084 - accuracy: 0.4669 - val_loss: 0.6913 - val_accuracy: 0.3666\n",
      "Epoch 438/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5082 - accuracy: 0.4647 - val_loss: 0.7029 - val_accuracy: 0.3721\n",
      "Epoch 439/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5146 - accuracy: 0.4630 - val_loss: 0.6997 - val_accuracy: 0.3702\n",
      "Epoch 440/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5138 - accuracy: 0.4604 - val_loss: 0.7020 - val_accuracy: 0.3667\n",
      "Epoch 441/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5094 - accuracy: 0.4674 - val_loss: 0.6904 - val_accuracy: 0.3733\n",
      "Epoch 442/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.5081 - accuracy: 0.4653 - val_loss: 0.7009 - val_accuracy: 0.3711\n",
      "Epoch 443/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5047 - accuracy: 0.4665 - val_loss: 0.6938 - val_accuracy: 0.3631\n",
      "Epoch 444/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5087 - accuracy: 0.4632 - val_loss: 0.7019 - val_accuracy: 0.3603\n",
      "Epoch 445/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5074 - accuracy: 0.4650 - val_loss: 0.6902 - val_accuracy: 0.3699\n",
      "Epoch 446/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5030 - accuracy: 0.4663 - val_loss: 0.7103 - val_accuracy: 0.3697\n",
      "Epoch 447/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5106 - accuracy: 0.4595 - val_loss: 0.7017 - val_accuracy: 0.3721\n",
      "Epoch 448/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5059 - accuracy: 0.4663 - val_loss: 0.7003 - val_accuracy: 0.3735\n",
      "Epoch 449/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5039 - accuracy: 0.4683 - val_loss: 0.6904 - val_accuracy: 0.3718\n",
      "Epoch 450/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5080 - accuracy: 0.4643 - val_loss: 0.6920 - val_accuracy: 0.3712\n",
      "Epoch 451/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.5027 - accuracy: 0.4697 - val_loss: 0.6954 - val_accuracy: 0.3679\n",
      "Epoch 452/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5054 - accuracy: 0.4667 - val_loss: 0.6873 - val_accuracy: 0.3691\n",
      "Epoch 453/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5116 - accuracy: 0.4595 - val_loss: 0.7055 - val_accuracy: 0.3676\n",
      "Epoch 454/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5039 - accuracy: 0.4695 - val_loss: 0.6979 - val_accuracy: 0.3696\n",
      "Epoch 455/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.5021 - accuracy: 0.4698 - val_loss: 0.6935 - val_accuracy: 0.3706\n",
      "Epoch 456/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.5116 - accuracy: 0.4614 - val_loss: 0.6925 - val_accuracy: 0.3640\n",
      "Epoch 457/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.5086 - accuracy: 0.4647 - val_loss: 0.7099 - val_accuracy: 0.3663\n",
      "Epoch 458/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5062 - accuracy: 0.4691 - val_loss: 0.6985 - val_accuracy: 0.3654\n",
      "Epoch 459/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5046 - accuracy: 0.4645 - val_loss: 0.7014 - val_accuracy: 0.3691\n",
      "Epoch 460/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5053 - accuracy: 0.4664 - val_loss: 0.7077 - val_accuracy: 0.3657\n",
      "Epoch 461/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5035 - accuracy: 0.4702 - val_loss: 0.6953 - val_accuracy: 0.3667\n",
      "Epoch 462/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5011 - accuracy: 0.4724 - val_loss: 0.7043 - val_accuracy: 0.3651\n",
      "Epoch 463/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5006 - accuracy: 0.4690 - val_loss: 0.7095 - val_accuracy: 0.3697\n",
      "Epoch 464/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5055 - accuracy: 0.4704 - val_loss: 0.7042 - val_accuracy: 0.3750\n",
      "Epoch 465/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.5082 - accuracy: 0.4630 - val_loss: 0.6988 - val_accuracy: 0.3687\n",
      "Epoch 466/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.5023 - accuracy: 0.4707 - val_loss: 0.7086 - val_accuracy: 0.3709\n",
      "Epoch 467/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.5045 - accuracy: 0.4660 - val_loss: 0.7161 - val_accuracy: 0.3712\n",
      "Epoch 468/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5032 - accuracy: 0.4715 - val_loss: 0.7048 - val_accuracy: 0.3712\n",
      "Epoch 469/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5028 - accuracy: 0.4740 - val_loss: 0.6935 - val_accuracy: 0.3639\n",
      "Epoch 470/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.5049 - accuracy: 0.4661 - val_loss: 0.7103 - val_accuracy: 0.3706\n",
      "Epoch 471/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5015 - accuracy: 0.4704 - val_loss: 0.6981 - val_accuracy: 0.3741\n",
      "Epoch 472/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4947 - accuracy: 0.4773 - val_loss: 0.7023 - val_accuracy: 0.3690\n",
      "Epoch 473/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5010 - accuracy: 0.4693 - val_loss: 0.7172 - val_accuracy: 0.3714\n",
      "Epoch 474/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5002 - accuracy: 0.4739 - val_loss: 0.6958 - val_accuracy: 0.3720\n",
      "Epoch 475/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5011 - accuracy: 0.4720 - val_loss: 0.7000 - val_accuracy: 0.3727\n",
      "Epoch 476/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5024 - accuracy: 0.4695 - val_loss: 0.6978 - val_accuracy: 0.3666\n",
      "Epoch 477/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5049 - accuracy: 0.4678 - val_loss: 0.7010 - val_accuracy: 0.3693\n",
      "Epoch 478/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5000 - accuracy: 0.4738 - val_loss: 0.7016 - val_accuracy: 0.3717\n",
      "Epoch 479/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5022 - accuracy: 0.4682 - val_loss: 0.7047 - val_accuracy: 0.3702\n",
      "Epoch 480/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5009 - accuracy: 0.4698 - val_loss: 0.7056 - val_accuracy: 0.3694\n",
      "Epoch 481/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4981 - accuracy: 0.4716 - val_loss: 0.7015 - val_accuracy: 0.3693\n",
      "Epoch 482/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4993 - accuracy: 0.4758 - val_loss: 0.7049 - val_accuracy: 0.3661\n",
      "Epoch 483/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4998 - accuracy: 0.4697 - val_loss: 0.7018 - val_accuracy: 0.3620\n",
      "Epoch 484/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4991 - accuracy: 0.4725 - val_loss: 0.6946 - val_accuracy: 0.3684\n",
      "Epoch 485/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5142 - accuracy: 0.4591 - val_loss: 0.6923 - val_accuracy: 0.3697\n",
      "Epoch 486/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5049 - accuracy: 0.4693 - val_loss: 0.6992 - val_accuracy: 0.3697\n",
      "Epoch 487/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4989 - accuracy: 0.4736 - val_loss: 0.6911 - val_accuracy: 0.3703\n",
      "Epoch 488/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5014 - accuracy: 0.4700 - val_loss: 0.6961 - val_accuracy: 0.3714\n",
      "Epoch 489/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4963 - accuracy: 0.4784 - val_loss: 0.7085 - val_accuracy: 0.3727\n",
      "Epoch 490/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5025 - accuracy: 0.4709 - val_loss: 0.7087 - val_accuracy: 0.3678\n",
      "Epoch 491/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5144 - accuracy: 0.4625 - val_loss: 0.6921 - val_accuracy: 0.3693\n",
      "Epoch 492/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5123 - accuracy: 0.4572 - val_loss: 0.6834 - val_accuracy: 0.3652\n",
      "Epoch 493/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5288 - accuracy: 0.4652 - val_loss: 0.6958 - val_accuracy: 0.3694\n",
      "Epoch 494/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.5036 - accuracy: 0.4714 - val_loss: 0.6947 - val_accuracy: 0.3655\n",
      "Epoch 495/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4965 - accuracy: 0.4747 - val_loss: 0.6955 - val_accuracy: 0.3729\n",
      "Epoch 496/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4920 - accuracy: 0.4774 - val_loss: 0.7062 - val_accuracy: 0.3727\n",
      "Epoch 497/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4997 - accuracy: 0.4705 - val_loss: 0.6978 - val_accuracy: 0.3682\n",
      "Epoch 498/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.5011 - accuracy: 0.4697 - val_loss: 0.7029 - val_accuracy: 0.3663\n",
      "Epoch 499/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5029 - accuracy: 0.4722 - val_loss: 0.6936 - val_accuracy: 0.3637\n",
      "Epoch 500/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5006 - accuracy: 0.4707 - val_loss: 0.6921 - val_accuracy: 0.3640\n",
      "Epoch 501/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4919 - accuracy: 0.4813 - val_loss: 0.7008 - val_accuracy: 0.3699\n",
      "Epoch 502/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4967 - accuracy: 0.4756 - val_loss: 0.6929 - val_accuracy: 0.3697\n",
      "Epoch 503/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4973 - accuracy: 0.4761 - val_loss: 0.7056 - val_accuracy: 0.3670\n",
      "Epoch 504/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4992 - accuracy: 0.4713 - val_loss: 0.6989 - val_accuracy: 0.3711\n",
      "Epoch 505/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.5012 - accuracy: 0.4685 - val_loss: 0.7045 - val_accuracy: 0.3684\n",
      "Epoch 506/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.5119 - accuracy: 0.4629 - val_loss: 0.6996 - val_accuracy: 0.3687\n",
      "Epoch 507/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4964 - accuracy: 0.4752 - val_loss: 0.7044 - val_accuracy: 0.3723\n",
      "Epoch 508/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4986 - accuracy: 0.4733 - val_loss: 0.6948 - val_accuracy: 0.3681\n",
      "Epoch 509/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.5005 - accuracy: 0.4707 - val_loss: 0.7048 - val_accuracy: 0.3705\n",
      "Epoch 510/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4931 - accuracy: 0.4786 - val_loss: 0.6914 - val_accuracy: 0.3690\n",
      "Epoch 511/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4964 - accuracy: 0.4742 - val_loss: 0.7035 - val_accuracy: 0.3726\n",
      "Epoch 512/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4896 - accuracy: 0.4785 - val_loss: 0.7069 - val_accuracy: 0.3726\n",
      "Epoch 513/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4947 - accuracy: 0.4765 - val_loss: 0.6947 - val_accuracy: 0.3640\n",
      "Epoch 514/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.5029 - accuracy: 0.4696 - val_loss: 0.6996 - val_accuracy: 0.3696\n",
      "Epoch 515/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4926 - accuracy: 0.4791 - val_loss: 0.7087 - val_accuracy: 0.3675\n",
      "Epoch 516/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4990 - accuracy: 0.4698 - val_loss: 0.6967 - val_accuracy: 0.3705\n",
      "Epoch 517/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4942 - accuracy: 0.4727 - val_loss: 0.7036 - val_accuracy: 0.3708\n",
      "Epoch 518/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4979 - accuracy: 0.4725 - val_loss: 0.7007 - val_accuracy: 0.3673\n",
      "Epoch 519/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4915 - accuracy: 0.4765 - val_loss: 0.7080 - val_accuracy: 0.3676\n",
      "Epoch 520/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4924 - accuracy: 0.4810 - val_loss: 0.7138 - val_accuracy: 0.3733\n",
      "Epoch 521/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4891 - accuracy: 0.4798 - val_loss: 0.7084 - val_accuracy: 0.3690\n",
      "Epoch 522/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4931 - accuracy: 0.4775 - val_loss: 0.7141 - val_accuracy: 0.3634\n",
      "Epoch 523/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4941 - accuracy: 0.4781 - val_loss: 0.6932 - val_accuracy: 0.3669\n",
      "Epoch 524/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4951 - accuracy: 0.4779 - val_loss: 0.7052 - val_accuracy: 0.3760\n",
      "Epoch 525/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4933 - accuracy: 0.4813 - val_loss: 0.6982 - val_accuracy: 0.3727\n",
      "Epoch 526/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4889 - accuracy: 0.4799 - val_loss: 0.7049 - val_accuracy: 0.3771\n",
      "Epoch 527/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4893 - accuracy: 0.4824 - val_loss: 0.7040 - val_accuracy: 0.3691\n",
      "Epoch 528/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4930 - accuracy: 0.4789 - val_loss: 0.7159 - val_accuracy: 0.3760\n",
      "Epoch 529/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4895 - accuracy: 0.4824 - val_loss: 0.7125 - val_accuracy: 0.3669\n",
      "Epoch 530/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4908 - accuracy: 0.4852 - val_loss: 0.7044 - val_accuracy: 0.3712\n",
      "Epoch 531/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4914 - accuracy: 0.4786 - val_loss: 0.7031 - val_accuracy: 0.3711\n",
      "Epoch 532/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4919 - accuracy: 0.4798 - val_loss: 0.6922 - val_accuracy: 0.3675\n",
      "Epoch 533/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4891 - accuracy: 0.4803 - val_loss: 0.7157 - val_accuracy: 0.3700\n",
      "Epoch 534/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4951 - accuracy: 0.4770 - val_loss: 0.6991 - val_accuracy: 0.3657\n",
      "Epoch 535/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4834 - accuracy: 0.4846 - val_loss: 0.7127 - val_accuracy: 0.3726\n",
      "Epoch 536/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4905 - accuracy: 0.4793 - val_loss: 0.7085 - val_accuracy: 0.3747\n",
      "Epoch 537/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4940 - accuracy: 0.4790 - val_loss: 0.7047 - val_accuracy: 0.3694\n",
      "Epoch 538/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4865 - accuracy: 0.4843 - val_loss: 0.7179 - val_accuracy: 0.3729\n",
      "Epoch 539/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4860 - accuracy: 0.4826 - val_loss: 0.7024 - val_accuracy: 0.3697\n",
      "Epoch 540/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4905 - accuracy: 0.4801 - val_loss: 0.7003 - val_accuracy: 0.3715\n",
      "Epoch 541/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4852 - accuracy: 0.4807 - val_loss: 0.7041 - val_accuracy: 0.3678\n",
      "Epoch 542/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4842 - accuracy: 0.4867 - val_loss: 0.7069 - val_accuracy: 0.3691\n",
      "Epoch 543/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4855 - accuracy: 0.4844 - val_loss: 0.7128 - val_accuracy: 0.3753\n",
      "Epoch 544/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4845 - accuracy: 0.4855 - val_loss: 0.7142 - val_accuracy: 0.3727\n",
      "Epoch 545/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4851 - accuracy: 0.4852 - val_loss: 0.7055 - val_accuracy: 0.3735\n",
      "Epoch 546/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4818 - accuracy: 0.4855 - val_loss: 0.7082 - val_accuracy: 0.3699\n",
      "Epoch 547/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4889 - accuracy: 0.4813 - val_loss: 0.7244 - val_accuracy: 0.3732\n",
      "Epoch 548/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4819 - accuracy: 0.4884 - val_loss: 0.7210 - val_accuracy: 0.3712\n",
      "Epoch 549/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4883 - accuracy: 0.4821 - val_loss: 0.7146 - val_accuracy: 0.3711\n",
      "Epoch 550/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4871 - accuracy: 0.4833 - val_loss: 0.7072 - val_accuracy: 0.3751\n",
      "Epoch 551/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4938 - accuracy: 0.4782 - val_loss: 0.7052 - val_accuracy: 0.3723\n",
      "Epoch 552/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4854 - accuracy: 0.4834 - val_loss: 0.7081 - val_accuracy: 0.3727\n",
      "Epoch 553/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4857 - accuracy: 0.4821 - val_loss: 0.7120 - val_accuracy: 0.3783\n",
      "Epoch 554/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4859 - accuracy: 0.4844 - val_loss: 0.7101 - val_accuracy: 0.3676\n",
      "Epoch 555/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4900 - accuracy: 0.4798 - val_loss: 0.6976 - val_accuracy: 0.3682\n",
      "Epoch 556/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4884 - accuracy: 0.4807 - val_loss: 0.7008 - val_accuracy: 0.3724\n",
      "Epoch 557/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4837 - accuracy: 0.4842 - val_loss: 0.7173 - val_accuracy: 0.3765\n",
      "Epoch 558/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4847 - accuracy: 0.4851 - val_loss: 0.7077 - val_accuracy: 0.3661\n",
      "Epoch 559/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4828 - accuracy: 0.4884 - val_loss: 0.7078 - val_accuracy: 0.3681\n",
      "Epoch 560/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4854 - accuracy: 0.4843 - val_loss: 0.7116 - val_accuracy: 0.3646\n",
      "Epoch 561/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4849 - accuracy: 0.4848 - val_loss: 0.6985 - val_accuracy: 0.3691\n",
      "Epoch 562/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4850 - accuracy: 0.4816 - val_loss: 0.7032 - val_accuracy: 0.3756\n",
      "Epoch 563/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4804 - accuracy: 0.4880 - val_loss: 0.7199 - val_accuracy: 0.3718\n",
      "Epoch 564/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4889 - accuracy: 0.4809 - val_loss: 0.7109 - val_accuracy: 0.3667\n",
      "Epoch 565/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4844 - accuracy: 0.4874 - val_loss: 0.7119 - val_accuracy: 0.3672\n",
      "Epoch 566/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4863 - accuracy: 0.4839 - val_loss: 0.7020 - val_accuracy: 0.3750\n",
      "Epoch 567/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4872 - accuracy: 0.4840 - val_loss: 0.7121 - val_accuracy: 0.3729\n",
      "Epoch 568/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4865 - accuracy: 0.4860 - val_loss: 0.6927 - val_accuracy: 0.3681\n",
      "Epoch 569/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4963 - accuracy: 0.4805 - val_loss: 0.6908 - val_accuracy: 0.3687\n",
      "Epoch 570/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4929 - accuracy: 0.4780 - val_loss: 0.7145 - val_accuracy: 0.3684\n",
      "Epoch 571/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4853 - accuracy: 0.4860 - val_loss: 0.7133 - val_accuracy: 0.3780\n",
      "Epoch 572/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4843 - accuracy: 0.4892 - val_loss: 0.7020 - val_accuracy: 0.3721\n",
      "Epoch 573/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4829 - accuracy: 0.4843 - val_loss: 0.7170 - val_accuracy: 0.3718\n",
      "Epoch 574/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4862 - accuracy: 0.4877 - val_loss: 0.7116 - val_accuracy: 0.3729\n",
      "Epoch 575/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4827 - accuracy: 0.4905 - val_loss: 0.7019 - val_accuracy: 0.3720\n",
      "Epoch 576/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4916 - accuracy: 0.4822 - val_loss: 0.6996 - val_accuracy: 0.3684\n",
      "Epoch 577/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4842 - accuracy: 0.4893 - val_loss: 0.7055 - val_accuracy: 0.3661\n",
      "Epoch 578/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4862 - accuracy: 0.4862 - val_loss: 0.7057 - val_accuracy: 0.3724\n",
      "Epoch 579/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4854 - accuracy: 0.4874 - val_loss: 0.6997 - val_accuracy: 0.3700\n",
      "Epoch 580/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.4899 - accuracy: 0.4824 - val_loss: 0.6983 - val_accuracy: 0.3712\n",
      "Epoch 581/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4968 - accuracy: 0.4781 - val_loss: 0.7055 - val_accuracy: 0.3678\n",
      "Epoch 582/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4944 - accuracy: 0.4776 - val_loss: 0.7031 - val_accuracy: 0.3718\n",
      "Epoch 583/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4841 - accuracy: 0.4868 - val_loss: 0.7073 - val_accuracy: 0.3742\n",
      "Epoch 584/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4875 - accuracy: 0.4827 - val_loss: 0.6958 - val_accuracy: 0.3691\n",
      "Epoch 585/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4879 - accuracy: 0.4818 - val_loss: 0.7040 - val_accuracy: 0.3724\n",
      "Epoch 586/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4830 - accuracy: 0.4857 - val_loss: 0.7049 - val_accuracy: 0.3730\n",
      "Epoch 587/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4851 - accuracy: 0.4852 - val_loss: 0.7123 - val_accuracy: 0.3765\n",
      "Epoch 588/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4798 - accuracy: 0.4947 - val_loss: 0.7097 - val_accuracy: 0.3720\n",
      "Epoch 589/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4766 - accuracy: 0.4922 - val_loss: 0.7102 - val_accuracy: 0.3723\n",
      "Epoch 590/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4773 - accuracy: 0.4940 - val_loss: 0.7080 - val_accuracy: 0.3715\n",
      "Epoch 591/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4799 - accuracy: 0.4895 - val_loss: 0.7055 - val_accuracy: 0.3714\n",
      "Epoch 592/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4836 - accuracy: 0.4878 - val_loss: 0.7001 - val_accuracy: 0.3637\n",
      "Epoch 593/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4822 - accuracy: 0.4877 - val_loss: 0.7008 - val_accuracy: 0.3675\n",
      "Epoch 594/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4841 - accuracy: 0.4847 - val_loss: 0.7065 - val_accuracy: 0.3648\n",
      "Epoch 595/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4755 - accuracy: 0.4924 - val_loss: 0.7132 - val_accuracy: 0.3690\n",
      "Epoch 596/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4885 - accuracy: 0.4836 - val_loss: 0.7130 - val_accuracy: 0.3693\n",
      "Epoch 597/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4842 - accuracy: 0.4871 - val_loss: 0.7134 - val_accuracy: 0.3735\n",
      "Epoch 598/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4773 - accuracy: 0.4936 - val_loss: 0.7193 - val_accuracy: 0.3700\n",
      "Epoch 599/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4777 - accuracy: 0.4945 - val_loss: 0.7021 - val_accuracy: 0.3729\n",
      "Epoch 600/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4798 - accuracy: 0.4897 - val_loss: 0.7085 - val_accuracy: 0.3724\n",
      "Epoch 601/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4784 - accuracy: 0.4923 - val_loss: 0.7005 - val_accuracy: 0.3645\n",
      "Epoch 602/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4754 - accuracy: 0.4941 - val_loss: 0.7264 - val_accuracy: 0.3691\n",
      "Epoch 603/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4969 - accuracy: 0.4825 - val_loss: 0.7008 - val_accuracy: 0.3682\n",
      "Epoch 604/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4856 - accuracy: 0.4847 - val_loss: 0.7005 - val_accuracy: 0.3721\n",
      "Epoch 605/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4796 - accuracy: 0.4882 - val_loss: 0.7106 - val_accuracy: 0.3732\n",
      "Epoch 606/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4774 - accuracy: 0.4912 - val_loss: 0.7196 - val_accuracy: 0.3759\n",
      "Epoch 607/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4792 - accuracy: 0.4897 - val_loss: 0.7152 - val_accuracy: 0.3732\n",
      "Epoch 608/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4823 - accuracy: 0.4903 - val_loss: 0.7047 - val_accuracy: 0.3706\n",
      "Epoch 609/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4768 - accuracy: 0.4965 - val_loss: 0.7021 - val_accuracy: 0.3690\n",
      "Epoch 610/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4793 - accuracy: 0.4920 - val_loss: 0.7053 - val_accuracy: 0.3678\n",
      "Epoch 611/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4745 - accuracy: 0.4948 - val_loss: 0.7092 - val_accuracy: 0.3720\n",
      "Epoch 612/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4740 - accuracy: 0.4980 - val_loss: 0.7107 - val_accuracy: 0.3757\n",
      "Epoch 613/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4789 - accuracy: 0.4939 - val_loss: 0.7187 - val_accuracy: 0.3658\n",
      "Epoch 614/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4808 - accuracy: 0.4930 - val_loss: 0.7140 - val_accuracy: 0.3688\n",
      "Epoch 615/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4786 - accuracy: 0.4888 - val_loss: 0.7057 - val_accuracy: 0.3712\n",
      "Epoch 616/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4755 - accuracy: 0.4944 - val_loss: 0.7115 - val_accuracy: 0.3741\n",
      "Epoch 617/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4754 - accuracy: 0.4925 - val_loss: 0.7008 - val_accuracy: 0.3706\n",
      "Epoch 618/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4800 - accuracy: 0.4899 - val_loss: 0.7068 - val_accuracy: 0.3703\n",
      "Epoch 619/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4759 - accuracy: 0.4962 - val_loss: 0.7069 - val_accuracy: 0.3754\n",
      "Epoch 620/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4764 - accuracy: 0.4932 - val_loss: 0.7064 - val_accuracy: 0.3744\n",
      "Epoch 621/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4721 - accuracy: 0.4991 - val_loss: 0.7164 - val_accuracy: 0.3714\n",
      "Epoch 622/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4744 - accuracy: 0.4954 - val_loss: 0.7056 - val_accuracy: 0.3699\n",
      "Epoch 623/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4752 - accuracy: 0.4979 - val_loss: 0.7147 - val_accuracy: 0.3667\n",
      "Epoch 624/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4729 - accuracy: 0.4991 - val_loss: 0.7092 - val_accuracy: 0.3708\n",
      "Epoch 625/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4782 - accuracy: 0.4935 - val_loss: 0.7076 - val_accuracy: 0.3738\n",
      "Epoch 626/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4775 - accuracy: 0.4949 - val_loss: 0.7173 - val_accuracy: 0.3717\n",
      "Epoch 627/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4769 - accuracy: 0.4954 - val_loss: 0.7116 - val_accuracy: 0.3747\n",
      "Epoch 628/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4777 - accuracy: 0.4946 - val_loss: 0.7093 - val_accuracy: 0.3741\n",
      "Epoch 629/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4727 - accuracy: 0.4995 - val_loss: 0.7212 - val_accuracy: 0.3735\n",
      "Epoch 630/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4761 - accuracy: 0.4969 - val_loss: 0.7064 - val_accuracy: 0.3691\n",
      "Epoch 631/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4727 - accuracy: 0.4940 - val_loss: 0.7173 - val_accuracy: 0.3718\n",
      "Epoch 632/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4747 - accuracy: 0.4976 - val_loss: 0.7127 - val_accuracy: 0.3681\n",
      "Epoch 633/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4769 - accuracy: 0.4941 - val_loss: 0.6998 - val_accuracy: 0.3729\n",
      "Epoch 634/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4723 - accuracy: 0.4963 - val_loss: 0.7321 - val_accuracy: 0.3808\n",
      "Epoch 635/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4770 - accuracy: 0.4956 - val_loss: 0.7078 - val_accuracy: 0.3775\n",
      "Epoch 636/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4752 - accuracy: 0.4962 - val_loss: 0.7120 - val_accuracy: 0.3745\n",
      "Epoch 637/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4763 - accuracy: 0.4971 - val_loss: 0.7042 - val_accuracy: 0.3729\n",
      "Epoch 638/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4770 - accuracy: 0.4945 - val_loss: 0.7149 - val_accuracy: 0.3757\n",
      "Epoch 639/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4748 - accuracy: 0.4978 - val_loss: 0.7147 - val_accuracy: 0.3751\n",
      "Epoch 640/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4745 - accuracy: 0.4973 - val_loss: 0.7218 - val_accuracy: 0.3760\n",
      "Epoch 641/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4740 - accuracy: 0.5008 - val_loss: 0.7097 - val_accuracy: 0.3738\n",
      "Epoch 642/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4682 - accuracy: 0.4990 - val_loss: 0.7127 - val_accuracy: 0.3748\n",
      "Epoch 643/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4716 - accuracy: 0.4948 - val_loss: 0.7155 - val_accuracy: 0.3751\n",
      "Epoch 644/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4710 - accuracy: 0.4995 - val_loss: 0.7117 - val_accuracy: 0.3712\n",
      "Epoch 645/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4711 - accuracy: 0.4973 - val_loss: 0.7082 - val_accuracy: 0.3702\n",
      "Epoch 646/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4647 - accuracy: 0.5050 - val_loss: 0.7158 - val_accuracy: 0.3721\n",
      "Epoch 647/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4737 - accuracy: 0.4951 - val_loss: 0.7093 - val_accuracy: 0.3673\n",
      "Epoch 648/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4748 - accuracy: 0.4919 - val_loss: 0.7223 - val_accuracy: 0.3691\n",
      "Epoch 649/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4747 - accuracy: 0.4935 - val_loss: 0.7185 - val_accuracy: 0.3711\n",
      "Epoch 650/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4687 - accuracy: 0.5008 - val_loss: 0.7169 - val_accuracy: 0.3735\n",
      "Epoch 651/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4720 - accuracy: 0.4984 - val_loss: 0.7151 - val_accuracy: 0.3724\n",
      "Epoch 652/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4720 - accuracy: 0.4924 - val_loss: 0.7088 - val_accuracy: 0.3691\n",
      "Epoch 653/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4686 - accuracy: 0.4984 - val_loss: 0.7140 - val_accuracy: 0.3697\n",
      "Epoch 654/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4745 - accuracy: 0.4926 - val_loss: 0.7138 - val_accuracy: 0.3694\n",
      "Epoch 655/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4716 - accuracy: 0.4982 - val_loss: 0.7257 - val_accuracy: 0.3747\n",
      "Epoch 656/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4707 - accuracy: 0.4970 - val_loss: 0.7162 - val_accuracy: 0.3702\n",
      "Epoch 657/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4719 - accuracy: 0.4972 - val_loss: 0.7165 - val_accuracy: 0.3718\n",
      "Epoch 658/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4670 - accuracy: 0.5026 - val_loss: 0.7294 - val_accuracy: 0.3727\n",
      "Epoch 659/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4715 - accuracy: 0.4975 - val_loss: 0.7224 - val_accuracy: 0.3757\n",
      "Epoch 660/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4695 - accuracy: 0.4999 - val_loss: 0.7210 - val_accuracy: 0.3729\n",
      "Epoch 661/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4695 - accuracy: 0.4989 - val_loss: 0.7156 - val_accuracy: 0.3735\n",
      "Epoch 662/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4741 - accuracy: 0.4999 - val_loss: 0.7069 - val_accuracy: 0.3738\n",
      "Epoch 663/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4680 - accuracy: 0.5034 - val_loss: 0.7170 - val_accuracy: 0.3706\n",
      "Epoch 664/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4750 - accuracy: 0.4936 - val_loss: 0.7099 - val_accuracy: 0.3772\n",
      "Epoch 665/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4740 - accuracy: 0.4949 - val_loss: 0.7133 - val_accuracy: 0.3780\n",
      "Epoch 666/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4755 - accuracy: 0.4934 - val_loss: 0.7084 - val_accuracy: 0.3702\n",
      "Epoch 667/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4719 - accuracy: 0.4967 - val_loss: 0.7229 - val_accuracy: 0.3688\n",
      "Epoch 668/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4725 - accuracy: 0.4989 - val_loss: 0.7189 - val_accuracy: 0.3703- loss: 0.4661 - \n",
      "Epoch 669/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4704 - accuracy: 0.4995 - val_loss: 0.7206 - val_accuracy: 0.3730\n",
      "Epoch 670/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4688 - accuracy: 0.5013 - val_loss: 0.7220 - val_accuracy: 0.3720\n",
      "Epoch 671/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4699 - accuracy: 0.4960 - val_loss: 0.7232 - val_accuracy: 0.3742\n",
      "Epoch 672/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4729 - accuracy: 0.4994 - val_loss: 0.7142 - val_accuracy: 0.3691\n",
      "Epoch 673/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4700 - accuracy: 0.5014 - val_loss: 0.7177 - val_accuracy: 0.3667\n",
      "Epoch 674/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4681 - accuracy: 0.4983 - val_loss: 0.7219 - val_accuracy: 0.3721\n",
      "Epoch 675/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4678 - accuracy: 0.5057 - val_loss: 0.7119 - val_accuracy: 0.3697\n",
      "Epoch 676/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4672 - accuracy: 0.5016 - val_loss: 0.7236 - val_accuracy: 0.3732\n",
      "Epoch 677/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4695 - accuracy: 0.5008 - val_loss: 0.7267 - val_accuracy: 0.3717\n",
      "Epoch 678/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4767 - accuracy: 0.4970 - val_loss: 0.7122 - val_accuracy: 0.3693\n",
      "Epoch 679/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4705 - accuracy: 0.4968 - val_loss: 0.7132 - val_accuracy: 0.3693\n",
      "Epoch 680/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4723 - accuracy: 0.4945 - val_loss: 0.7268 - val_accuracy: 0.3681\n",
      "Epoch 681/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4694 - accuracy: 0.4998 - val_loss: 0.7173 - val_accuracy: 0.3687\n",
      "Epoch 682/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4726 - accuracy: 0.4990 - val_loss: 0.7170 - val_accuracy: 0.3687\n",
      "Epoch 683/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4692 - accuracy: 0.5019 - val_loss: 0.7175 - val_accuracy: 0.3675\n",
      "Epoch 684/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4730 - accuracy: 0.4972 - val_loss: 0.7187 - val_accuracy: 0.3699\n",
      "Epoch 685/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4686 - accuracy: 0.4995 - val_loss: 0.7188 - val_accuracy: 0.3730\n",
      "Epoch 686/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4721 - accuracy: 0.4986 - val_loss: 0.7193 - val_accuracy: 0.3705\n",
      "Epoch 687/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4721 - accuracy: 0.4973 - val_loss: 0.7123 - val_accuracy: 0.3738\n",
      "Epoch 688/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4699 - accuracy: 0.5031 - val_loss: 0.7116 - val_accuracy: 0.3751\n",
      "Epoch 689/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4672 - accuracy: 0.5044 - val_loss: 0.7190 - val_accuracy: 0.3736\n",
      "Epoch 690/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4694 - accuracy: 0.5015 - val_loss: 0.7203 - val_accuracy: 0.3687\n",
      "Epoch 691/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4684 - accuracy: 0.5040 - val_loss: 0.7258 - val_accuracy: 0.3720\n",
      "Epoch 692/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4711 - accuracy: 0.5011 - val_loss: 0.7108 - val_accuracy: 0.3711\n",
      "Epoch 693/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4695 - accuracy: 0.4992 - val_loss: 0.7209 - val_accuracy: 0.3729\n",
      "Epoch 694/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4649 - accuracy: 0.5041 - val_loss: 0.7174 - val_accuracy: 0.3748\n",
      "Epoch 695/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4679 - accuracy: 0.5022 - val_loss: 0.7211 - val_accuracy: 0.3708\n",
      "Epoch 696/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4744 - accuracy: 0.4977 - val_loss: 0.7135 - val_accuracy: 0.3732\n",
      "Epoch 697/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4714 - accuracy: 0.4953 - val_loss: 0.7332 - val_accuracy: 0.3745\n",
      "Epoch 698/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4689 - accuracy: 0.5046 - val_loss: 0.7144 - val_accuracy: 0.3724\n",
      "Epoch 699/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4660 - accuracy: 0.5052 - val_loss: 0.7216 - val_accuracy: 0.3735\n",
      "Epoch 700/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4790 - accuracy: 0.4956 - val_loss: 0.7267 - val_accuracy: 0.3765\n",
      "Epoch 701/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4652 - accuracy: 0.5076 - val_loss: 0.7093 - val_accuracy: 0.3753\n",
      "Epoch 702/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4628 - accuracy: 0.5081 - val_loss: 0.7151 - val_accuracy: 0.3759\n",
      "Epoch 703/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4634 - accuracy: 0.5037 - val_loss: 0.7234 - val_accuracy: 0.3718\n",
      "Epoch 704/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4795 - accuracy: 0.4911 - val_loss: 0.7034 - val_accuracy: 0.3672\n",
      "Epoch 705/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4688 - accuracy: 0.5013 - val_loss: 0.7109 - val_accuracy: 0.3709\n",
      "Epoch 706/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4733 - accuracy: 0.5018 - val_loss: 0.7048 - val_accuracy: 0.3691\n",
      "Epoch 707/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4669 - accuracy: 0.5039 - val_loss: 0.7110 - val_accuracy: 0.3732\n",
      "Epoch 708/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4725 - accuracy: 0.5005 - val_loss: 0.7070 - val_accuracy: 0.3735\n",
      "Epoch 709/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4686 - accuracy: 0.5032 - val_loss: 0.7202 - val_accuracy: 0.3721\n",
      "Epoch 710/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4600 - accuracy: 0.5098 - val_loss: 0.7280 - val_accuracy: 0.3741\n",
      "Epoch 711/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4750 - accuracy: 0.5010 - val_loss: 0.7175 - val_accuracy: 0.3747\n",
      "Epoch 712/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4733 - accuracy: 0.4990 - val_loss: 0.7203 - val_accuracy: 0.3762\n",
      "Epoch 713/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4646 - accuracy: 0.5077 - val_loss: 0.7228 - val_accuracy: 0.3727\n",
      "Epoch 714/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4651 - accuracy: 0.5043 - val_loss: 0.7239 - val_accuracy: 0.3784\n",
      "Epoch 715/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4641 - accuracy: 0.5042 - val_loss: 0.7109 - val_accuracy: 0.3759\n",
      "Epoch 716/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4659 - accuracy: 0.5004 - val_loss: 0.7059 - val_accuracy: 0.3687\n",
      "Epoch 717/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4611 - accuracy: 0.5045 - val_loss: 0.7170 - val_accuracy: 0.3778\n",
      "Epoch 718/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4705 - accuracy: 0.4995 - val_loss: 0.7177 - val_accuracy: 0.3759\n",
      "Epoch 719/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4654 - accuracy: 0.5052 - val_loss: 0.7215 - val_accuracy: 0.3729\n",
      "Epoch 720/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4654 - accuracy: 0.5079 - val_loss: 0.7078 - val_accuracy: 0.3648\n",
      "Epoch 721/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4565 - accuracy: 0.5088 - val_loss: 0.7132 - val_accuracy: 0.3739\n",
      "Epoch 722/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4598 - accuracy: 0.5118 - val_loss: 0.7120 - val_accuracy: 0.3726\n",
      "Epoch 723/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4579 - accuracy: 0.5082 - val_loss: 0.7203 - val_accuracy: 0.3738\n",
      "Epoch 724/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4587 - accuracy: 0.5108 - val_loss: 0.7294 - val_accuracy: 0.3705\n",
      "Epoch 725/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4611 - accuracy: 0.5048 - val_loss: 0.7174 - val_accuracy: 0.3700\n",
      "Epoch 726/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4590 - accuracy: 0.5083 - val_loss: 0.7215 - val_accuracy: 0.3708\n",
      "Epoch 727/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4665 - accuracy: 0.5029 - val_loss: 0.7264 - val_accuracy: 0.3651\n",
      "Epoch 728/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4657 - accuracy: 0.4985 - val_loss: 0.7112 - val_accuracy: 0.3735\n",
      "Epoch 729/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4626 - accuracy: 0.5085 - val_loss: 0.7134 - val_accuracy: 0.3748\n",
      "Epoch 730/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.4651 - accuracy: 0.5048 - val_loss: 0.7174 - val_accuracy: 0.3762\n",
      "Epoch 731/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4598 - accuracy: 0.5110 - val_loss: 0.7223 - val_accuracy: 0.3780\n",
      "Epoch 732/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4586 - accuracy: 0.5126 - val_loss: 0.7177 - val_accuracy: 0.3790\n",
      "Epoch 733/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4615 - accuracy: 0.5074 - val_loss: 0.7182 - val_accuracy: 0.3726\n",
      "Epoch 734/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4710 - accuracy: 0.4996 - val_loss: 0.7214 - val_accuracy: 0.3724\n",
      "Epoch 735/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4618 - accuracy: 0.5081 - val_loss: 0.7156 - val_accuracy: 0.3681\n",
      "Epoch 736/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4602 - accuracy: 0.5112 - val_loss: 0.7216 - val_accuracy: 0.3762\n",
      "Epoch 737/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4609 - accuracy: 0.5102 - val_loss: 0.7203 - val_accuracy: 0.3678\n",
      "Epoch 738/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4581 - accuracy: 0.5149 - val_loss: 0.7315 - val_accuracy: 0.3739\n",
      "Epoch 739/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4613 - accuracy: 0.5075 - val_loss: 0.7149 - val_accuracy: 0.3694\n",
      "Epoch 740/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4606 - accuracy: 0.5121 - val_loss: 0.7167 - val_accuracy: 0.3742\n",
      "Epoch 741/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4609 - accuracy: 0.5131 - val_loss: 0.7192 - val_accuracy: 0.3715\n",
      "Epoch 742/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4603 - accuracy: 0.5115 - val_loss: 0.7256 - val_accuracy: 0.3688\n",
      "Epoch 743/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4568 - accuracy: 0.5159 - val_loss: 0.7290 - val_accuracy: 0.3693\n",
      "Epoch 744/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4601 - accuracy: 0.5094 - val_loss: 0.7356 - val_accuracy: 0.3762\n",
      "Epoch 745/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4624 - accuracy: 0.5046 - val_loss: 0.7241 - val_accuracy: 0.3747\n",
      "Epoch 746/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4556 - accuracy: 0.5138 - val_loss: 0.7215 - val_accuracy: 0.3712\n",
      "Epoch 747/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4618 - accuracy: 0.5101 - val_loss: 0.7264 - val_accuracy: 0.3735\n",
      "Epoch 748/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4601 - accuracy: 0.5093 - val_loss: 0.7183 - val_accuracy: 0.3729\n",
      "Epoch 749/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4599 - accuracy: 0.5107 - val_loss: 0.7243 - val_accuracy: 0.3714\n",
      "Epoch 750/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4570 - accuracy: 0.5133 - val_loss: 0.7216 - val_accuracy: 0.3726\n",
      "Epoch 751/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4596 - accuracy: 0.5115 - val_loss: 0.7197 - val_accuracy: 0.3681\n",
      "Epoch 752/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4526 - accuracy: 0.5147 - val_loss: 0.7193 - val_accuracy: 0.3715\n",
      "Epoch 753/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4593 - accuracy: 0.5087 - val_loss: 0.7234 - val_accuracy: 0.3742\n",
      "Epoch 754/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4567 - accuracy: 0.5138 - val_loss: 0.7210 - val_accuracy: 0.3714\n",
      "Epoch 755/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4568 - accuracy: 0.5115 - val_loss: 0.7202 - val_accuracy: 0.3720\n",
      "Epoch 756/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4576 - accuracy: 0.5100 - val_loss: 0.7256 - val_accuracy: 0.3712\n",
      "Epoch 757/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4571 - accuracy: 0.5129 - val_loss: 0.7176 - val_accuracy: 0.3705\n",
      "Epoch 758/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4561 - accuracy: 0.5136 - val_loss: 0.7249 - val_accuracy: 0.3667\n",
      "Epoch 759/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4626 - accuracy: 0.5054 - val_loss: 0.7188 - val_accuracy: 0.3732\n",
      "Epoch 760/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4577 - accuracy: 0.5116 - val_loss: 0.7321 - val_accuracy: 0.3745\n",
      "Epoch 761/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4549 - accuracy: 0.5140 - val_loss: 0.7310 - val_accuracy: 0.3714\n",
      "Epoch 762/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4556 - accuracy: 0.5156 - val_loss: 0.7191 - val_accuracy: 0.3673\n",
      "Epoch 763/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4606 - accuracy: 0.5100 - val_loss: 0.7292 - val_accuracy: 0.3733\n",
      "Epoch 764/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4782 - accuracy: 0.4945 - val_loss: 0.7126 - val_accuracy: 0.3742\n",
      "Epoch 765/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4619 - accuracy: 0.5060 - val_loss: 0.7250 - val_accuracy: 0.3762\n",
      "Epoch 766/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4600 - accuracy: 0.5128 - val_loss: 0.7238 - val_accuracy: 0.3717\n",
      "Epoch 767/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4534 - accuracy: 0.5148 - val_loss: 0.7246 - val_accuracy: 0.3709\n",
      "Epoch 768/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4642 - accuracy: 0.5087 - val_loss: 0.7143 - val_accuracy: 0.3712\n",
      "Epoch 769/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4632 - accuracy: 0.5048 - val_loss: 0.7265 - val_accuracy: 0.3736\n",
      "Epoch 770/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4639 - accuracy: 0.5082 - val_loss: 0.7193 - val_accuracy: 0.3715\n",
      "Epoch 771/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4547 - accuracy: 0.5126 - val_loss: 0.7255 - val_accuracy: 0.3666\n",
      "Epoch 772/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4627 - accuracy: 0.5093 - val_loss: 0.7118 - val_accuracy: 0.3700\n",
      "Epoch 773/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4530 - accuracy: 0.5117 - val_loss: 0.7268 - val_accuracy: 0.3718\n",
      "Epoch 774/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4512 - accuracy: 0.5183 - val_loss: 0.7356 - val_accuracy: 0.3669\n",
      "Epoch 775/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4648 - accuracy: 0.5092 - val_loss: 0.7057 - val_accuracy: 0.3688\n",
      "Epoch 776/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4738 - accuracy: 0.5004 - val_loss: 0.7152 - val_accuracy: 0.3599\n",
      "Epoch 777/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4702 - accuracy: 0.5041 - val_loss: 0.7168 - val_accuracy: 0.3658\n",
      "Epoch 778/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4664 - accuracy: 0.5087 - val_loss: 0.7124 - val_accuracy: 0.3682\n",
      "Epoch 779/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4666 - accuracy: 0.5085 - val_loss: 0.7216 - val_accuracy: 0.3608\n",
      "Epoch 780/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4756 - accuracy: 0.4983 - val_loss: 0.7171 - val_accuracy: 0.3615\n",
      "Epoch 781/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4581 - accuracy: 0.5089 - val_loss: 0.7272 - val_accuracy: 0.3651\n",
      "Epoch 782/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4568 - accuracy: 0.5135 - val_loss: 0.6995 - val_accuracy: 0.3691\n",
      "Epoch 783/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4602 - accuracy: 0.5128 - val_loss: 0.7172 - val_accuracy: 0.3717\n",
      "Epoch 784/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4552 - accuracy: 0.5153 - val_loss: 0.7324 - val_accuracy: 0.3741\n",
      "Epoch 785/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4593 - accuracy: 0.5126 - val_loss: 0.7265 - val_accuracy: 0.3721\n",
      "Epoch 786/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4574 - accuracy: 0.5122 - val_loss: 0.7232 - val_accuracy: 0.3768\n",
      "Epoch 787/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4647 - accuracy: 0.5058 - val_loss: 0.7085 - val_accuracy: 0.3732\n",
      "Epoch 788/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4583 - accuracy: 0.5092 - val_loss: 0.7225 - val_accuracy: 0.3781\n",
      "Epoch 789/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4652 - accuracy: 0.5054 - val_loss: 0.7329 - val_accuracy: 0.3730\n",
      "Epoch 790/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4604 - accuracy: 0.5106 - val_loss: 0.7294 - val_accuracy: 0.3765\n",
      "Epoch 791/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4811 - accuracy: 0.4968 - val_loss: 0.7324 - val_accuracy: 0.3753\n",
      "Epoch 792/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4609 - accuracy: 0.5132 - val_loss: 0.7197 - val_accuracy: 0.3705\n",
      "Epoch 793/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4582 - accuracy: 0.5141 - val_loss: 0.7209 - val_accuracy: 0.3714\n",
      "Epoch 794/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4758 - accuracy: 0.4971 - val_loss: 0.7192 - val_accuracy: 0.3684\n",
      "Epoch 795/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.4781 - accuracy: 0.4942 - val_loss: 0.7229 - val_accuracy: 0.3747\n",
      "Epoch 796/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4567 - accuracy: 0.5126 - val_loss: 0.7300 - val_accuracy: 0.3729\n",
      "Epoch 797/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4604 - accuracy: 0.5149 - val_loss: 0.7217 - val_accuracy: 0.3712\n",
      "Epoch 798/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4552 - accuracy: 0.5127 - val_loss: 0.7241 - val_accuracy: 0.3625\n",
      "Epoch 799/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4558 - accuracy: 0.5108 - val_loss: 0.7236 - val_accuracy: 0.3678\n",
      "Epoch 800/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4454 - accuracy: 0.5231 - val_loss: 0.7354 - val_accuracy: 0.3744\n",
      "Epoch 801/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4547 - accuracy: 0.5156 - val_loss: 0.7180 - val_accuracy: 0.3696\n",
      "Epoch 802/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4527 - accuracy: 0.5173 - val_loss: 0.7330 - val_accuracy: 0.3763\n",
      "Epoch 803/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4557 - accuracy: 0.5126 - val_loss: 0.7247 - val_accuracy: 0.3672\n",
      "Epoch 804/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4517 - accuracy: 0.5184 - val_loss: 0.7388 - val_accuracy: 0.3723\n",
      "Epoch 805/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4517 - accuracy: 0.5221 - val_loss: 0.7147 - val_accuracy: 0.3714\n",
      "Epoch 806/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4570 - accuracy: 0.5143 - val_loss: 0.7172 - val_accuracy: 0.3712\n",
      "Epoch 807/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4471 - accuracy: 0.5231 - val_loss: 0.7224 - val_accuracy: 0.3691\n",
      "Epoch 808/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4543 - accuracy: 0.5144 - val_loss: 0.7179 - val_accuracy: 0.3637\n",
      "Epoch 809/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4535 - accuracy: 0.5164 - val_loss: 0.7181 - val_accuracy: 0.3693\n",
      "Epoch 810/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4513 - accuracy: 0.5180 - val_loss: 0.7232 - val_accuracy: 0.3681\n",
      "Epoch 811/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4565 - accuracy: 0.5157 - val_loss: 0.7179 - val_accuracy: 0.3640\n",
      "Epoch 812/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4574 - accuracy: 0.5136 - val_loss: 0.7230 - val_accuracy: 0.3681\n",
      "Epoch 813/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4577 - accuracy: 0.5116 - val_loss: 0.7252 - val_accuracy: 0.3724\n",
      "Epoch 814/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4658 - accuracy: 0.5114 - val_loss: 0.7172 - val_accuracy: 0.3673\n",
      "Epoch 815/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4704 - accuracy: 0.5026 - val_loss: 0.7250 - val_accuracy: 0.3690\n",
      "Epoch 816/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4561 - accuracy: 0.5133 - val_loss: 0.7239 - val_accuracy: 0.3684\n",
      "Epoch 817/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4515 - accuracy: 0.5191 - val_loss: 0.7295 - val_accuracy: 0.3660\n",
      "Epoch 818/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4530 - accuracy: 0.5201 - val_loss: 0.7259 - val_accuracy: 0.3766\n",
      "Epoch 819/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4547 - accuracy: 0.5151 - val_loss: 0.7372 - val_accuracy: 0.3735\n",
      "Epoch 820/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4596 - accuracy: 0.5093 - val_loss: 0.7262 - val_accuracy: 0.3747\n",
      "Epoch 821/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4554 - accuracy: 0.5169 - val_loss: 0.7204 - val_accuracy: 0.3717\n",
      "Epoch 822/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4501 - accuracy: 0.5178 - val_loss: 0.7250 - val_accuracy: 0.3721\n",
      "Epoch 823/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4502 - accuracy: 0.5177 - val_loss: 0.7360 - val_accuracy: 0.3696\n",
      "Epoch 824/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4556 - accuracy: 0.5130 - val_loss: 0.7412 - val_accuracy: 0.3681\n",
      "Epoch 825/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4521 - accuracy: 0.5190 - val_loss: 0.7325 - val_accuracy: 0.3738\n",
      "Epoch 826/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4489 - accuracy: 0.5200 - val_loss: 0.7234 - val_accuracy: 0.3754\n",
      "Epoch 827/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4516 - accuracy: 0.5147 - val_loss: 0.7192 - val_accuracy: 0.3681\n",
      "Epoch 828/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4539 - accuracy: 0.5137 - val_loss: 0.7287 - val_accuracy: 0.3715\n",
      "Epoch 829/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4526 - accuracy: 0.5173 - val_loss: 0.7242 - val_accuracy: 0.3733\n",
      "Epoch 830/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4590 - accuracy: 0.5100 - val_loss: 0.7109 - val_accuracy: 0.3705\n",
      "Epoch 831/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4477 - accuracy: 0.5186 - val_loss: 0.7274 - val_accuracy: 0.3745\n",
      "Epoch 832/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4495 - accuracy: 0.5211 - val_loss: 0.7167 - val_accuracy: 0.3670\n",
      "Epoch 833/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4521 - accuracy: 0.5180 - val_loss: 0.7329 - val_accuracy: 0.3747\n",
      "Epoch 834/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4549 - accuracy: 0.5149 - val_loss: 0.7174 - val_accuracy: 0.3702\n",
      "Epoch 835/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4506 - accuracy: 0.5205 - val_loss: 0.7156 - val_accuracy: 0.3727\n",
      "Epoch 836/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4504 - accuracy: 0.5221 - val_loss: 0.7170 - val_accuracy: 0.3705\n",
      "Epoch 837/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.4444 - accuracy: 0.5237 - val_loss: 0.7235 - val_accuracy: 0.3715\n",
      "Epoch 838/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4504 - accuracy: 0.5186 - val_loss: 0.7311 - val_accuracy: 0.3645\n",
      "Epoch 839/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4556 - accuracy: 0.5145 - val_loss: 0.7154 - val_accuracy: 0.3733\n",
      "Epoch 840/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4499 - accuracy: 0.5184 - val_loss: 0.7182 - val_accuracy: 0.3751\n",
      "Epoch 841/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4571 - accuracy: 0.5129 - val_loss: 0.7267 - val_accuracy: 0.3720\n",
      "Epoch 842/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4535 - accuracy: 0.5166 - val_loss: 0.7242 - val_accuracy: 0.3745\n",
      "Epoch 843/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4528 - accuracy: 0.5171 - val_loss: 0.7131 - val_accuracy: 0.3714\n",
      "Epoch 844/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4490 - accuracy: 0.5228 - val_loss: 0.7219 - val_accuracy: 0.3730\n",
      "Epoch 845/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4533 - accuracy: 0.5165 - val_loss: 0.7129 - val_accuracy: 0.3709\n",
      "Epoch 846/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4568 - accuracy: 0.5138 - val_loss: 0.7321 - val_accuracy: 0.3762\n",
      "Epoch 847/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4541 - accuracy: 0.5154 - val_loss: 0.7294 - val_accuracy: 0.3753\n",
      "Epoch 848/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4544 - accuracy: 0.5131 - val_loss: 0.7220 - val_accuracy: 0.3711\n",
      "Epoch 849/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4496 - accuracy: 0.5209 - val_loss: 0.7249 - val_accuracy: 0.3682\n",
      "Epoch 850/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4484 - accuracy: 0.5162 - val_loss: 0.7349 - val_accuracy: 0.3679\n",
      "Epoch 851/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4433 - accuracy: 0.5207 - val_loss: 0.7265 - val_accuracy: 0.3718\n",
      "Epoch 852/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4477 - accuracy: 0.5203 - val_loss: 0.7302 - val_accuracy: 0.3729\n",
      "Epoch 853/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4434 - accuracy: 0.5253 - val_loss: 0.7299 - val_accuracy: 0.3741\n",
      "Epoch 854/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4477 - accuracy: 0.5208 - val_loss: 0.7225 - val_accuracy: 0.3751\n",
      "Epoch 855/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4552 - accuracy: 0.5155 - val_loss: 0.7164 - val_accuracy: 0.3765\n",
      "Epoch 856/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4459 - accuracy: 0.5226 - val_loss: 0.7230 - val_accuracy: 0.3760\n",
      "Epoch 857/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4494 - accuracy: 0.5207 - val_loss: 0.7342 - val_accuracy: 0.3744\n",
      "Epoch 858/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4502 - accuracy: 0.5185 - val_loss: 0.7414 - val_accuracy: 0.3699\n",
      "Epoch 859/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4529 - accuracy: 0.5174 - val_loss: 0.7240 - val_accuracy: 0.3696\n",
      "Epoch 860/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4516 - accuracy: 0.5178 - val_loss: 0.7342 - val_accuracy: 0.3730\n",
      "Epoch 861/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4485 - accuracy: 0.5201 - val_loss: 0.7396 - val_accuracy: 0.3772\n",
      "Epoch 862/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4527 - accuracy: 0.5157 - val_loss: 0.7293 - val_accuracy: 0.3730\n",
      "Epoch 863/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4515 - accuracy: 0.5196 - val_loss: 0.7278 - val_accuracy: 0.3732\n",
      "Epoch 864/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4524 - accuracy: 0.5172 - val_loss: 0.7413 - val_accuracy: 0.3754\n",
      "Epoch 865/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4487 - accuracy: 0.5209 - val_loss: 0.7192 - val_accuracy: 0.3748\n",
      "Epoch 866/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4488 - accuracy: 0.5188 - val_loss: 0.7256 - val_accuracy: 0.3762\n",
      "Epoch 867/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4518 - accuracy: 0.5196 - val_loss: 0.7331 - val_accuracy: 0.3750\n",
      "Epoch 868/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4489 - accuracy: 0.5220 - val_loss: 0.7304 - val_accuracy: 0.3708\n",
      "Epoch 869/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4491 - accuracy: 0.5210 - val_loss: 0.7262 - val_accuracy: 0.3766\n",
      "Epoch 870/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4504 - accuracy: 0.5213 - val_loss: 0.7273 - val_accuracy: 0.3685\n",
      "Epoch 871/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4544 - accuracy: 0.5174 - val_loss: 0.7346 - val_accuracy: 0.3729\n",
      "Epoch 872/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4439 - accuracy: 0.5246 - val_loss: 0.7421 - val_accuracy: 0.3693\n",
      "Epoch 873/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4507 - accuracy: 0.5205 - val_loss: 0.7197 - val_accuracy: 0.3741\n",
      "Epoch 874/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4494 - accuracy: 0.5167 - val_loss: 0.7421 - val_accuracy: 0.3760\n",
      "Epoch 875/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4523 - accuracy: 0.5191 - val_loss: 0.7145 - val_accuracy: 0.3676\n",
      "Epoch 876/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4537 - accuracy: 0.5151 - val_loss: 0.7214 - val_accuracy: 0.3684\n",
      "Epoch 877/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4478 - accuracy: 0.5225 - val_loss: 0.7349 - val_accuracy: 0.3742\n",
      "Epoch 878/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4546 - accuracy: 0.5170 - val_loss: 0.7225 - val_accuracy: 0.3729\n",
      "Epoch 879/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4475 - accuracy: 0.5218 - val_loss: 0.7348 - val_accuracy: 0.3769\n",
      "Epoch 880/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4429 - accuracy: 0.5254 - val_loss: 0.7264 - val_accuracy: 0.3780\n",
      "Epoch 881/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4434 - accuracy: 0.5238 - val_loss: 0.7291 - val_accuracy: 0.3736\n",
      "Epoch 882/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4466 - accuracy: 0.5222 - val_loss: 0.7168 - val_accuracy: 0.3608\n",
      "Epoch 883/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4478 - accuracy: 0.5214 - val_loss: 0.7201 - val_accuracy: 0.3676\n",
      "Epoch 884/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4489 - accuracy: 0.5196 - val_loss: 0.7280 - val_accuracy: 0.3747\n",
      "Epoch 885/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4496 - accuracy: 0.5195 - val_loss: 0.7407 - val_accuracy: 0.3709\n",
      "Epoch 886/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4427 - accuracy: 0.5265 - val_loss: 0.7443 - val_accuracy: 0.3772\n",
      "Epoch 887/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4445 - accuracy: 0.5241 - val_loss: 0.7370 - val_accuracy: 0.3745\n",
      "Epoch 888/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4565 - accuracy: 0.5161 - val_loss: 0.7297 - val_accuracy: 0.3693\n",
      "Epoch 889/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4469 - accuracy: 0.5240 - val_loss: 0.7274 - val_accuracy: 0.3769\n",
      "Epoch 890/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4457 - accuracy: 0.5265 - val_loss: 0.7277 - val_accuracy: 0.3724\n",
      "Epoch 891/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4512 - accuracy: 0.5223 - val_loss: 0.7264 - val_accuracy: 0.3720\n",
      "Epoch 892/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4462 - accuracy: 0.5242 - val_loss: 0.7445 - val_accuracy: 0.3771\n",
      "Epoch 893/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4454 - accuracy: 0.5276 - val_loss: 0.7477 - val_accuracy: 0.3754\n",
      "Epoch 894/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4491 - accuracy: 0.5225 - val_loss: 0.7305 - val_accuracy: 0.3699\n",
      "Epoch 895/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4441 - accuracy: 0.5239 - val_loss: 0.7393 - val_accuracy: 0.3711\n",
      "Epoch 896/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4427 - accuracy: 0.5266 - val_loss: 0.7357 - val_accuracy: 0.3733\n",
      "Epoch 897/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4457 - accuracy: 0.5256 - val_loss: 0.7318 - val_accuracy: 0.3739\n",
      "Epoch 898/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4444 - accuracy: 0.5258 - val_loss: 0.7320 - val_accuracy: 0.3778\n",
      "Epoch 899/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4475 - accuracy: 0.5250 - val_loss: 0.7267 - val_accuracy: 0.3676\n",
      "Epoch 900/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4462 - accuracy: 0.5228 - val_loss: 0.7241 - val_accuracy: 0.3738\n",
      "Epoch 901/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4476 - accuracy: 0.5195 - val_loss: 0.7269 - val_accuracy: 0.3715\n",
      "Epoch 902/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4467 - accuracy: 0.5237 - val_loss: 0.7305 - val_accuracy: 0.3739\n",
      "Epoch 903/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4379 - accuracy: 0.5285 - val_loss: 0.7311 - val_accuracy: 0.3703\n",
      "Epoch 904/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4433 - accuracy: 0.5251 - val_loss: 0.7365 - val_accuracy: 0.3681\n",
      "Epoch 905/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4464 - accuracy: 0.5208 - val_loss: 0.7320 - val_accuracy: 0.3705\n",
      "Epoch 906/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4490 - accuracy: 0.5216 - val_loss: 0.7184 - val_accuracy: 0.3679\n",
      "Epoch 907/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4523 - accuracy: 0.5171 - val_loss: 0.7294 - val_accuracy: 0.3666\n",
      "Epoch 908/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4480 - accuracy: 0.5243 - val_loss: 0.7234 - val_accuracy: 0.3709\n",
      "Epoch 909/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4431 - accuracy: 0.5266 - val_loss: 0.7289 - val_accuracy: 0.3766\n",
      "Epoch 910/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4430 - accuracy: 0.5268 - val_loss: 0.7435 - val_accuracy: 0.3721\n",
      "Epoch 911/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4476 - accuracy: 0.5208 - val_loss: 0.7351 - val_accuracy: 0.3733\n",
      "Epoch 912/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4453 - accuracy: 0.5220 - val_loss: 0.7342 - val_accuracy: 0.3669\n",
      "Epoch 913/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4417 - accuracy: 0.5321 - val_loss: 0.7368 - val_accuracy: 0.3655\n",
      "Epoch 914/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4408 - accuracy: 0.5283 - val_loss: 0.7357 - val_accuracy: 0.3708\n",
      "Epoch 915/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4409 - accuracy: 0.5325 - val_loss: 0.7344 - val_accuracy: 0.3690\n",
      "Epoch 916/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4388 - accuracy: 0.5291 - val_loss: 0.7337 - val_accuracy: 0.3742\n",
      "Epoch 917/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4485 - accuracy: 0.5247 - val_loss: 0.7336 - val_accuracy: 0.3709\n",
      "Epoch 918/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4482 - accuracy: 0.5219 - val_loss: 0.7349 - val_accuracy: 0.3754\n",
      "Epoch 919/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4530 - accuracy: 0.5191 - val_loss: 0.7331 - val_accuracy: 0.3697\n",
      "Epoch 920/2500\n",
      "20056/20056 [==============================] - 1s 51us/step - loss: 0.4440 - accuracy: 0.5291 - val_loss: 0.7219 - val_accuracy: 0.3738\n",
      "Epoch 921/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4465 - accuracy: 0.5246 - val_loss: 0.7227 - val_accuracy: 0.3681\n",
      "Epoch 922/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4455 - accuracy: 0.5234 - val_loss: 0.7319 - val_accuracy: 0.3727\n",
      "Epoch 923/2500\n",
      "20056/20056 [==============================] - 1s 51us/step - loss: 0.4453 - accuracy: 0.5248 - val_loss: 0.7344 - val_accuracy: 0.3717\n",
      "Epoch 924/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4392 - accuracy: 0.5306 - val_loss: 0.7451 - val_accuracy: 0.3700\n",
      "Epoch 925/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4451 - accuracy: 0.5247 - val_loss: 0.7442 - val_accuracy: 0.3714\n",
      "Epoch 926/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4488 - accuracy: 0.5222 - val_loss: 0.7262 - val_accuracy: 0.3727\n",
      "Epoch 927/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4399 - accuracy: 0.5315 - val_loss: 0.7406 - val_accuracy: 0.3729\n",
      "Epoch 928/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4353 - accuracy: 0.5327 - val_loss: 0.7303 - val_accuracy: 0.3733\n",
      "Epoch 929/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4425 - accuracy: 0.5264 - val_loss: 0.7270 - val_accuracy: 0.3645\n",
      "Epoch 930/2500\n",
      "20056/20056 [==============================] - 1s 51us/step - loss: 0.4406 - accuracy: 0.5283 - val_loss: 0.7340 - val_accuracy: 0.3675\n",
      "Epoch 931/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4443 - accuracy: 0.5254 - val_loss: 0.7402 - val_accuracy: 0.3684\n",
      "Epoch 932/2500\n",
      "20056/20056 [==============================] - 1s 51us/step - loss: 0.4507 - accuracy: 0.5232 - val_loss: 0.7208 - val_accuracy: 0.3690\n",
      "Epoch 933/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4497 - accuracy: 0.5220 - val_loss: 0.7316 - val_accuracy: 0.3652\n",
      "Epoch 934/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4415 - accuracy: 0.5286 - val_loss: 0.7409 - val_accuracy: 0.3721\n",
      "Epoch 935/2500\n",
      "20056/20056 [==============================] - 1s 51us/step - loss: 0.4424 - accuracy: 0.5269 - val_loss: 0.7346 - val_accuracy: 0.3748\n",
      "Epoch 936/2500\n",
      "20056/20056 [==============================] - 1s 51us/step - loss: 0.4415 - accuracy: 0.5313 - val_loss: 0.7394 - val_accuracy: 0.3711\n",
      "Epoch 937/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4607 - accuracy: 0.5109 - val_loss: 0.7302 - val_accuracy: 0.3717\n",
      "Epoch 938/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4490 - accuracy: 0.5241 - val_loss: 0.7258 - val_accuracy: 0.3631\n",
      "Epoch 939/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4443 - accuracy: 0.5257 - val_loss: 0.7386 - val_accuracy: 0.3651\n",
      "Epoch 940/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4436 - accuracy: 0.5284 - val_loss: 0.7485 - val_accuracy: 0.3693\n",
      "Epoch 941/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4457 - accuracy: 0.5272 - val_loss: 0.7412 - val_accuracy: 0.3706\n",
      "Epoch 942/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4415 - accuracy: 0.5284 - val_loss: 0.7288 - val_accuracy: 0.3763\n",
      "Epoch 943/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4406 - accuracy: 0.5284 - val_loss: 0.7365 - val_accuracy: 0.3811\n",
      "Epoch 944/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4457 - accuracy: 0.5255 - val_loss: 0.7407 - val_accuracy: 0.3691\n",
      "Epoch 945/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4393 - accuracy: 0.5302 - val_loss: 0.7260 - val_accuracy: 0.3726\n",
      "Epoch 946/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4444 - accuracy: 0.5251 - val_loss: 0.7477 - val_accuracy: 0.3762\n",
      "Epoch 947/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4409 - accuracy: 0.5285 - val_loss: 0.7408 - val_accuracy: 0.3724\n",
      "Epoch 948/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4464 - accuracy: 0.5268 - val_loss: 0.7422 - val_accuracy: 0.3762\n",
      "Epoch 949/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4553 - accuracy: 0.5152 - val_loss: 0.7257 - val_accuracy: 0.3747\n",
      "Epoch 950/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4559 - accuracy: 0.5169 - val_loss: 0.7375 - val_accuracy: 0.3796\n",
      "Epoch 951/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4407 - accuracy: 0.5319 - val_loss: 0.7484 - val_accuracy: 0.3730\n",
      "Epoch 952/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4444 - accuracy: 0.5302 - val_loss: 0.7436 - val_accuracy: 0.3777\n",
      "Epoch 953/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4395 - accuracy: 0.5295 - val_loss: 0.7486 - val_accuracy: 0.3720\n",
      "Epoch 954/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4396 - accuracy: 0.5322 - val_loss: 0.7543 - val_accuracy: 0.3744\n",
      "Epoch 955/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4403 - accuracy: 0.5338 - val_loss: 0.7426 - val_accuracy: 0.3709\n",
      "Epoch 956/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4385 - accuracy: 0.5319 - val_loss: 0.7416 - val_accuracy: 0.3667\n",
      "Epoch 957/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4420 - accuracy: 0.5271 - val_loss: 0.7430 - val_accuracy: 0.3708\n",
      "Epoch 958/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4356 - accuracy: 0.5321 - val_loss: 0.7289 - val_accuracy: 0.3723\n",
      "Epoch 959/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4511 - accuracy: 0.5211 - val_loss: 0.7306 - val_accuracy: 0.3727\n",
      "Epoch 960/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4386 - accuracy: 0.5307 - val_loss: 0.7332 - val_accuracy: 0.3702\n",
      "Epoch 961/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4433 - accuracy: 0.5249 - val_loss: 0.7272 - val_accuracy: 0.3664\n",
      "Epoch 962/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4439 - accuracy: 0.5241 - val_loss: 0.7385 - val_accuracy: 0.3730\n",
      "Epoch 963/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4420 - accuracy: 0.5238 - val_loss: 0.7354 - val_accuracy: 0.3720\n",
      "Epoch 964/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4363 - accuracy: 0.5326 - val_loss: 0.7442 - val_accuracy: 0.3712\n",
      "Epoch 965/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4376 - accuracy: 0.5315 - val_loss: 0.7467 - val_accuracy: 0.3739\n",
      "Epoch 966/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4396 - accuracy: 0.5316 - val_loss: 0.7399 - val_accuracy: 0.3708\n",
      "Epoch 967/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4352 - accuracy: 0.5347 - val_loss: 0.7445 - val_accuracy: 0.3775\n",
      "Epoch 968/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4388 - accuracy: 0.5324 - val_loss: 0.7311 - val_accuracy: 0.3673\n",
      "Epoch 969/2500\n",
      "20056/20056 [==============================] - 2s 77us/step - loss: 0.4404 - accuracy: 0.5285 - val_loss: 0.7259 - val_accuracy: 0.3739\n",
      "Epoch 970/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4371 - accuracy: 0.5346 - val_loss: 0.7467 - val_accuracy: 0.3754\n",
      "Epoch 971/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4380 - accuracy: 0.5286 - val_loss: 0.7355 - val_accuracy: 0.3724\n",
      "Epoch 972/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4396 - accuracy: 0.5340 - val_loss: 0.7437 - val_accuracy: 0.3732\n",
      "Epoch 973/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.4459 - accuracy: 0.5223 - val_loss: 0.7353 - val_accuracy: 0.3706\n",
      "Epoch 974/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4383 - accuracy: 0.5320 - val_loss: 0.7341 - val_accuracy: 0.3693\n",
      "Epoch 975/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4411 - accuracy: 0.5296 - val_loss: 0.7346 - val_accuracy: 0.3741\n",
      "Epoch 976/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4558 - accuracy: 0.5183 - val_loss: 0.7323 - val_accuracy: 0.3690\n",
      "Epoch 977/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4571 - accuracy: 0.5165 - val_loss: 0.7199 - val_accuracy: 0.3694\n",
      "Epoch 978/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4449 - accuracy: 0.5282 - val_loss: 0.7241 - val_accuracy: 0.3747\n",
      "Epoch 979/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4466 - accuracy: 0.5245 - val_loss: 0.7372 - val_accuracy: 0.3681\n",
      "Epoch 980/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4439 - accuracy: 0.5265 - val_loss: 0.7410 - val_accuracy: 0.3747\n",
      "Epoch 981/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4432 - accuracy: 0.5294 - val_loss: 0.7423 - val_accuracy: 0.3789\n",
      "Epoch 982/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4461 - accuracy: 0.5289 - val_loss: 0.7243 - val_accuracy: 0.3724\n",
      "Epoch 983/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4397 - accuracy: 0.5320 - val_loss: 0.7292 - val_accuracy: 0.3706\n",
      "Epoch 984/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4367 - accuracy: 0.5323 - val_loss: 0.7347 - val_accuracy: 0.3754\n",
      "Epoch 985/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.4404 - accuracy: 0.5314 - val_loss: 0.7339 - val_accuracy: 0.3708\n",
      "Epoch 986/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4441 - accuracy: 0.5272 - val_loss: 0.7299 - val_accuracy: 0.3729\n",
      "Epoch 987/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4394 - accuracy: 0.5340 - val_loss: 0.7306 - val_accuracy: 0.3748\n",
      "Epoch 988/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4452 - accuracy: 0.5256 - val_loss: 0.7314 - val_accuracy: 0.3711\n",
      "Epoch 989/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4403 - accuracy: 0.5323 - val_loss: 0.7428 - val_accuracy: 0.3766\n",
      "Epoch 990/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4423 - accuracy: 0.5305 - val_loss: 0.7265 - val_accuracy: 0.3684\n",
      "Epoch 991/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4361 - accuracy: 0.5311 - val_loss: 0.7483 - val_accuracy: 0.3765\n",
      "Epoch 992/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4322 - accuracy: 0.5389 - val_loss: 0.7371 - val_accuracy: 0.3750\n",
      "Epoch 993/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4368 - accuracy: 0.5306 - val_loss: 0.7348 - val_accuracy: 0.3759\n",
      "Epoch 994/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4386 - accuracy: 0.5329 - val_loss: 0.7453 - val_accuracy: 0.3714\n",
      "Epoch 995/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4337 - accuracy: 0.5350 - val_loss: 0.7423 - val_accuracy: 0.3756\n",
      "Epoch 996/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4361 - accuracy: 0.5309 - val_loss: 0.7385 - val_accuracy: 0.3748\n",
      "Epoch 997/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4374 - accuracy: 0.5317 - val_loss: 0.7353 - val_accuracy: 0.3735\n",
      "Epoch 998/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4374 - accuracy: 0.5318 - val_loss: 0.7338 - val_accuracy: 0.3679\n",
      "Epoch 999/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4368 - accuracy: 0.5348 - val_loss: 0.7286 - val_accuracy: 0.3748\n",
      "Epoch 1000/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4394 - accuracy: 0.5283 - val_loss: 0.7421 - val_accuracy: 0.3745\n",
      "Epoch 1001/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4330 - accuracy: 0.5371 - val_loss: 0.7452 - val_accuracy: 0.3783\n",
      "Epoch 1002/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4384 - accuracy: 0.5269 - val_loss: 0.7245 - val_accuracy: 0.3745\n",
      "Epoch 1003/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4345 - accuracy: 0.5395 - val_loss: 0.7365 - val_accuracy: 0.3784\n",
      "Epoch 1004/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4366 - accuracy: 0.5358 - val_loss: 0.7291 - val_accuracy: 0.3727\n",
      "Epoch 1005/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4465 - accuracy: 0.5270 - val_loss: 0.7429 - val_accuracy: 0.3705\n",
      "Epoch 1006/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4392 - accuracy: 0.5324 - val_loss: 0.7346 - val_accuracy: 0.3732\n",
      "Epoch 1007/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4411 - accuracy: 0.5281 - val_loss: 0.7427 - val_accuracy: 0.3715\n",
      "Epoch 1008/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4358 - accuracy: 0.5341 - val_loss: 0.7455 - val_accuracy: 0.3723\n",
      "Epoch 1009/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4340 - accuracy: 0.5378 - val_loss: 0.7443 - val_accuracy: 0.3750\n",
      "Epoch 1010/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.4409 - accuracy: 0.5295 - val_loss: 0.7262 - val_accuracy: 0.3750\n",
      "Epoch 1011/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4364 - accuracy: 0.5327 - val_loss: 0.7578 - val_accuracy: 0.3765\n",
      "Epoch 1012/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4348 - accuracy: 0.5415 - val_loss: 0.7363 - val_accuracy: 0.3735\n",
      "Epoch 1013/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4351 - accuracy: 0.5362 - val_loss: 0.7388 - val_accuracy: 0.3714\n",
      "Epoch 1014/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4347 - accuracy: 0.5367 - val_loss: 0.7424 - val_accuracy: 0.3766\n",
      "Epoch 1015/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4355 - accuracy: 0.5377 - val_loss: 0.7465 - val_accuracy: 0.3745\n",
      "Epoch 1016/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4415 - accuracy: 0.5351 - val_loss: 0.7459 - val_accuracy: 0.3768\n",
      "Epoch 1017/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4610 - accuracy: 0.5164 - val_loss: 0.7390 - val_accuracy: 0.3757\n",
      "Epoch 1018/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4378 - accuracy: 0.5316 - val_loss: 0.7309 - val_accuracy: 0.3735\n",
      "Epoch 1019/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4345 - accuracy: 0.5316 - val_loss: 0.7382 - val_accuracy: 0.3735\n",
      "Epoch 1020/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4427 - accuracy: 0.5288 - val_loss: 0.7285 - val_accuracy: 0.3697\n",
      "Epoch 1021/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4394 - accuracy: 0.5316 - val_loss: 0.7354 - val_accuracy: 0.3724\n",
      "Epoch 1022/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4377 - accuracy: 0.5303 - val_loss: 0.7383 - val_accuracy: 0.3705\n",
      "Epoch 1023/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4522 - accuracy: 0.5193 - val_loss: 0.7343 - val_accuracy: 0.3699\n",
      "Epoch 1024/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4404 - accuracy: 0.5315 - val_loss: 0.7425 - val_accuracy: 0.3735\n",
      "Epoch 1025/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4350 - accuracy: 0.5335 - val_loss: 0.7550 - val_accuracy: 0.3715\n",
      "Epoch 1026/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4414 - accuracy: 0.5335 - val_loss: 0.7476 - val_accuracy: 0.3733\n",
      "Epoch 1027/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4346 - accuracy: 0.5374 - val_loss: 0.7392 - val_accuracy: 0.3724\n",
      "Epoch 1028/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4326 - accuracy: 0.5366 - val_loss: 0.7422 - val_accuracy: 0.3762\n",
      "Epoch 1029/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4257 - accuracy: 0.5430 - val_loss: 0.7445 - val_accuracy: 0.3741\n",
      "Epoch 1030/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4309 - accuracy: 0.5412 - val_loss: 0.7372 - val_accuracy: 0.3727\n",
      "Epoch 1031/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4322 - accuracy: 0.5374 - val_loss: 0.7437 - val_accuracy: 0.3729\n",
      "Epoch 1032/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4365 - accuracy: 0.5338 - val_loss: 0.7289 - val_accuracy: 0.3715\n",
      "Epoch 1033/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4404 - accuracy: 0.5330 - val_loss: 0.7366 - val_accuracy: 0.3706\n",
      "Epoch 1034/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4463 - accuracy: 0.5291 - val_loss: 0.7211 - val_accuracy: 0.3754\n",
      "Epoch 1035/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4473 - accuracy: 0.5229 - val_loss: 0.7267 - val_accuracy: 0.3678\n",
      "Epoch 1036/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4550 - accuracy: 0.5179 - val_loss: 0.7244 - val_accuracy: 0.3673\n",
      "Epoch 1037/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4482 - accuracy: 0.5192 - val_loss: 0.7318 - val_accuracy: 0.3693\n",
      "Epoch 1038/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4451 - accuracy: 0.5263 - val_loss: 0.7382 - val_accuracy: 0.3711\n",
      "Epoch 1039/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4353 - accuracy: 0.5377 - val_loss: 0.7353 - val_accuracy: 0.3711\n",
      "Epoch 1040/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4297 - accuracy: 0.5407 - val_loss: 0.7339 - val_accuracy: 0.3703\n",
      "Epoch 1041/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4317 - accuracy: 0.5376 - val_loss: 0.7432 - val_accuracy: 0.3651\n",
      "Epoch 1042/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4420 - accuracy: 0.5320 - val_loss: 0.7441 - val_accuracy: 0.3756\n",
      "Epoch 1043/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4323 - accuracy: 0.5391 - val_loss: 0.7490 - val_accuracy: 0.3715\n",
      "Epoch 1044/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4464 - accuracy: 0.5283 - val_loss: 0.7488 - val_accuracy: 0.3742\n",
      "Epoch 1045/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4415 - accuracy: 0.5321 - val_loss: 0.7510 - val_accuracy: 0.3700\n",
      "Epoch 1046/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4357 - accuracy: 0.5352 - val_loss: 0.7422 - val_accuracy: 0.3720\n",
      "Epoch 1047/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4423 - accuracy: 0.5304 - val_loss: 0.7428 - val_accuracy: 0.3733\n",
      "Epoch 1048/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4339 - accuracy: 0.5367 - val_loss: 0.7489 - val_accuracy: 0.3736\n",
      "Epoch 1049/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4368 - accuracy: 0.5350 - val_loss: 0.7496 - val_accuracy: 0.3696\n",
      "Epoch 1050/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4651 - accuracy: 0.5077 - val_loss: 0.7324 - val_accuracy: 0.3660\n",
      "Epoch 1051/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4459 - accuracy: 0.5257 - val_loss: 0.7410 - val_accuracy: 0.3748\n",
      "Epoch 1052/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4341 - accuracy: 0.5337 - val_loss: 0.7334 - val_accuracy: 0.3774\n",
      "Epoch 1053/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4315 - accuracy: 0.5435 - val_loss: 0.7350 - val_accuracy: 0.3784\n",
      "Epoch 1054/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4362 - accuracy: 0.5342 - val_loss: 0.7276 - val_accuracy: 0.3733\n",
      "Epoch 1055/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4331 - accuracy: 0.5333 - val_loss: 0.7344 - val_accuracy: 0.3766\n",
      "Epoch 1056/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4380 - accuracy: 0.5347 - val_loss: 0.7314 - val_accuracy: 0.3771\n",
      "Epoch 1057/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4348 - accuracy: 0.5345 - val_loss: 0.7318 - val_accuracy: 0.3715\n",
      "Epoch 1058/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4293 - accuracy: 0.5419 - val_loss: 0.7379 - val_accuracy: 0.3720\n",
      "Epoch 1059/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.4331 - accuracy: 0.5387 - val_loss: 0.7346 - val_accuracy: 0.3691\n",
      "Epoch 1060/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4340 - accuracy: 0.5368 - val_loss: 0.7331 - val_accuracy: 0.3694\n",
      "Epoch 1061/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.4418 - accuracy: 0.5319 - val_loss: 0.7539 - val_accuracy: 0.3732\n",
      "Epoch 1062/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4393 - accuracy: 0.5344 - val_loss: 0.7279 - val_accuracy: 0.3694\n",
      "Epoch 1063/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.4443 - accuracy: 0.5266 - val_loss: 0.7271 - val_accuracy: 0.3748\n",
      "Epoch 1064/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4516 - accuracy: 0.5172 - val_loss: 0.7240 - val_accuracy: 0.3741\n",
      "Epoch 1065/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4391 - accuracy: 0.5280 - val_loss: 0.7335 - val_accuracy: 0.3762\n",
      "Epoch 1066/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4362 - accuracy: 0.5315 - val_loss: 0.7464 - val_accuracy: 0.3694\n",
      "Epoch 1067/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4290 - accuracy: 0.5384 - val_loss: 0.7329 - val_accuracy: 0.3747\n",
      "Epoch 1068/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4369 - accuracy: 0.5287 - val_loss: 0.7329 - val_accuracy: 0.3732\n",
      "Epoch 1069/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4347 - accuracy: 0.5316 - val_loss: 0.7397 - val_accuracy: 0.3742\n",
      "Epoch 1070/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.4366 - accuracy: 0.5356 - val_loss: 0.7326 - val_accuracy: 0.3691\n",
      "Epoch 1071/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4302 - accuracy: 0.5385 - val_loss: 0.7470 - val_accuracy: 0.3763\n",
      "Epoch 1072/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4321 - accuracy: 0.5385 - val_loss: 0.7452 - val_accuracy: 0.3736\n",
      "Epoch 1073/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4319 - accuracy: 0.5370 - val_loss: 0.7625 - val_accuracy: 0.3727\n",
      "Epoch 1074/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4369 - accuracy: 0.5314 - val_loss: 0.7471 - val_accuracy: 0.3721\n",
      "Epoch 1075/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4307 - accuracy: 0.5400 - val_loss: 0.7616 - val_accuracy: 0.3745\n",
      "Epoch 1076/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4412 - accuracy: 0.5285 - val_loss: 0.7392 - val_accuracy: 0.3690\n",
      "Epoch 1077/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4514 - accuracy: 0.5206 - val_loss: 0.7272 - val_accuracy: 0.3717\n",
      "Epoch 1078/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4373 - accuracy: 0.5328 - val_loss: 0.7340 - val_accuracy: 0.3724\n",
      "Epoch 1079/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4382 - accuracy: 0.5324 - val_loss: 0.7394 - val_accuracy: 0.3778\n",
      "Epoch 1080/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4300 - accuracy: 0.5397 - val_loss: 0.7491 - val_accuracy: 0.3735\n",
      "Epoch 1081/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4270 - accuracy: 0.5394 - val_loss: 0.7378 - val_accuracy: 0.3760\n",
      "Epoch 1082/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4319 - accuracy: 0.5408 - val_loss: 0.7329 - val_accuracy: 0.3735\n",
      "Epoch 1083/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4264 - accuracy: 0.5385 - val_loss: 0.7386 - val_accuracy: 0.3751\n",
      "Epoch 1084/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4272 - accuracy: 0.5450 - val_loss: 0.7268 - val_accuracy: 0.3717\n",
      "Epoch 1085/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4356 - accuracy: 0.5383 - val_loss: 0.7421 - val_accuracy: 0.3741\n",
      "Epoch 1086/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.4273 - accuracy: 0.5420 - val_loss: 0.7500 - val_accuracy: 0.3759\n",
      "Epoch 1087/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.4425 - accuracy: 0.5271 - val_loss: 0.7324 - val_accuracy: 0.3717\n",
      "Epoch 1088/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.4395 - accuracy: 0.5305 - val_loss: 0.7175 - val_accuracy: 0.3690\n",
      "Epoch 1089/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.4399 - accuracy: 0.5277 - val_loss: 0.7259 - val_accuracy: 0.3699\n",
      "Epoch 1090/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4360 - accuracy: 0.5325 - val_loss: 0.7254 - val_accuracy: 0.3766\n",
      "Epoch 1091/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4351 - accuracy: 0.5359 - val_loss: 0.7436 - val_accuracy: 0.3748\n",
      "Epoch 1092/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4312 - accuracy: 0.5369 - val_loss: 0.7394 - val_accuracy: 0.3711\n",
      "Epoch 1093/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4268 - accuracy: 0.5394 - val_loss: 0.7539 - val_accuracy: 0.3714\n",
      "Epoch 1094/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4314 - accuracy: 0.5380 - val_loss: 0.7383 - val_accuracy: 0.3729\n",
      "Epoch 1095/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4300 - accuracy: 0.5390 - val_loss: 0.7377 - val_accuracy: 0.3738\n",
      "Epoch 1096/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4302 - accuracy: 0.5372 - val_loss: 0.7417 - val_accuracy: 0.3715\n",
      "Epoch 1097/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4336 - accuracy: 0.5370 - val_loss: 0.7331 - val_accuracy: 0.3711\n",
      "Epoch 1098/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4264 - accuracy: 0.5412 - val_loss: 0.7462 - val_accuracy: 0.3676\n",
      "Epoch 1099/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4287 - accuracy: 0.5367 - val_loss: 0.7487 - val_accuracy: 0.3757\n",
      "Epoch 1100/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4259 - accuracy: 0.5432 - val_loss: 0.7378 - val_accuracy: 0.3741\n",
      "Epoch 1101/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4372 - accuracy: 0.5344 - val_loss: 0.7390 - val_accuracy: 0.3762\n",
      "Epoch 1102/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.4286 - accuracy: 0.5420 - val_loss: 0.7417 - val_accuracy: 0.3768\n",
      "Epoch 1103/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4246 - accuracy: 0.5415 - val_loss: 0.7421 - val_accuracy: 0.3757\n",
      "Epoch 1104/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4255 - accuracy: 0.5452 - val_loss: 0.7483 - val_accuracy: 0.3765\n",
      "Epoch 1105/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4244 - accuracy: 0.5424 - val_loss: 0.7428 - val_accuracy: 0.3778\n",
      "Epoch 1106/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4339 - accuracy: 0.5344 - val_loss: 0.7586 - val_accuracy: 0.3772\n",
      "Epoch 1107/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4324 - accuracy: 0.5350 - val_loss: 0.7370 - val_accuracy: 0.3789\n",
      "Epoch 1108/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4703 - accuracy: 0.5081 - val_loss: 0.7297 - val_accuracy: 0.3745\n",
      "Epoch 1109/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4452 - accuracy: 0.5272 - val_loss: 0.7451 - val_accuracy: 0.3781\n",
      "Epoch 1110/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.4341 - accuracy: 0.5375 - val_loss: 0.7419 - val_accuracy: 0.3793\n",
      "Epoch 1111/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4284 - accuracy: 0.5410 - val_loss: 0.7485 - val_accuracy: 0.3742\n",
      "Epoch 1112/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4339 - accuracy: 0.5357 - val_loss: 0.7440 - val_accuracy: 0.3693\n",
      "Epoch 1113/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4302 - accuracy: 0.5411 - val_loss: 0.7479 - val_accuracy: 0.3708\n",
      "Epoch 1114/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4317 - accuracy: 0.5377 - val_loss: 0.7532 - val_accuracy: 0.3744\n",
      "Epoch 1115/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4246 - accuracy: 0.5432 - val_loss: 0.7529 - val_accuracy: 0.3745\n",
      "Epoch 1116/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4340 - accuracy: 0.5349 - val_loss: 0.7380 - val_accuracy: 0.3735\n",
      "Epoch 1117/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4287 - accuracy: 0.5412 - val_loss: 0.7415 - val_accuracy: 0.3709\n",
      "Epoch 1118/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4452 - accuracy: 0.5285 - val_loss: 0.7265 - val_accuracy: 0.3745\n",
      "Epoch 1119/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4366 - accuracy: 0.5373 - val_loss: 0.7347 - val_accuracy: 0.3744\n",
      "Epoch 1120/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4363 - accuracy: 0.5350 - val_loss: 0.7245 - val_accuracy: 0.3720\n",
      "Epoch 1121/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4363 - accuracy: 0.5345 - val_loss: 0.7336 - val_accuracy: 0.3690\n",
      "Epoch 1122/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4307 - accuracy: 0.5434 - val_loss: 0.7451 - val_accuracy: 0.3715\n",
      "Epoch 1123/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4253 - accuracy: 0.5448 - val_loss: 0.7471 - val_accuracy: 0.3771\n",
      "Epoch 1124/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.4313 - accuracy: 0.5373 - val_loss: 0.7539 - val_accuracy: 0.3759\n",
      "Epoch 1125/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.4319 - accuracy: 0.5378 - val_loss: 0.7303 - val_accuracy: 0.3652\n",
      "Epoch 1126/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.4306 - accuracy: 0.5423 - val_loss: 0.7329 - val_accuracy: 0.3733\n",
      "Epoch 1127/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.4280 - accuracy: 0.5432 - val_loss: 0.7446 - val_accuracy: 0.3742\n",
      "Epoch 1128/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4273 - accuracy: 0.5432 - val_loss: 0.7467 - val_accuracy: 0.3736\n",
      "Epoch 1129/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.4252 - accuracy: 0.5448 - val_loss: 0.7419 - val_accuracy: 0.3769\n",
      "Epoch 1130/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4269 - accuracy: 0.5408 - val_loss: 0.7391 - val_accuracy: 0.3714\n",
      "Epoch 1131/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4339 - accuracy: 0.5347 - val_loss: 0.7411 - val_accuracy: 0.3738\n",
      "Epoch 1132/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.4294 - accuracy: 0.5405 - val_loss: 0.7395 - val_accuracy: 0.3750\n",
      "Epoch 1133/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.4298 - accuracy: 0.5410 - val_loss: 0.7389 - val_accuracy: 0.3739\n",
      "Epoch 1134/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.4231 - accuracy: 0.5447 - val_loss: 0.7445 - val_accuracy: 0.3742\n",
      "Epoch 1135/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4329 - accuracy: 0.5376 - val_loss: 0.7311 - val_accuracy: 0.3676\n",
      "Epoch 1136/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4247 - accuracy: 0.5453 - val_loss: 0.7416 - val_accuracy: 0.3715\n",
      "Epoch 1137/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4309 - accuracy: 0.5412 - val_loss: 0.7407 - val_accuracy: 0.3702\n",
      "Epoch 1138/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4351 - accuracy: 0.5357 - val_loss: 0.7326 - val_accuracy: 0.3766\n",
      "Epoch 1139/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.4281 - accuracy: 0.5438 - val_loss: 0.7381 - val_accuracy: 0.3775\n",
      "Epoch 1140/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4281 - accuracy: 0.5407 - val_loss: 0.7490 - val_accuracy: 0.3766\n",
      "Epoch 1141/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.4261 - accuracy: 0.5444 - val_loss: 0.7322 - val_accuracy: 0.3700\n",
      "Epoch 1142/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4244 - accuracy: 0.5447 - val_loss: 0.7508 - val_accuracy: 0.3717\n",
      "Epoch 1143/2500\n",
      "20056/20056 [==============================] - 2s 81us/step - loss: 0.4238 - accuracy: 0.5479 - val_loss: 0.7350 - val_accuracy: 0.3718\n",
      "Epoch 1144/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4273 - accuracy: 0.5433 - val_loss: 0.7573 - val_accuracy: 0.3684\n",
      "Epoch 1145/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4257 - accuracy: 0.5446 - val_loss: 0.7430 - val_accuracy: 0.3760\n",
      "Epoch 1146/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4289 - accuracy: 0.5456 - val_loss: 0.7531 - val_accuracy: 0.3750\n",
      "Epoch 1147/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4268 - accuracy: 0.5445 - val_loss: 0.7465 - val_accuracy: 0.3751\n",
      "Epoch 1148/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4227 - accuracy: 0.5455 - val_loss: 0.7461 - val_accuracy: 0.3783\n",
      "Epoch 1149/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.4219 - accuracy: 0.5470 - val_loss: 0.7528 - val_accuracy: 0.3727\n",
      "Epoch 1150/2500\n",
      "20056/20056 [==============================] - 2s 87us/step - loss: 0.4408 - accuracy: 0.5289 - val_loss: 0.7380 - val_accuracy: 0.3735\n",
      "Epoch 1151/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.4368 - accuracy: 0.5304 - val_loss: 0.7378 - val_accuracy: 0.3736\n",
      "Epoch 1152/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.4250 - accuracy: 0.5454 - val_loss: 0.7581 - val_accuracy: 0.3703\n",
      "Epoch 1153/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.4306 - accuracy: 0.5399 - val_loss: 0.7412 - val_accuracy: 0.3721\n",
      "Epoch 1154/2500\n",
      "20056/20056 [==============================] - 2s 87us/step - loss: 0.4305 - accuracy: 0.5437 - val_loss: 0.7403 - val_accuracy: 0.3723\n",
      "Epoch 1155/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4332 - accuracy: 0.5389 - val_loss: 0.7328 - val_accuracy: 0.3693\n",
      "Epoch 1156/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4286 - accuracy: 0.5388 - val_loss: 0.7425 - val_accuracy: 0.3691\n",
      "Epoch 1157/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4360 - accuracy: 0.5340 - val_loss: 0.7521 - val_accuracy: 0.3792\n",
      "Epoch 1158/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.4357 - accuracy: 0.5394 - val_loss: 0.7507 - val_accuracy: 0.3736\n",
      "Epoch 1159/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.4225 - accuracy: 0.5468 - val_loss: 0.7490 - val_accuracy: 0.3720\n",
      "Epoch 1160/2500\n",
      "20056/20056 [==============================] - 2s 77us/step - loss: 0.4317 - accuracy: 0.5384 - val_loss: 0.7351 - val_accuracy: 0.3786\n",
      "Epoch 1161/2500\n",
      "20056/20056 [==============================] - 2s 86us/step - loss: 0.4264 - accuracy: 0.5464 - val_loss: 0.7435 - val_accuracy: 0.3717\n",
      "Epoch 1162/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.4224 - accuracy: 0.5433 - val_loss: 0.7502 - val_accuracy: 0.3774\n",
      "Epoch 1163/2500\n",
      "20056/20056 [==============================] - 2s 86us/step - loss: 0.4258 - accuracy: 0.5464 - val_loss: 0.7507 - val_accuracy: 0.3781\n",
      "Epoch 1164/2500\n",
      "20056/20056 [==============================] - 2s 80us/step - loss: 0.4341 - accuracy: 0.5403 - val_loss: 0.7370 - val_accuracy: 0.3783\n",
      "Epoch 1165/2500\n",
      "20056/20056 [==============================] - 2s 79us/step - loss: 0.4240 - accuracy: 0.5443 - val_loss: 0.7443 - val_accuracy: 0.3747\n",
      "Epoch 1166/2500\n",
      "20056/20056 [==============================] - 2s 90us/step - loss: 0.4257 - accuracy: 0.5460 - val_loss: 0.7431 - val_accuracy: 0.3815\n",
      "Epoch 1167/2500\n",
      "20056/20056 [==============================] - 2s 80us/step - loss: 0.4254 - accuracy: 0.5444 - val_loss: 0.7590 - val_accuracy: 0.3745\n",
      "Epoch 1168/2500\n",
      "20056/20056 [==============================] - 2s 80us/step - loss: 0.4226 - accuracy: 0.5454 - val_loss: 0.7468 - val_accuracy: 0.3789\n",
      "Epoch 1169/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.4210 - accuracy: 0.5472 - val_loss: 0.7585 - val_accuracy: 0.3808\n",
      "Epoch 1170/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4217 - accuracy: 0.5460 - val_loss: 0.7660 - val_accuracy: 0.3751\n",
      "Epoch 1171/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4258 - accuracy: 0.5406 - val_loss: 0.7467 - val_accuracy: 0.3733\n",
      "Epoch 1172/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4288 - accuracy: 0.5426 - val_loss: 0.7439 - val_accuracy: 0.3759\n",
      "Epoch 1173/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4239 - accuracy: 0.5453 - val_loss: 0.7556 - val_accuracy: 0.3762\n",
      "Epoch 1174/2500\n",
      "20056/20056 [==============================] - 2s 80us/step - loss: 0.4295 - accuracy: 0.5412 - val_loss: 0.7404 - val_accuracy: 0.3711\n",
      "Epoch 1175/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4216 - accuracy: 0.5454 - val_loss: 0.7504 - val_accuracy: 0.3706\n",
      "Epoch 1176/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.4262 - accuracy: 0.5434 - val_loss: 0.7503 - val_accuracy: 0.3757\n",
      "Epoch 1177/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4321 - accuracy: 0.5406 - val_loss: 0.7427 - val_accuracy: 0.3741\n",
      "Epoch 1178/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4320 - accuracy: 0.5350 - val_loss: 0.7515 - val_accuracy: 0.3774\n",
      "Epoch 1179/2500\n",
      "20056/20056 [==============================] - 2s 77us/step - loss: 0.4218 - accuracy: 0.5467 - val_loss: 0.7521 - val_accuracy: 0.3783\n",
      "Epoch 1180/2500\n",
      "20056/20056 [==============================] - 2s 83us/step - loss: 0.4184 - accuracy: 0.5476 - val_loss: 0.7538 - val_accuracy: 0.3757\n",
      "Epoch 1181/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4273 - accuracy: 0.5437 - val_loss: 0.7368 - val_accuracy: 0.3750\n",
      "Epoch 1182/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4251 - accuracy: 0.5416 - val_loss: 0.7586 - val_accuracy: 0.3774\n",
      "Epoch 1183/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4184 - accuracy: 0.5510 - val_loss: 0.7462 - val_accuracy: 0.3744\n",
      "Epoch 1184/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.4191 - accuracy: 0.5501 - val_loss: 0.7470 - val_accuracy: 0.3744\n",
      "Epoch 1185/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4268 - accuracy: 0.5424 - val_loss: 0.7399 - val_accuracy: 0.3742\n",
      "Epoch 1186/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.4275 - accuracy: 0.5454 - val_loss: 0.7505 - val_accuracy: 0.3781\n",
      "Epoch 1187/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4173 - accuracy: 0.5544 - val_loss: 0.7409 - val_accuracy: 0.3748\n",
      "Epoch 1188/2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4225 - accuracy: 0.5420 - val_loss: 0.7491 - val_accuracy: 0.3765\n",
      "Epoch 1189/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4164 - accuracy: 0.5518 - val_loss: 0.7480 - val_accuracy: 0.3756\n",
      "Epoch 1190/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4192 - accuracy: 0.5508 - val_loss: 0.7546 - val_accuracy: 0.3751\n",
      "Epoch 1191/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4271 - accuracy: 0.5425 - val_loss: 0.7376 - val_accuracy: 0.3703\n",
      "Epoch 1192/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4332 - accuracy: 0.5364 - val_loss: 0.7426 - val_accuracy: 0.3700\n",
      "Epoch 1193/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4236 - accuracy: 0.5469 - val_loss: 0.7585 - val_accuracy: 0.3774\n",
      "Epoch 1194/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4253 - accuracy: 0.5464 - val_loss: 0.7449 - val_accuracy: 0.3720\n",
      "Epoch 1195/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4178 - accuracy: 0.5515 - val_loss: 0.7546 - val_accuracy: 0.3806\n",
      "Epoch 1196/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.4219 - accuracy: 0.5468 - val_loss: 0.7524 - val_accuracy: 0.3775\n",
      "Epoch 1197/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4241 - accuracy: 0.5473 - val_loss: 0.7495 - val_accuracy: 0.3777\n",
      "Epoch 1198/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4313 - accuracy: 0.5437 - val_loss: 0.7341 - val_accuracy: 0.3726\n",
      "Epoch 1199/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4247 - accuracy: 0.5449 - val_loss: 0.7519 - val_accuracy: 0.3727\n",
      "Epoch 1200/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.4211 - accuracy: 0.5460 - val_loss: 0.7487 - val_accuracy: 0.3720\n",
      "Epoch 1201/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.4219 - accuracy: 0.5464 - val_loss: 0.7564 - val_accuracy: 0.3735\n",
      "Epoch 1202/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.4217 - accuracy: 0.5471 - val_loss: 0.7528 - val_accuracy: 0.3738\n",
      "Epoch 1203/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.4220 - accuracy: 0.5530 - val_loss: 0.7547 - val_accuracy: 0.3739\n",
      "Epoch 1204/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4181 - accuracy: 0.5537 - val_loss: 0.7434 - val_accuracy: 0.3757\n",
      "Epoch 1205/2500\n",
      "20056/20056 [==============================] - 2s 102us/step - loss: 0.4302 - accuracy: 0.5399 - val_loss: 0.7381 - val_accuracy: 0.3792\n",
      "Epoch 1206/2500\n",
      "20056/20056 [==============================] - 2s 95us/step - loss: 0.4264 - accuracy: 0.5413 - val_loss: 0.7485 - val_accuracy: 0.3729\n",
      "Epoch 1207/2500\n",
      "20056/20056 [==============================] - 2s 96us/step - loss: 0.4265 - accuracy: 0.5454 - val_loss: 0.7444 - val_accuracy: 0.3729\n",
      "Epoch 1208/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.4210 - accuracy: 0.5475 - val_loss: 0.7495 - val_accuracy: 0.3771\n",
      "Epoch 1209/2500\n",
      "20056/20056 [==============================] - 1s 75us/step - loss: 0.4257 - accuracy: 0.5460 - val_loss: 0.7533 - val_accuracy: 0.3708\n",
      "Epoch 1210/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.4188 - accuracy: 0.5506 - val_loss: 0.7513 - val_accuracy: 0.3769\n",
      "Epoch 1211/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4203 - accuracy: 0.5460 - val_loss: 0.7618 - val_accuracy: 0.3802\n",
      "Epoch 1212/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4228 - accuracy: 0.5456 - val_loss: 0.7492 - val_accuracy: 0.3744\n",
      "Epoch 1213/2500\n",
      "20056/20056 [==============================] - 2s 80us/step - loss: 0.4261 - accuracy: 0.5385 - val_loss: 0.7370 - val_accuracy: 0.3685\n",
      "Epoch 1214/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.4311 - accuracy: 0.5428 - val_loss: 0.7269 - val_accuracy: 0.3739\n",
      "Epoch 1215/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.4246 - accuracy: 0.5430 - val_loss: 0.7279 - val_accuracy: 0.3670\n",
      "Epoch 1216/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4257 - accuracy: 0.5454 - val_loss: 0.7524 - val_accuracy: 0.3774\n",
      "Epoch 1217/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4202 - accuracy: 0.5489 - val_loss: 0.7501 - val_accuracy: 0.3756\n",
      "Epoch 1218/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4208 - accuracy: 0.5472 - val_loss: 0.7466 - val_accuracy: 0.3774\n",
      "Epoch 1219/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.4280 - accuracy: 0.5438 - val_loss: 0.7515 - val_accuracy: 0.3711\n",
      "Epoch 1220/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.4197 - accuracy: 0.5482 - val_loss: 0.7452 - val_accuracy: 0.3751\n",
      "Epoch 1221/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.4226 - accuracy: 0.5472 - val_loss: 0.7417 - val_accuracy: 0.3742\n",
      "Epoch 1222/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.4198 - accuracy: 0.5503 - val_loss: 0.7494 - val_accuracy: 0.3729\n",
      "Epoch 1223/2500\n",
      "20056/20056 [==============================] - 1s 75us/step - loss: 0.4177 - accuracy: 0.5518 - val_loss: 0.7570 - val_accuracy: 0.3739\n",
      "Epoch 1224/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4270 - accuracy: 0.5404 - val_loss: 0.7476 - val_accuracy: 0.3718\n",
      "Epoch 1225/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4178 - accuracy: 0.5501 - val_loss: 0.7684 - val_accuracy: 0.3751\n",
      "Epoch 1226/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4176 - accuracy: 0.5509 - val_loss: 0.7578 - val_accuracy: 0.3763\n",
      "Epoch 1227/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.4235 - accuracy: 0.5446 - val_loss: 0.7398 - val_accuracy: 0.3706\n",
      "Epoch 1228/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4260 - accuracy: 0.5448 - val_loss: 0.7365 - val_accuracy: 0.3739\n",
      "Epoch 1229/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4164 - accuracy: 0.5507 - val_loss: 0.7500 - val_accuracy: 0.3711\n",
      "Epoch 1230/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4222 - accuracy: 0.5475 - val_loss: 0.7492 - val_accuracy: 0.3700\n",
      "Epoch 1231/2500\n",
      "20056/20056 [==============================] - 2s 78us/step - loss: 0.4332 - accuracy: 0.5349 - val_loss: 0.7352 - val_accuracy: 0.3694\n",
      "Epoch 1232/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.4175 - accuracy: 0.5485 - val_loss: 0.7640 - val_accuracy: 0.3756\n",
      "Epoch 1233/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.4195 - accuracy: 0.5525 - val_loss: 0.7371 - val_accuracy: 0.3727\n",
      "Epoch 1234/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.4179 - accuracy: 0.5536 - val_loss: 0.7521 - val_accuracy: 0.3682\n",
      "Epoch 1235/2500\n",
      "20056/20056 [==============================] - 2s 82us/step - loss: 0.4205 - accuracy: 0.5512 - val_loss: 0.7468 - val_accuracy: 0.3673\n",
      "Epoch 1236/2500\n",
      "20056/20056 [==============================] - 2s 85us/step - loss: 0.4176 - accuracy: 0.5536 - val_loss: 0.7544 - val_accuracy: 0.3760\n",
      "Epoch 1237/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.4160 - accuracy: 0.5558 - val_loss: 0.7638 - val_accuracy: 0.3759\n",
      "Epoch 1238/2500\n",
      "20056/20056 [==============================] - 2s 82us/step - loss: 0.4200 - accuracy: 0.5525 - val_loss: 0.7507 - val_accuracy: 0.3739\n",
      "Epoch 1239/2500\n",
      "20056/20056 [==============================] - 2s 86us/step - loss: 0.4205 - accuracy: 0.5471 - val_loss: 0.7496 - val_accuracy: 0.3742\n",
      "Epoch 1240/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.4186 - accuracy: 0.5504 - val_loss: 0.7451 - val_accuracy: 0.3733\n",
      "Epoch 1241/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.4200 - accuracy: 0.5498 - val_loss: 0.7445 - val_accuracy: 0.3727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1242/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.4215 - accuracy: 0.5468 - val_loss: 0.7609 - val_accuracy: 0.3733\n",
      "Epoch 1243/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.4169 - accuracy: 0.5501 - val_loss: 0.7462 - val_accuracy: 0.3687\n",
      "Epoch 1244/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.4197 - accuracy: 0.5478 - val_loss: 0.7545 - val_accuracy: 0.3709\n",
      "Epoch 1245/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.4316 - accuracy: 0.5383 - val_loss: 0.7369 - val_accuracy: 0.3669\n",
      "Epoch 1246/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4258 - accuracy: 0.5435 - val_loss: 0.7412 - val_accuracy: 0.3715\n",
      "Epoch 1247/2500\n",
      "20056/20056 [==============================] - 2s 87us/step - loss: 0.4242 - accuracy: 0.5420 - val_loss: 0.7504 - val_accuracy: 0.3727\n",
      "Epoch 1248/2500\n",
      "20056/20056 [==============================] - 2s 88us/step - loss: 0.4249 - accuracy: 0.5453 - val_loss: 0.7428 - val_accuracy: 0.3694\n",
      "Epoch 1249/2500\n",
      "20056/20056 [==============================] - 2s 92us/step - loss: 0.4282 - accuracy: 0.5389 - val_loss: 0.7436 - val_accuracy: 0.3748\n",
      "Epoch 1250/2500\n",
      "20056/20056 [==============================] - 2s 89us/step - loss: 0.4241 - accuracy: 0.5424 - val_loss: 0.7442 - val_accuracy: 0.3750\n",
      "Epoch 1251/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.4229 - accuracy: 0.5473 - val_loss: 0.7474 - val_accuracy: 0.3751\n",
      "Epoch 1252/2500\n",
      "20056/20056 [==============================] - 2s 89us/step - loss: 0.4154 - accuracy: 0.5493 - val_loss: 0.7563 - val_accuracy: 0.3699\n",
      "Epoch 1253/2500\n",
      "20056/20056 [==============================] - 2s 80us/step - loss: 0.4327 - accuracy: 0.5381 - val_loss: 0.7479 - val_accuracy: 0.3759\n",
      "Epoch 1254/2500\n",
      "20056/20056 [==============================] - 2s 87us/step - loss: 0.4183 - accuracy: 0.5475 - val_loss: 0.7527 - val_accuracy: 0.3769\n",
      "Epoch 1255/2500\n",
      "20056/20056 [==============================] - 2s 84us/step - loss: 0.4185 - accuracy: 0.5487 - val_loss: 0.7586 - val_accuracy: 0.3696\n",
      "Epoch 1256/2500\n",
      "20056/20056 [==============================] - 2s 87us/step - loss: 0.4247 - accuracy: 0.5447 - val_loss: 0.7514 - val_accuracy: 0.3744\n",
      "Epoch 1257/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.4246 - accuracy: 0.5442 - val_loss: 0.7427 - val_accuracy: 0.3688\n",
      "Epoch 1258/2500\n",
      "20056/20056 [==============================] - 2s 80us/step - loss: 0.4161 - accuracy: 0.5512 - val_loss: 0.7522 - val_accuracy: 0.3756\n",
      "Epoch 1259/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.4264 - accuracy: 0.5414 - val_loss: 0.7376 - val_accuracy: 0.3784\n",
      "Epoch 1260/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.4247 - accuracy: 0.5433 - val_loss: 0.7347 - val_accuracy: 0.3700\n",
      "Epoch 1261/2500\n",
      "20056/20056 [==============================] - 2s 80us/step - loss: 0.4304 - accuracy: 0.5401 - val_loss: 0.7447 - val_accuracy: 0.3639\n",
      "Epoch 1262/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.4262 - accuracy: 0.5458 - val_loss: 0.7311 - val_accuracy: 0.3726\n",
      "Epoch 1263/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.4246 - accuracy: 0.5457 - val_loss: 0.7326 - val_accuracy: 0.3721\n",
      "Epoch 1264/2500\n",
      "20056/20056 [==============================] - 2s 78us/step - loss: 0.4157 - accuracy: 0.5526 - val_loss: 0.7546 - val_accuracy: 0.3774\n",
      "Epoch 1265/2500\n",
      "20056/20056 [==============================] - 2s 79us/step - loss: 0.4196 - accuracy: 0.5459 - val_loss: 0.7433 - val_accuracy: 0.3699\n",
      "Epoch 1266/2500\n",
      "20056/20056 [==============================] - 2s 80us/step - loss: 0.4168 - accuracy: 0.5508 - val_loss: 0.7436 - val_accuracy: 0.3714\n",
      "Epoch 1267/2500\n",
      "20056/20056 [==============================] - 2s 85us/step - loss: 0.4154 - accuracy: 0.5536 - val_loss: 0.7517 - val_accuracy: 0.3783\n",
      "Epoch 1268/2500\n",
      "20056/20056 [==============================] - 2s 85us/step - loss: 0.4214 - accuracy: 0.5475 - val_loss: 0.7569 - val_accuracy: 0.3736\n",
      "Epoch 1269/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.4132 - accuracy: 0.5566 - val_loss: 0.7531 - val_accuracy: 0.3702\n",
      "Epoch 1270/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.4189 - accuracy: 0.5495 - val_loss: 0.7505 - val_accuracy: 0.3664\n",
      "Epoch 1271/2500\n",
      "20056/20056 [==============================] - 2s 75us/step - loss: 0.4144 - accuracy: 0.5541 - val_loss: 0.7330 - val_accuracy: 0.3705\n",
      "Epoch 1272/2500\n",
      "20056/20056 [==============================] - 2s 88us/step - loss: 0.4130 - accuracy: 0.5546 - val_loss: 0.7469 - val_accuracy: 0.3691\n",
      "Epoch 1273/2500\n",
      "20056/20056 [==============================] - 2s 85us/step - loss: 0.4131 - accuracy: 0.5532 - val_loss: 0.7589 - val_accuracy: 0.3736\n",
      "Epoch 1274/2500\n",
      "20056/20056 [==============================] - 2s 99us/step - loss: 0.4173 - accuracy: 0.5542 - val_loss: 0.7408 - val_accuracy: 0.3711\n",
      "Epoch 1275/2500\n",
      "20056/20056 [==============================] - 2s 99us/step - loss: 0.4200 - accuracy: 0.5480 - val_loss: 0.7620 - val_accuracy: 0.3751\n",
      "Epoch 1276/2500\n",
      "20056/20056 [==============================] - 2s 87us/step - loss: 0.4193 - accuracy: 0.5465 - val_loss: 0.7501 - val_accuracy: 0.3705\n",
      "Epoch 1277/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.4126 - accuracy: 0.5561 - val_loss: 0.7455 - val_accuracy: 0.3724\n",
      "Epoch 1278/2500\n",
      "20056/20056 [==============================] - 2s 100us/step - loss: 0.4173 - accuracy: 0.5496 - val_loss: 0.7438 - val_accuracy: 0.3714\n",
      "Epoch 1279/2500\n",
      "20056/20056 [==============================] - 2s 93us/step - loss: 0.4169 - accuracy: 0.5503 - val_loss: 0.7554 - val_accuracy: 0.3726\n",
      "Epoch 1280/2500\n",
      "20056/20056 [==============================] - 2s 81us/step - loss: 0.4184 - accuracy: 0.5497 - val_loss: 0.7571 - val_accuracy: 0.3724\n",
      "Epoch 1281/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.4111 - accuracy: 0.5567 - val_loss: 0.7581 - val_accuracy: 0.3702\n",
      "Epoch 1282/2500\n",
      "20056/20056 [==============================] - 2s 79us/step - loss: 0.4151 - accuracy: 0.5521 - val_loss: 0.7687 - val_accuracy: 0.3708\n",
      "Epoch 1283/2500\n",
      "20056/20056 [==============================] - 2s 80us/step - loss: 0.4108 - accuracy: 0.5588 - val_loss: 0.7625 - val_accuracy: 0.3739\n",
      "Epoch 1284/2500\n",
      "20056/20056 [==============================] - 2s 77us/step - loss: 0.4202 - accuracy: 0.5507 - val_loss: 0.7545 - val_accuracy: 0.3721\n",
      "Epoch 1285/2500\n",
      "20056/20056 [==============================] - 2s 75us/step - loss: 0.4219 - accuracy: 0.5477 - val_loss: 0.7533 - val_accuracy: 0.3756\n",
      "Epoch 1286/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.4146 - accuracy: 0.5529 - val_loss: 0.7556 - val_accuracy: 0.3777\n",
      "Epoch 1287/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.4157 - accuracy: 0.5563 - val_loss: 0.7645 - val_accuracy: 0.3759\n",
      "Epoch 1288/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.4150 - accuracy: 0.5540 - val_loss: 0.7417 - val_accuracy: 0.3742\n",
      "Epoch 1289/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.4084 - accuracy: 0.5590 - val_loss: 0.7558 - val_accuracy: 0.3726\n",
      "Epoch 1290/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.4178 - accuracy: 0.5494 - val_loss: 0.7482 - val_accuracy: 0.3709\n",
      "Epoch 1291/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.4148 - accuracy: 0.5537 - val_loss: 0.7517 - val_accuracy: 0.3751\n",
      "Epoch 1292/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.4215 - accuracy: 0.5494 - val_loss: 0.7496 - val_accuracy: 0.3705\n",
      "Epoch 1293/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.4189 - accuracy: 0.5502 - val_loss: 0.7453 - val_accuracy: 0.3729\n",
      "Epoch 1294/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.4152 - accuracy: 0.5529 - val_loss: 0.7573 - val_accuracy: 0.3718\n",
      "Epoch 1295/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.4153 - accuracy: 0.5531 - val_loss: 0.7447 - val_accuracy: 0.3729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1296/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4217 - accuracy: 0.5483 - val_loss: 0.7458 - val_accuracy: 0.3718\n",
      "Epoch 1297/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4217 - accuracy: 0.5469 - val_loss: 0.7591 - val_accuracy: 0.3679\n",
      "Epoch 1298/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4186 - accuracy: 0.5482 - val_loss: 0.7500 - val_accuracy: 0.3726\n",
      "Epoch 1299/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.4183 - accuracy: 0.5494 - val_loss: 0.7506 - val_accuracy: 0.3744\n",
      "Epoch 1300/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4766 - accuracy: 0.5065 - val_loss: 0.7258 - val_accuracy: 0.3648\n",
      "Epoch 1301/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4382 - accuracy: 0.5321 - val_loss: 0.7452 - val_accuracy: 0.3717\n",
      "Epoch 1302/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.4222 - accuracy: 0.5477 - val_loss: 0.7545 - val_accuracy: 0.3735\n",
      "Epoch 1303/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4149 - accuracy: 0.5571 - val_loss: 0.7591 - val_accuracy: 0.3726\n",
      "Epoch 1304/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4156 - accuracy: 0.5541 - val_loss: 0.7451 - val_accuracy: 0.3772\n",
      "Epoch 1305/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.4174 - accuracy: 0.5514 - val_loss: 0.7434 - val_accuracy: 0.3748\n",
      "Epoch 1306/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.4152 - accuracy: 0.5517 - val_loss: 0.7483 - val_accuracy: 0.3751\n",
      "Epoch 1307/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4065 - accuracy: 0.5619 - val_loss: 0.7479 - val_accuracy: 0.3729\n",
      "Epoch 1308/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4167 - accuracy: 0.5540 - val_loss: 0.7674 - val_accuracy: 0.3786\n",
      "Epoch 1309/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4197 - accuracy: 0.5522 - val_loss: 0.7571 - val_accuracy: 0.3786\n",
      "Epoch 1310/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4219 - accuracy: 0.5488 - val_loss: 0.7558 - val_accuracy: 0.3777\n",
      "Epoch 1311/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4180 - accuracy: 0.5544 - val_loss: 0.7450 - val_accuracy: 0.3700\n",
      "Epoch 1312/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4154 - accuracy: 0.5538 - val_loss: 0.7547 - val_accuracy: 0.3717\n",
      "Epoch 1313/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4115 - accuracy: 0.5579 - val_loss: 0.7584 - val_accuracy: 0.3775\n",
      "Epoch 1314/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4160 - accuracy: 0.5550 - val_loss: 0.7399 - val_accuracy: 0.3729\n",
      "Epoch 1315/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4171 - accuracy: 0.5531 - val_loss: 0.7525 - val_accuracy: 0.3744\n",
      "Epoch 1316/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4198 - accuracy: 0.5475 - val_loss: 0.7604 - val_accuracy: 0.3793\n",
      "Epoch 1317/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.4150 - accuracy: 0.5544 - val_loss: 0.7531 - val_accuracy: 0.3691\n",
      "Epoch 1318/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4149 - accuracy: 0.5536 - val_loss: 0.7662 - val_accuracy: 0.3748\n",
      "Epoch 1319/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4108 - accuracy: 0.5578 - val_loss: 0.7545 - val_accuracy: 0.3751\n",
      "Epoch 1320/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4087 - accuracy: 0.5586 - val_loss: 0.7487 - val_accuracy: 0.3757\n",
      "Epoch 1321/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.4100 - accuracy: 0.5563 - val_loss: 0.7487 - val_accuracy: 0.3703\n",
      "Epoch 1322/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4179 - accuracy: 0.5540 - val_loss: 0.7572 - val_accuracy: 0.3729\n",
      "Epoch 1323/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.4128 - accuracy: 0.5543 - val_loss: 0.7548 - val_accuracy: 0.3729\n",
      "Epoch 1324/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.4134 - accuracy: 0.5561 - val_loss: 0.7522 - val_accuracy: 0.3717\n",
      "Epoch 1325/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.4161 - accuracy: 0.5555 - val_loss: 0.7360 - val_accuracy: 0.3771\n",
      "Epoch 1326/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4202 - accuracy: 0.5485 - val_loss: 0.7647 - val_accuracy: 0.3789\n",
      "Epoch 1327/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4208 - accuracy: 0.5519 - val_loss: 0.7573 - val_accuracy: 0.3762\n",
      "Epoch 1328/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.4127 - accuracy: 0.5515 - val_loss: 0.7674 - val_accuracy: 0.3774\n",
      "Epoch 1329/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4183 - accuracy: 0.5515 - val_loss: 0.7416 - val_accuracy: 0.3780\n",
      "Epoch 1330/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4211 - accuracy: 0.5513 - val_loss: 0.7459 - val_accuracy: 0.3736\n",
      "Epoch 1331/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.4245 - accuracy: 0.5428 - val_loss: 0.7550 - val_accuracy: 0.3703\n",
      "Epoch 1332/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.4217 - accuracy: 0.5449 - val_loss: 0.7568 - val_accuracy: 0.3721\n",
      "Epoch 1333/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.4160 - accuracy: 0.5551 - val_loss: 0.7538 - val_accuracy: 0.3763\n",
      "Epoch 1334/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.4128 - accuracy: 0.5550 - val_loss: 0.7597 - val_accuracy: 0.3793\n",
      "Epoch 1335/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4149 - accuracy: 0.5556 - val_loss: 0.7458 - val_accuracy: 0.3777\n",
      "Epoch 1336/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.4188 - accuracy: 0.5528 - val_loss: 0.7464 - val_accuracy: 0.3730\n",
      "Epoch 1337/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4155 - accuracy: 0.5482 - val_loss: 0.7497 - val_accuracy: 0.3757\n",
      "Epoch 1338/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4077 - accuracy: 0.5596 - val_loss: 0.7501 - val_accuracy: 0.3673\n",
      "Epoch 1339/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4177 - accuracy: 0.5533 - val_loss: 0.7579 - val_accuracy: 0.3747\n",
      "Epoch 1340/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4171 - accuracy: 0.5569 - val_loss: 0.7564 - val_accuracy: 0.3754\n",
      "Epoch 1341/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4109 - accuracy: 0.5602 - val_loss: 0.7528 - val_accuracy: 0.3733\n",
      "Epoch 1342/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.4112 - accuracy: 0.5569 - val_loss: 0.7585 - val_accuracy: 0.3750\n",
      "Epoch 1343/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4087 - accuracy: 0.5561 - val_loss: 0.7554 - val_accuracy: 0.3765\n",
      "Epoch 1344/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.4100 - accuracy: 0.5565 - val_loss: 0.7642 - val_accuracy: 0.3783\n",
      "Epoch 1345/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.4181 - accuracy: 0.5536 - val_loss: 0.7602 - val_accuracy: 0.3651\n",
      "Epoch 1346/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4225 - accuracy: 0.5430 - val_loss: 0.7576 - val_accuracy: 0.3685\n",
      "Epoch 1347/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4178 - accuracy: 0.5523 - val_loss: 0.7389 - val_accuracy: 0.3706\n",
      "Epoch 1348/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.4172 - accuracy: 0.5502 - val_loss: 0.7541 - val_accuracy: 0.3726\n",
      "Epoch 1349/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.4182 - accuracy: 0.5508 - val_loss: 0.7493 - val_accuracy: 0.3732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1350/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.4092 - accuracy: 0.5562 - val_loss: 0.7446 - val_accuracy: 0.3733\n",
      "Epoch 1351/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4061 - accuracy: 0.5606 - val_loss: 0.7580 - val_accuracy: 0.3757\n",
      "Epoch 1352/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.4149 - accuracy: 0.5545 - val_loss: 0.7493 - val_accuracy: 0.3720\n",
      "Epoch 1353/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.4141 - accuracy: 0.5539 - val_loss: 0.7567 - val_accuracy: 0.3754\n",
      "Epoch 1354/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.4096 - accuracy: 0.5570 - val_loss: 0.7439 - val_accuracy: 0.3786\n",
      "Epoch 1355/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.4156 - accuracy: 0.5511 - val_loss: 0.7418 - val_accuracy: 0.3672\n",
      "Epoch 1356/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.4121 - accuracy: 0.5539 - val_loss: 0.7432 - val_accuracy: 0.3739\n",
      "Epoch 1357/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4158 - accuracy: 0.5526 - val_loss: 0.7430 - val_accuracy: 0.3720\n",
      "Epoch 1358/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4202 - accuracy: 0.5501 - val_loss: 0.7418 - val_accuracy: 0.3721\n",
      "Epoch 1359/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.4117 - accuracy: 0.5551 - val_loss: 0.7566 - val_accuracy: 0.3757\n",
      "Epoch 1360/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4184 - accuracy: 0.5530 - val_loss: 0.7450 - val_accuracy: 0.3688\n",
      "Epoch 1361/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4123 - accuracy: 0.5564 - val_loss: 0.7491 - val_accuracy: 0.3757\n",
      "Epoch 1362/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.4092 - accuracy: 0.5566 - val_loss: 0.7571 - val_accuracy: 0.3736\n",
      "Epoch 1363/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.4212 - accuracy: 0.5526 - val_loss: 0.7452 - val_accuracy: 0.3721\n",
      "Epoch 1364/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.4129 - accuracy: 0.5561 - val_loss: 0.7554 - val_accuracy: 0.3720\n",
      "Epoch 1365/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4076 - accuracy: 0.5591 - val_loss: 0.7559 - val_accuracy: 0.3699\n",
      "Epoch 1366/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4088 - accuracy: 0.5571 - val_loss: 0.7629 - val_accuracy: 0.3790\n",
      "Epoch 1367/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4087 - accuracy: 0.5598 - val_loss: 0.7639 - val_accuracy: 0.3760\n",
      "Epoch 1368/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4041 - accuracy: 0.5618 - val_loss: 0.7611 - val_accuracy: 0.3723\n",
      "Epoch 1369/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4186 - accuracy: 0.5529 - val_loss: 0.7381 - val_accuracy: 0.3757\n",
      "Epoch 1370/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4197 - accuracy: 0.5500 - val_loss: 0.7555 - val_accuracy: 0.3700\n",
      "Epoch 1371/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4199 - accuracy: 0.5484 - val_loss: 0.7496 - val_accuracy: 0.3753\n",
      "Epoch 1372/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4109 - accuracy: 0.5574 - val_loss: 0.7768 - val_accuracy: 0.3739\n",
      "Epoch 1373/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4100 - accuracy: 0.5601 - val_loss: 0.7604 - val_accuracy: 0.3777\n",
      "Epoch 1374/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4086 - accuracy: 0.5587 - val_loss: 0.7614 - val_accuracy: 0.3738\n",
      "Epoch 1375/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4059 - accuracy: 0.5659 - val_loss: 0.7624 - val_accuracy: 0.3729\n",
      "Epoch 1376/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4138 - accuracy: 0.5534 - val_loss: 0.7612 - val_accuracy: 0.3729\n",
      "Epoch 1377/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4132 - accuracy: 0.5562 - val_loss: 0.7519 - val_accuracy: 0.3759\n",
      "Epoch 1378/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4159 - accuracy: 0.5513 - val_loss: 0.7401 - val_accuracy: 0.3723\n",
      "Epoch 1379/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4116 - accuracy: 0.5550 - val_loss: 0.7437 - val_accuracy: 0.3747\n",
      "Epoch 1380/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4167 - accuracy: 0.5547 - val_loss: 0.7439 - val_accuracy: 0.3708\n",
      "Epoch 1381/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4132 - accuracy: 0.5569 - val_loss: 0.7500 - val_accuracy: 0.3796\n",
      "Epoch 1382/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4085 - accuracy: 0.5603 - val_loss: 0.7672 - val_accuracy: 0.3753\n",
      "Epoch 1383/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4083 - accuracy: 0.5599 - val_loss: 0.7684 - val_accuracy: 0.3817\n",
      "Epoch 1384/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4144 - accuracy: 0.5570 - val_loss: 0.7464 - val_accuracy: 0.3739\n",
      "Epoch 1385/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.4072 - accuracy: 0.5614 - val_loss: 0.7533 - val_accuracy: 0.3739\n",
      "Epoch 1386/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4113 - accuracy: 0.5594 - val_loss: 0.7549 - val_accuracy: 0.3685\n",
      "Epoch 1387/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4085 - accuracy: 0.5628 - val_loss: 0.7533 - val_accuracy: 0.3757\n",
      "Epoch 1388/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4174 - accuracy: 0.5557 - val_loss: 0.7503 - val_accuracy: 0.3735\n",
      "Epoch 1389/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4136 - accuracy: 0.5534 - val_loss: 0.7492 - val_accuracy: 0.3690\n",
      "Epoch 1390/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4124 - accuracy: 0.5589 - val_loss: 0.7655 - val_accuracy: 0.3777\n",
      "Epoch 1391/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4136 - accuracy: 0.5550 - val_loss: 0.7557 - val_accuracy: 0.3727\n",
      "Epoch 1392/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4129 - accuracy: 0.5567 - val_loss: 0.7563 - val_accuracy: 0.3718\n",
      "Epoch 1393/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4176 - accuracy: 0.5559 - val_loss: 0.7539 - val_accuracy: 0.3684\n",
      "Epoch 1394/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4394 - accuracy: 0.5342 - val_loss: 0.7458 - val_accuracy: 0.3727\n",
      "Epoch 1395/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4213 - accuracy: 0.5493 - val_loss: 0.7458 - val_accuracy: 0.3739\n",
      "Epoch 1396/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4166 - accuracy: 0.5539 - val_loss: 0.7431 - val_accuracy: 0.3724\n",
      "Epoch 1397/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4164 - accuracy: 0.5505 - val_loss: 0.7605 - val_accuracy: 0.3799\n",
      "Epoch 1398/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4178 - accuracy: 0.5547 - val_loss: 0.7517 - val_accuracy: 0.3658\n",
      "Epoch 1399/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4254 - accuracy: 0.5516 - val_loss: 0.7422 - val_accuracy: 0.3696\n",
      "Epoch 1400/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4161 - accuracy: 0.5533 - val_loss: 0.7567 - val_accuracy: 0.3781\n",
      "Epoch 1401/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4048 - accuracy: 0.5640 - val_loss: 0.7573 - val_accuracy: 0.3654\n",
      "Epoch 1402/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4105 - accuracy: 0.5595 - val_loss: 0.7526 - val_accuracy: 0.3681\n",
      "Epoch 1403/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4094 - accuracy: 0.5578 - val_loss: 0.7549 - val_accuracy: 0.3726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1404/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4143 - accuracy: 0.5552 - val_loss: 0.7624 - val_accuracy: 0.3775\n",
      "Epoch 1405/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4146 - accuracy: 0.5536 - val_loss: 0.7440 - val_accuracy: 0.3723\n",
      "Epoch 1406/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4087 - accuracy: 0.5576 - val_loss: 0.7612 - val_accuracy: 0.3757\n",
      "Epoch 1407/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4189 - accuracy: 0.5523 - val_loss: 0.7421 - val_accuracy: 0.3781\n",
      "Epoch 1408/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4129 - accuracy: 0.5578 - val_loss: 0.7424 - val_accuracy: 0.3763\n",
      "Epoch 1409/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4133 - accuracy: 0.5573 - val_loss: 0.7485 - val_accuracy: 0.3777\n",
      "Epoch 1410/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4069 - accuracy: 0.5644 - val_loss: 0.7735 - val_accuracy: 0.3736\n",
      "Epoch 1411/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4139 - accuracy: 0.5577 - val_loss: 0.7487 - val_accuracy: 0.3729\n",
      "Epoch 1412/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4137 - accuracy: 0.5564 - val_loss: 0.7457 - val_accuracy: 0.3760\n",
      "Epoch 1413/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4133 - accuracy: 0.5548 - val_loss: 0.7588 - val_accuracy: 0.3769\n",
      "Epoch 1414/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4133 - accuracy: 0.5572 - val_loss: 0.7578 - val_accuracy: 0.3803\n",
      "Epoch 1415/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4083 - accuracy: 0.5616 - val_loss: 0.7586 - val_accuracy: 0.3736\n",
      "Epoch 1416/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4145 - accuracy: 0.5558 - val_loss: 0.7552 - val_accuracy: 0.3757\n",
      "Epoch 1417/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4097 - accuracy: 0.5593 - val_loss: 0.7512 - val_accuracy: 0.3783\n",
      "Epoch 1418/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4110 - accuracy: 0.5585 - val_loss: 0.7463 - val_accuracy: 0.3772\n",
      "Epoch 1419/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4166 - accuracy: 0.5542 - val_loss: 0.7605 - val_accuracy: 0.3786\n",
      "Epoch 1420/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.4166 - accuracy: 0.5543 - val_loss: 0.7404 - val_accuracy: 0.3711\n",
      "Epoch 1421/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.4097 - accuracy: 0.5594 - val_loss: 0.7628 - val_accuracy: 0.3793\n",
      "Epoch 1422/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4072 - accuracy: 0.5612 - val_loss: 0.7541 - val_accuracy: 0.3750\n",
      "Epoch 1423/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.4067 - accuracy: 0.5603 - val_loss: 0.7486 - val_accuracy: 0.3736\n",
      "Epoch 1424/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.4226 - accuracy: 0.5502 - val_loss: 0.7420 - val_accuracy: 0.3747\n",
      "Epoch 1425/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.4134 - accuracy: 0.5538 - val_loss: 0.7484 - val_accuracy: 0.3753\n",
      "Epoch 1426/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.4026 - accuracy: 0.5678 - val_loss: 0.7591 - val_accuracy: 0.3739\n",
      "Epoch 1427/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.4176 - accuracy: 0.5575 - val_loss: 0.7486 - val_accuracy: 0.3693\n",
      "Epoch 1428/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4124 - accuracy: 0.5585 - val_loss: 0.7326 - val_accuracy: 0.3754\n",
      "Epoch 1429/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4069 - accuracy: 0.5632 - val_loss: 0.7495 - val_accuracy: 0.3744\n",
      "Epoch 1430/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4140 - accuracy: 0.5565 - val_loss: 0.7554 - val_accuracy: 0.3708\n",
      "Epoch 1431/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4122 - accuracy: 0.5573 - val_loss: 0.7457 - val_accuracy: 0.3726\n",
      "Epoch 1432/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4153 - accuracy: 0.5552 - val_loss: 0.7681 - val_accuracy: 0.3703\n",
      "Epoch 1433/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4064 - accuracy: 0.5643 - val_loss: 0.7487 - val_accuracy: 0.3741\n",
      "Epoch 1434/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4103 - accuracy: 0.5636 - val_loss: 0.7419 - val_accuracy: 0.3750\n",
      "Epoch 1435/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4014 - accuracy: 0.5679 - val_loss: 0.7578 - val_accuracy: 0.3724\n",
      "Epoch 1436/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4034 - accuracy: 0.5646 - val_loss: 0.7575 - val_accuracy: 0.3763\n",
      "Epoch 1437/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4091 - accuracy: 0.5619 - val_loss: 0.7459 - val_accuracy: 0.3745\n",
      "Epoch 1438/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4071 - accuracy: 0.5646 - val_loss: 0.7731 - val_accuracy: 0.3721\n",
      "Epoch 1439/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4136 - accuracy: 0.5589 - val_loss: 0.7526 - val_accuracy: 0.3742\n",
      "Epoch 1440/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4119 - accuracy: 0.5615 - val_loss: 0.7474 - val_accuracy: 0.3760\n",
      "Epoch 1441/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4049 - accuracy: 0.5620 - val_loss: 0.7606 - val_accuracy: 0.3750\n",
      "Epoch 1442/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4051 - accuracy: 0.5651 - val_loss: 0.7650 - val_accuracy: 0.3781\n",
      "Epoch 1443/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4094 - accuracy: 0.5617 - val_loss: 0.7495 - val_accuracy: 0.3790\n",
      "Epoch 1444/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4062 - accuracy: 0.5644 - val_loss: 0.7648 - val_accuracy: 0.3756\n",
      "Epoch 1445/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4171 - accuracy: 0.5577 - val_loss: 0.7595 - val_accuracy: 0.3753\n",
      "Epoch 1446/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4123 - accuracy: 0.5552 - val_loss: 0.7637 - val_accuracy: 0.3760\n",
      "Epoch 1447/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4132 - accuracy: 0.5567 - val_loss: 0.7356 - val_accuracy: 0.3736\n",
      "Epoch 1448/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.4049 - accuracy: 0.5604 - val_loss: 0.7460 - val_accuracy: 0.3775\n",
      "Epoch 1449/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4164 - accuracy: 0.5555 - val_loss: 0.7344 - val_accuracy: 0.3648\n",
      "Epoch 1450/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4153 - accuracy: 0.5484 - val_loss: 0.7418 - val_accuracy: 0.3718\n",
      "Epoch 1451/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4112 - accuracy: 0.5558 - val_loss: 0.7353 - val_accuracy: 0.3742\n",
      "Epoch 1452/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4067 - accuracy: 0.5596 - val_loss: 0.7482 - val_accuracy: 0.3769\n",
      "Epoch 1453/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4146 - accuracy: 0.5574 - val_loss: 0.7381 - val_accuracy: 0.3760\n",
      "Epoch 1454/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4077 - accuracy: 0.5623 - val_loss: 0.7445 - val_accuracy: 0.3724\n",
      "Epoch 1455/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4115 - accuracy: 0.5573 - val_loss: 0.7383 - val_accuracy: 0.3768\n",
      "Epoch 1456/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4106 - accuracy: 0.5572 - val_loss: 0.7494 - val_accuracy: 0.3688\n",
      "Epoch 1457/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4122 - accuracy: 0.5543 - val_loss: 0.7618 - val_accuracy: 0.3765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1458/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4114 - accuracy: 0.5580 - val_loss: 0.7537 - val_accuracy: 0.3774\n",
      "Epoch 1459/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4084 - accuracy: 0.5595 - val_loss: 0.7515 - val_accuracy: 0.3729\n",
      "Epoch 1460/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4179 - accuracy: 0.5573 - val_loss: 0.7513 - val_accuracy: 0.3708\n",
      "Epoch 1461/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4229 - accuracy: 0.5510 - val_loss: 0.7480 - val_accuracy: 0.3744\n",
      "Epoch 1462/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4091 - accuracy: 0.5604 - val_loss: 0.7547 - val_accuracy: 0.3735\n",
      "Epoch 1463/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4087 - accuracy: 0.5619 - val_loss: 0.7557 - val_accuracy: 0.3765\n",
      "Epoch 1464/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4500 - accuracy: 0.5284 - val_loss: 0.7073 - val_accuracy: 0.3765\n",
      "Epoch 1465/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4758 - accuracy: 0.5011 - val_loss: 0.7249 - val_accuracy: 0.3744\n",
      "Epoch 1466/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4392 - accuracy: 0.5282 - val_loss: 0.7543 - val_accuracy: 0.3682\n",
      "Epoch 1467/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4313 - accuracy: 0.5424 - val_loss: 0.7473 - val_accuracy: 0.3676\n",
      "Epoch 1468/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4162 - accuracy: 0.5502 - val_loss: 0.7631 - val_accuracy: 0.3793\n",
      "Epoch 1469/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4111 - accuracy: 0.5589 - val_loss: 0.7691 - val_accuracy: 0.3765\n",
      "Epoch 1470/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4102 - accuracy: 0.5598 - val_loss: 0.7568 - val_accuracy: 0.3792\n",
      "Epoch 1471/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4161 - accuracy: 0.5548 - val_loss: 0.7482 - val_accuracy: 0.3750\n",
      "Epoch 1472/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4103 - accuracy: 0.5562 - val_loss: 0.7383 - val_accuracy: 0.3753\n",
      "Epoch 1473/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4102 - accuracy: 0.5581 - val_loss: 0.7539 - val_accuracy: 0.3735\n",
      "Epoch 1474/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4156 - accuracy: 0.5550 - val_loss: 0.7600 - val_accuracy: 0.3803\n",
      "Epoch 1475/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4166 - accuracy: 0.5548 - val_loss: 0.7462 - val_accuracy: 0.3649\n",
      "Epoch 1476/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4098 - accuracy: 0.5583 - val_loss: 0.7570 - val_accuracy: 0.3711\n",
      "Epoch 1477/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4116 - accuracy: 0.5626 - val_loss: 0.7523 - val_accuracy: 0.3697\n",
      "Epoch 1478/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4395 - accuracy: 0.5355 - val_loss: 0.7383 - val_accuracy: 0.3697\n",
      "Epoch 1479/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4172 - accuracy: 0.5518 - val_loss: 0.7547 - val_accuracy: 0.3748\n",
      "Epoch 1480/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4184 - accuracy: 0.5509 - val_loss: 0.7718 - val_accuracy: 0.3736\n",
      "Epoch 1481/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4240 - accuracy: 0.5460 - val_loss: 0.7447 - val_accuracy: 0.3702\n",
      "Epoch 1482/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4034 - accuracy: 0.5622 - val_loss: 0.7777 - val_accuracy: 0.3753\n",
      "Epoch 1483/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4077 - accuracy: 0.5600 - val_loss: 0.7536 - val_accuracy: 0.3691\n",
      "Epoch 1484/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4072 - accuracy: 0.5641 - val_loss: 0.7691 - val_accuracy: 0.3756\n",
      "Epoch 1485/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4004 - accuracy: 0.5676 - val_loss: 0.7687 - val_accuracy: 0.3759\n",
      "Epoch 1486/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4115 - accuracy: 0.5569 - val_loss: 0.7672 - val_accuracy: 0.3730\n",
      "Epoch 1487/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4230 - accuracy: 0.5525 - val_loss: 0.7286 - val_accuracy: 0.3682\n",
      "Epoch 1488/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4350 - accuracy: 0.5369 - val_loss: 0.7368 - val_accuracy: 0.3720\n",
      "Epoch 1489/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4152 - accuracy: 0.5493 - val_loss: 0.7645 - val_accuracy: 0.3733\n",
      "Epoch 1490/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4093 - accuracy: 0.5591 - val_loss: 0.7582 - val_accuracy: 0.3744\n",
      "Epoch 1491/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4046 - accuracy: 0.5645 - val_loss: 0.7603 - val_accuracy: 0.3724\n",
      "Epoch 1492/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4097 - accuracy: 0.5610 - val_loss: 0.7524 - val_accuracy: 0.3762\n",
      "Epoch 1493/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4129 - accuracy: 0.5556 - val_loss: 0.7552 - val_accuracy: 0.3763\n",
      "Epoch 1494/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4045 - accuracy: 0.5659 - val_loss: 0.7627 - val_accuracy: 0.3747\n",
      "Epoch 1495/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4156 - accuracy: 0.5567 - val_loss: 0.7621 - val_accuracy: 0.3696\n",
      "Epoch 1496/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4097 - accuracy: 0.5590 - val_loss: 0.7628 - val_accuracy: 0.3744\n",
      "Epoch 1497/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4076 - accuracy: 0.5601 - val_loss: 0.7654 - val_accuracy: 0.3678\n",
      "Epoch 1498/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4055 - accuracy: 0.5642 - val_loss: 0.7775 - val_accuracy: 0.3729\n",
      "Epoch 1499/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4134 - accuracy: 0.5583 - val_loss: 0.7547 - val_accuracy: 0.3703\n",
      "Epoch 1500/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4323 - accuracy: 0.5426 - val_loss: 0.7361 - val_accuracy: 0.3718\n",
      "Epoch 1501/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4302 - accuracy: 0.5449 - val_loss: 0.7567 - val_accuracy: 0.3721\n",
      "Epoch 1502/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4243 - accuracy: 0.5473 - val_loss: 0.7607 - val_accuracy: 0.3757\n",
      "Epoch 1503/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4054 - accuracy: 0.5661 - val_loss: 0.7512 - val_accuracy: 0.3700\n",
      "Epoch 1504/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4146 - accuracy: 0.5556 - val_loss: 0.7475 - val_accuracy: 0.3751\n",
      "Epoch 1505/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4471 - accuracy: 0.5310 - val_loss: 0.7391 - val_accuracy: 0.3786\n",
      "Epoch 1506/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4240 - accuracy: 0.5449 - val_loss: 0.7583 - val_accuracy: 0.3748\n",
      "Epoch 1507/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4308 - accuracy: 0.5405 - val_loss: 0.7623 - val_accuracy: 0.3730\n",
      "Epoch 1508/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4242 - accuracy: 0.5460 - val_loss: 0.7621 - val_accuracy: 0.3768\n",
      "Epoch 1509/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4176 - accuracy: 0.5528 - val_loss: 0.7575 - val_accuracy: 0.3724\n",
      "Epoch 1510/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4132 - accuracy: 0.5565 - val_loss: 0.7685 - val_accuracy: 0.3772\n",
      "Epoch 1511/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4130 - accuracy: 0.5589 - val_loss: 0.7482 - val_accuracy: 0.3711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1512/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4058 - accuracy: 0.5604 - val_loss: 0.7615 - val_accuracy: 0.3759\n",
      "Epoch 1513/2500\n",
      "20056/20056 [==============================] - 1s 51us/step - loss: 0.4043 - accuracy: 0.5614 - val_loss: 0.7563 - val_accuracy: 0.3763\n",
      "Epoch 1514/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4089 - accuracy: 0.5593 - val_loss: 0.7592 - val_accuracy: 0.3754\n",
      "Epoch 1515/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4173 - accuracy: 0.5582 - val_loss: 0.7481 - val_accuracy: 0.3750\n",
      "Epoch 1516/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4146 - accuracy: 0.5564 - val_loss: 0.7354 - val_accuracy: 0.3741\n",
      "Epoch 1517/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4205 - accuracy: 0.5501 - val_loss: 0.7500 - val_accuracy: 0.3778\n",
      "Epoch 1518/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4208 - accuracy: 0.5476 - val_loss: 0.7574 - val_accuracy: 0.3757\n",
      "Epoch 1519/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4151 - accuracy: 0.5593 - val_loss: 0.7460 - val_accuracy: 0.3736\n",
      "Epoch 1520/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4172 - accuracy: 0.5515 - val_loss: 0.7531 - val_accuracy: 0.3802\n",
      "Epoch 1521/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4069 - accuracy: 0.5597 - val_loss: 0.7490 - val_accuracy: 0.3760\n",
      "Epoch 1522/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4098 - accuracy: 0.5582 - val_loss: 0.7682 - val_accuracy: 0.3769\n",
      "Epoch 1523/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.3990 - accuracy: 0.5682 - val_loss: 0.7508 - val_accuracy: 0.3720\n",
      "Epoch 1524/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4087 - accuracy: 0.5582 - val_loss: 0.7534 - val_accuracy: 0.3766\n",
      "Epoch 1525/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4181 - accuracy: 0.5538 - val_loss: 0.7627 - val_accuracy: 0.3817\n",
      "Epoch 1526/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4160 - accuracy: 0.5537 - val_loss: 0.7577 - val_accuracy: 0.3727\n",
      "Epoch 1527/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4315 - accuracy: 0.5418 - val_loss: 0.7522 - val_accuracy: 0.3741\n",
      "Epoch 1528/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4069 - accuracy: 0.5601 - val_loss: 0.7531 - val_accuracy: 0.3751\n",
      "Epoch 1529/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4084 - accuracy: 0.5604 - val_loss: 0.7576 - val_accuracy: 0.3742\n",
      "Epoch 1530/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4066 - accuracy: 0.5609 - val_loss: 0.7531 - val_accuracy: 0.3745\n",
      "Epoch 1531/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4201 - accuracy: 0.5537 - val_loss: 0.7636 - val_accuracy: 0.3754\n",
      "Epoch 1532/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4163 - accuracy: 0.5540 - val_loss: 0.7636 - val_accuracy: 0.3790\n",
      "Epoch 1533/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4125 - accuracy: 0.5557 - val_loss: 0.7610 - val_accuracy: 0.3772\n",
      "Epoch 1534/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4475 - accuracy: 0.5281 - val_loss: 0.7312 - val_accuracy: 0.3669\n",
      "Epoch 1535/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4445 - accuracy: 0.5288 - val_loss: 0.7165 - val_accuracy: 0.3664\n",
      "Epoch 1536/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4380 - accuracy: 0.5318 - val_loss: 0.7465 - val_accuracy: 0.3717\n",
      "Epoch 1537/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4291 - accuracy: 0.5428 - val_loss: 0.7452 - val_accuracy: 0.3760\n",
      "Epoch 1538/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4280 - accuracy: 0.5433 - val_loss: 0.7362 - val_accuracy: 0.3738\n",
      "Epoch 1539/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4197 - accuracy: 0.5492 - val_loss: 0.7336 - val_accuracy: 0.3727\n",
      "Epoch 1540/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4279 - accuracy: 0.5392 - val_loss: 0.7349 - val_accuracy: 0.3720\n",
      "Epoch 1541/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4323 - accuracy: 0.5369 - val_loss: 0.7156 - val_accuracy: 0.3652\n",
      "Epoch 1542/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4623 - accuracy: 0.5136 - val_loss: 0.7302 - val_accuracy: 0.3763\n",
      "Epoch 1543/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4330 - accuracy: 0.5379 - val_loss: 0.7330 - val_accuracy: 0.3711\n",
      "Epoch 1544/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4297 - accuracy: 0.5390 - val_loss: 0.7615 - val_accuracy: 0.3778\n",
      "Epoch 1545/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4231 - accuracy: 0.5479 - val_loss: 0.7234 - val_accuracy: 0.3711\n",
      "Epoch 1546/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4386 - accuracy: 0.5337 - val_loss: 0.7304 - val_accuracy: 0.3742\n",
      "Epoch 1547/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4183 - accuracy: 0.5485 - val_loss: 0.7508 - val_accuracy: 0.3769\n",
      "Epoch 1548/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4205 - accuracy: 0.5560 - val_loss: 0.7348 - val_accuracy: 0.3814\n",
      "Epoch 1549/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4399 - accuracy: 0.5375 - val_loss: 0.7072 - val_accuracy: 0.3747\n",
      "Epoch 1550/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4474 - accuracy: 0.5258 - val_loss: 0.7496 - val_accuracy: 0.3748\n",
      "Epoch 1551/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4315 - accuracy: 0.5436 - val_loss: 0.7286 - val_accuracy: 0.3687\n",
      "Epoch 1552/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4164 - accuracy: 0.5529 - val_loss: 0.7493 - val_accuracy: 0.3696\n",
      "Epoch 1553/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4110 - accuracy: 0.5581 - val_loss: 0.7489 - val_accuracy: 0.3748\n",
      "Epoch 1554/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4057 - accuracy: 0.5603 - val_loss: 0.7581 - val_accuracy: 0.3800\n",
      "Epoch 1555/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4089 - accuracy: 0.5577 - val_loss: 0.7571 - val_accuracy: 0.3745\n",
      "Epoch 1556/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4140 - accuracy: 0.5546 - val_loss: 0.7360 - val_accuracy: 0.3820\n",
      "Epoch 1557/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4164 - accuracy: 0.5530 - val_loss: 0.7570 - val_accuracy: 0.3745\n",
      "Epoch 1558/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4172 - accuracy: 0.5568 - val_loss: 0.7408 - val_accuracy: 0.3729\n",
      "Epoch 1559/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4268 - accuracy: 0.5460 - val_loss: 0.7363 - val_accuracy: 0.3660\n",
      "Epoch 1560/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4158 - accuracy: 0.5502 - val_loss: 0.7292 - val_accuracy: 0.3724\n",
      "Epoch 1561/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4130 - accuracy: 0.5537 - val_loss: 0.7623 - val_accuracy: 0.3771\n",
      "Epoch 1562/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4382 - accuracy: 0.5352 - val_loss: 0.7475 - val_accuracy: 0.3678\n",
      "Epoch 1563/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4374 - accuracy: 0.5349 - val_loss: 0.7380 - val_accuracy: 0.3760\n",
      "Epoch 1564/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4126 - accuracy: 0.5557 - val_loss: 0.7580 - val_accuracy: 0.3754\n",
      "Epoch 1565/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4022 - accuracy: 0.5675 - val_loss: 0.7447 - val_accuracy: 0.3691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1566/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4163 - accuracy: 0.5546 - val_loss: 0.7583 - val_accuracy: 0.3738\n",
      "Epoch 1567/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4023 - accuracy: 0.5675 - val_loss: 0.7477 - val_accuracy: 0.3745\n",
      "Epoch 1568/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4045 - accuracy: 0.5658 - val_loss: 0.7614 - val_accuracy: 0.3723\n",
      "Epoch 1569/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.3994 - accuracy: 0.5681 - val_loss: 0.7631 - val_accuracy: 0.3783\n",
      "Epoch 1570/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4066 - accuracy: 0.5623 - val_loss: 0.7629 - val_accuracy: 0.3768\n",
      "Epoch 1571/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4076 - accuracy: 0.5620 - val_loss: 0.7464 - val_accuracy: 0.3694\n",
      "Epoch 1572/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4067 - accuracy: 0.5639 - val_loss: 0.7454 - val_accuracy: 0.3747\n",
      "Epoch 1573/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4060 - accuracy: 0.5663 - val_loss: 0.7558 - val_accuracy: 0.3744\n",
      "Epoch 1574/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4036 - accuracy: 0.5653 - val_loss: 0.7555 - val_accuracy: 0.3753\n",
      "Epoch 1575/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3988 - accuracy: 0.5686 - val_loss: 0.7617 - val_accuracy: 0.3739\n",
      "Epoch 1576/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4051 - accuracy: 0.5646 - val_loss: 0.7601 - val_accuracy: 0.3715\n",
      "Epoch 1577/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4048 - accuracy: 0.5658 - val_loss: 0.7540 - val_accuracy: 0.3748\n",
      "Epoch 1578/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4160 - accuracy: 0.5571 - val_loss: 0.7519 - val_accuracy: 0.3732\n",
      "Epoch 1579/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4090 - accuracy: 0.5600 - val_loss: 0.7605 - val_accuracy: 0.3714\n",
      "Epoch 1580/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4087 - accuracy: 0.5636 - val_loss: 0.7632 - val_accuracy: 0.3745\n",
      "Epoch 1581/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4085 - accuracy: 0.5634 - val_loss: 0.7437 - val_accuracy: 0.3667\n",
      "Epoch 1582/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4089 - accuracy: 0.5621 - val_loss: 0.7527 - val_accuracy: 0.3757\n",
      "Epoch 1583/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4129 - accuracy: 0.5589 - val_loss: 0.7377 - val_accuracy: 0.3678\n",
      "Epoch 1584/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4148 - accuracy: 0.5571 - val_loss: 0.7583 - val_accuracy: 0.3714\n",
      "Epoch 1585/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4067 - accuracy: 0.5656 - val_loss: 0.7612 - val_accuracy: 0.3774\n",
      "Epoch 1586/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4054 - accuracy: 0.5630 - val_loss: 0.7596 - val_accuracy: 0.3744\n",
      "Epoch 1587/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4110 - accuracy: 0.5628 - val_loss: 0.7517 - val_accuracy: 0.3702\n",
      "Epoch 1588/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4360 - accuracy: 0.5380 - val_loss: 0.7193 - val_accuracy: 0.3676\n",
      "Epoch 1589/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4489 - accuracy: 0.5262 - val_loss: 0.7177 - val_accuracy: 0.3709\n",
      "Epoch 1590/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4258 - accuracy: 0.5405 - val_loss: 0.7381 - val_accuracy: 0.3812\n",
      "Epoch 1591/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4244 - accuracy: 0.5435 - val_loss: 0.7586 - val_accuracy: 0.3792\n",
      "Epoch 1592/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4268 - accuracy: 0.5467 - val_loss: 0.7291 - val_accuracy: 0.3763\n",
      "Epoch 1593/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4116 - accuracy: 0.5606 - val_loss: 0.7656 - val_accuracy: 0.3800\n",
      "Epoch 1594/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4263 - accuracy: 0.5489 - val_loss: 0.7373 - val_accuracy: 0.3735\n",
      "Epoch 1595/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4694 - accuracy: 0.5128 - val_loss: 0.7273 - val_accuracy: 0.3730\n",
      "Epoch 1596/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4481 - accuracy: 0.5249 - val_loss: 0.7562 - val_accuracy: 0.3802\n",
      "Epoch 1597/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4203 - accuracy: 0.5518 - val_loss: 0.7285 - val_accuracy: 0.3745\n",
      "Epoch 1598/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4342 - accuracy: 0.5360 - val_loss: 0.7410 - val_accuracy: 0.3751\n",
      "Epoch 1599/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4118 - accuracy: 0.5611 - val_loss: 0.7648 - val_accuracy: 0.3712\n",
      "Epoch 1600/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4182 - accuracy: 0.5595 - val_loss: 0.7341 - val_accuracy: 0.3691\n",
      "Epoch 1601/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4170 - accuracy: 0.5511 - val_loss: 0.7486 - val_accuracy: 0.3771\n",
      "Epoch 1602/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4013 - accuracy: 0.5666 - val_loss: 0.7628 - val_accuracy: 0.3762\n",
      "Epoch 1603/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4056 - accuracy: 0.5652 - val_loss: 0.7738 - val_accuracy: 0.3747\n",
      "Epoch 1604/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.4089 - accuracy: 0.5595 - val_loss: 0.7443 - val_accuracy: 0.3741\n",
      "Epoch 1605/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4134 - accuracy: 0.5559 - val_loss: 0.7588 - val_accuracy: 0.3717\n",
      "Epoch 1606/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4134 - accuracy: 0.5565 - val_loss: 0.7491 - val_accuracy: 0.3723\n",
      "Epoch 1607/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4127 - accuracy: 0.5557 - val_loss: 0.7465 - val_accuracy: 0.3711\n",
      "Epoch 1608/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4194 - accuracy: 0.5522 - val_loss: 0.7551 - val_accuracy: 0.3757\n",
      "Epoch 1609/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4025 - accuracy: 0.5631 - val_loss: 0.7565 - val_accuracy: 0.3790\n",
      "Epoch 1610/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3978 - accuracy: 0.5679 - val_loss: 0.7654 - val_accuracy: 0.3756\n",
      "Epoch 1611/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4031 - accuracy: 0.5633 - val_loss: 0.7700 - val_accuracy: 0.3780\n",
      "Epoch 1612/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3948 - accuracy: 0.5727 - val_loss: 0.7795 - val_accuracy: 0.3784\n",
      "Epoch 1613/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4029 - accuracy: 0.5657 - val_loss: 0.7497 - val_accuracy: 0.3724\n",
      "Epoch 1614/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4151 - accuracy: 0.5561 - val_loss: 0.7455 - val_accuracy: 0.3741\n",
      "Epoch 1615/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4092 - accuracy: 0.5578 - val_loss: 0.7516 - val_accuracy: 0.3753\n",
      "Epoch 1616/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3975 - accuracy: 0.5701 - val_loss: 0.7668 - val_accuracy: 0.3712\n",
      "Epoch 1617/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4027 - accuracy: 0.5654 - val_loss: 0.7459 - val_accuracy: 0.3724\n",
      "Epoch 1618/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4179 - accuracy: 0.5548 - val_loss: 0.7368 - val_accuracy: 0.3797\n",
      "Epoch 1619/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4168 - accuracy: 0.5546 - val_loss: 0.7444 - val_accuracy: 0.3732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1620/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4168 - accuracy: 0.5547 - val_loss: 0.7615 - val_accuracy: 0.3751\n",
      "Epoch 1621/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4036 - accuracy: 0.5653 - val_loss: 0.7528 - val_accuracy: 0.3750\n",
      "Epoch 1622/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4003 - accuracy: 0.5658 - val_loss: 0.7658 - val_accuracy: 0.3724\n",
      "Epoch 1623/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4120 - accuracy: 0.5562 - val_loss: 0.7603 - val_accuracy: 0.3756\n",
      "Epoch 1624/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4027 - accuracy: 0.5649 - val_loss: 0.7447 - val_accuracy: 0.3735\n",
      "Epoch 1625/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4003 - accuracy: 0.5667 - val_loss: 0.7586 - val_accuracy: 0.3708\n",
      "Epoch 1626/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4107 - accuracy: 0.5566 - val_loss: 0.7484 - val_accuracy: 0.3682\n",
      "Epoch 1627/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4228 - accuracy: 0.5483 - val_loss: 0.7448 - val_accuracy: 0.3715\n",
      "Epoch 1628/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4159 - accuracy: 0.5536 - val_loss: 0.7392 - val_accuracy: 0.3670\n",
      "Epoch 1629/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4084 - accuracy: 0.5590 - val_loss: 0.7659 - val_accuracy: 0.3732\n",
      "Epoch 1630/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4058 - accuracy: 0.5625 - val_loss: 0.7409 - val_accuracy: 0.3681\n",
      "Epoch 1631/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4214 - accuracy: 0.5490 - val_loss: 0.7421 - val_accuracy: 0.3702\n",
      "Epoch 1632/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4103 - accuracy: 0.5575 - val_loss: 0.7497 - val_accuracy: 0.3714\n",
      "Epoch 1633/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4022 - accuracy: 0.5651 - val_loss: 0.7509 - val_accuracy: 0.3750\n",
      "Epoch 1634/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4022 - accuracy: 0.5635 - val_loss: 0.7540 - val_accuracy: 0.3754\n",
      "Epoch 1635/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4160 - accuracy: 0.5530 - val_loss: 0.7585 - val_accuracy: 0.3720\n",
      "Epoch 1636/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4039 - accuracy: 0.5651 - val_loss: 0.7675 - val_accuracy: 0.3733\n",
      "Epoch 1637/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4081 - accuracy: 0.5644 - val_loss: 0.7607 - val_accuracy: 0.3771\n",
      "Epoch 1638/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4080 - accuracy: 0.5598 - val_loss: 0.7536 - val_accuracy: 0.3759\n",
      "Epoch 1639/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4074 - accuracy: 0.5616 - val_loss: 0.7661 - val_accuracy: 0.3765\n",
      "Epoch 1640/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4046 - accuracy: 0.5677 - val_loss: 0.7502 - val_accuracy: 0.3747\n",
      "Epoch 1641/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4158 - accuracy: 0.5559 - val_loss: 0.7512 - val_accuracy: 0.3679\n",
      "Epoch 1642/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4217 - accuracy: 0.5478 - val_loss: 0.7643 - val_accuracy: 0.3781\n",
      "Epoch 1643/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4327 - accuracy: 0.5427 - val_loss: 0.7387 - val_accuracy: 0.3690\n",
      "Epoch 1644/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4091 - accuracy: 0.5593 - val_loss: 0.7726 - val_accuracy: 0.3780\n",
      "Epoch 1645/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4232 - accuracy: 0.5530 - val_loss: 0.7428 - val_accuracy: 0.3732\n",
      "Epoch 1646/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4119 - accuracy: 0.5604 - val_loss: 0.7372 - val_accuracy: 0.3784\n",
      "Epoch 1647/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4181 - accuracy: 0.5498 - val_loss: 0.7525 - val_accuracy: 0.3781\n",
      "Epoch 1648/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4128 - accuracy: 0.5550 - val_loss: 0.7499 - val_accuracy: 0.3732\n",
      "Epoch 1649/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4091 - accuracy: 0.5579 - val_loss: 0.7476 - val_accuracy: 0.3684\n",
      "Epoch 1650/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4052 - accuracy: 0.5628 - val_loss: 0.7636 - val_accuracy: 0.3751\n",
      "Epoch 1651/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.3978 - accuracy: 0.5691 - val_loss: 0.7607 - val_accuracy: 0.3744\n",
      "Epoch 1652/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3941 - accuracy: 0.5707 - val_loss: 0.7649 - val_accuracy: 0.3748\n",
      "Epoch 1653/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4100 - accuracy: 0.5649 - val_loss: 0.7425 - val_accuracy: 0.3723\n",
      "Epoch 1654/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4150 - accuracy: 0.5569 - val_loss: 0.7589 - val_accuracy: 0.3744\n",
      "Epoch 1655/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4043 - accuracy: 0.5670 - val_loss: 0.7644 - val_accuracy: 0.3766\n",
      "Epoch 1656/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4001 - accuracy: 0.5699 - val_loss: 0.7411 - val_accuracy: 0.3681\n",
      "Epoch 1657/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4054 - accuracy: 0.5603 - val_loss: 0.7502 - val_accuracy: 0.3783\n",
      "Epoch 1658/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4070 - accuracy: 0.5630 - val_loss: 0.7475 - val_accuracy: 0.3700\n",
      "Epoch 1659/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4097 - accuracy: 0.5595 - val_loss: 0.7463 - val_accuracy: 0.3675\n",
      "Epoch 1660/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4029 - accuracy: 0.5639 - val_loss: 0.7602 - val_accuracy: 0.3684\n",
      "Epoch 1661/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4139 - accuracy: 0.5572 - val_loss: 0.7595 - val_accuracy: 0.3703\n",
      "Epoch 1662/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4041 - accuracy: 0.5640 - val_loss: 0.7479 - val_accuracy: 0.3733\n",
      "Epoch 1663/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.4115 - accuracy: 0.5579 - val_loss: 0.7500 - val_accuracy: 0.3727\n",
      "Epoch 1664/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4052 - accuracy: 0.5654 - val_loss: 0.7620 - val_accuracy: 0.3717\n",
      "Epoch 1665/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4125 - accuracy: 0.5590 - val_loss: 0.7598 - val_accuracy: 0.3771\n",
      "Epoch 1666/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4075 - accuracy: 0.5599 - val_loss: 0.7561 - val_accuracy: 0.3757\n",
      "Epoch 1667/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3989 - accuracy: 0.5705 - val_loss: 0.7711 - val_accuracy: 0.3774\n",
      "Epoch 1668/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4026 - accuracy: 0.5663 - val_loss: 0.7792 - val_accuracy: 0.3757\n",
      "Epoch 1669/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4064 - accuracy: 0.5635 - val_loss: 0.7558 - val_accuracy: 0.3720\n",
      "Epoch 1670/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3995 - accuracy: 0.5664 - val_loss: 0.7547 - val_accuracy: 0.3751\n",
      "Epoch 1671/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4013 - accuracy: 0.5671 - val_loss: 0.7578 - val_accuracy: 0.3757\n",
      "Epoch 1672/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3967 - accuracy: 0.5744 - val_loss: 0.7574 - val_accuracy: 0.3774\n",
      "Epoch 1673/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4003 - accuracy: 0.5717 - val_loss: 0.7625 - val_accuracy: 0.3753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1674/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.3991 - accuracy: 0.5702 - val_loss: 0.7650 - val_accuracy: 0.3812\n",
      "Epoch 1675/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4090 - accuracy: 0.5591 - val_loss: 0.7506 - val_accuracy: 0.3706\n",
      "Epoch 1676/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4019 - accuracy: 0.5688 - val_loss: 0.7686 - val_accuracy: 0.3751\n",
      "Epoch 1677/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4033 - accuracy: 0.5638 - val_loss: 0.7593 - val_accuracy: 0.3711\n",
      "Epoch 1678/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4042 - accuracy: 0.5643 - val_loss: 0.7551 - val_accuracy: 0.3744\n",
      "Epoch 1679/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4049 - accuracy: 0.5628 - val_loss: 0.7546 - val_accuracy: 0.3694\n",
      "Epoch 1680/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4041 - accuracy: 0.5647 - val_loss: 0.7530 - val_accuracy: 0.3739\n",
      "Epoch 1681/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4107 - accuracy: 0.5614 - val_loss: 0.7588 - val_accuracy: 0.3718\n",
      "Epoch 1682/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4074 - accuracy: 0.5618 - val_loss: 0.7571 - val_accuracy: 0.3738\n",
      "Epoch 1683/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4118 - accuracy: 0.5597 - val_loss: 0.7387 - val_accuracy: 0.3705\n",
      "Epoch 1684/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4126 - accuracy: 0.5605 - val_loss: 0.7533 - val_accuracy: 0.3772\n",
      "Epoch 1685/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4122 - accuracy: 0.5626 - val_loss: 0.7522 - val_accuracy: 0.3729\n",
      "Epoch 1686/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4144 - accuracy: 0.5593 - val_loss: 0.7320 - val_accuracy: 0.3702\n",
      "Epoch 1687/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4088 - accuracy: 0.5586 - val_loss: 0.7433 - val_accuracy: 0.3756\n",
      "Epoch 1688/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4073 - accuracy: 0.5619 - val_loss: 0.7411 - val_accuracy: 0.3727\n",
      "Epoch 1689/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4075 - accuracy: 0.5596 - val_loss: 0.7593 - val_accuracy: 0.3766\n",
      "Epoch 1690/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4037 - accuracy: 0.5704 - val_loss: 0.7544 - val_accuracy: 0.3739\n",
      "Epoch 1691/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3972 - accuracy: 0.5686 - val_loss: 0.7561 - val_accuracy: 0.3727\n",
      "Epoch 1692/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4028 - accuracy: 0.5653 - val_loss: 0.7690 - val_accuracy: 0.3738\n",
      "Epoch 1693/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4173 - accuracy: 0.5584 - val_loss: 0.7483 - val_accuracy: 0.3736\n",
      "Epoch 1694/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4097 - accuracy: 0.5599 - val_loss: 0.7598 - val_accuracy: 0.3693\n",
      "Epoch 1695/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4061 - accuracy: 0.5626 - val_loss: 0.7390 - val_accuracy: 0.3727\n",
      "Epoch 1696/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4124 - accuracy: 0.5583 - val_loss: 0.7553 - val_accuracy: 0.3775\n",
      "Epoch 1697/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4026 - accuracy: 0.5671 - val_loss: 0.7693 - val_accuracy: 0.3696\n",
      "Epoch 1698/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4062 - accuracy: 0.5668 - val_loss: 0.7505 - val_accuracy: 0.3762\n",
      "Epoch 1699/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4265 - accuracy: 0.5485 - val_loss: 0.7606 - val_accuracy: 0.3762\n",
      "Epoch 1700/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4102 - accuracy: 0.5592 - val_loss: 0.7578 - val_accuracy: 0.3777\n",
      "Epoch 1701/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4069 - accuracy: 0.5629 - val_loss: 0.7545 - val_accuracy: 0.3742\n",
      "Epoch 1702/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4017 - accuracy: 0.5695 - val_loss: 0.7489 - val_accuracy: 0.3745\n",
      "Epoch 1703/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4238 - accuracy: 0.5469 - val_loss: 0.7525 - val_accuracy: 0.3700\n",
      "Epoch 1704/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4151 - accuracy: 0.5517 - val_loss: 0.7551 - val_accuracy: 0.3739\n",
      "Epoch 1705/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4060 - accuracy: 0.5618 - val_loss: 0.7658 - val_accuracy: 0.3702\n",
      "Epoch 1706/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4060 - accuracy: 0.5622 - val_loss: 0.7571 - val_accuracy: 0.3717\n",
      "Epoch 1707/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4168 - accuracy: 0.5514 - val_loss: 0.7482 - val_accuracy: 0.3729\n",
      "Epoch 1708/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4017 - accuracy: 0.5661 - val_loss: 0.7461 - val_accuracy: 0.3733\n",
      "Epoch 1709/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4106 - accuracy: 0.5575 - val_loss: 0.7570 - val_accuracy: 0.3748\n",
      "Epoch 1710/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3981 - accuracy: 0.5704 - val_loss: 0.7724 - val_accuracy: 0.3753\n",
      "Epoch 1711/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4230 - accuracy: 0.5506 - val_loss: 0.7089 - val_accuracy: 0.3697\n",
      "Epoch 1712/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4524 - accuracy: 0.5223 - val_loss: 0.7356 - val_accuracy: 0.3690\n",
      "Epoch 1713/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.4216 - accuracy: 0.5452 - val_loss: 0.7434 - val_accuracy: 0.3745\n",
      "Epoch 1714/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4109 - accuracy: 0.5559 - val_loss: 0.7546 - val_accuracy: 0.3763\n",
      "Epoch 1715/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3960 - accuracy: 0.5670 - val_loss: 0.7576 - val_accuracy: 0.3747\n",
      "Epoch 1716/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3992 - accuracy: 0.5667 - val_loss: 0.7668 - val_accuracy: 0.3751\n",
      "Epoch 1717/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4068 - accuracy: 0.5620 - val_loss: 0.7619 - val_accuracy: 0.3759\n",
      "Epoch 1718/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4097 - accuracy: 0.5624 - val_loss: 0.7618 - val_accuracy: 0.3732\n",
      "Epoch 1719/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4053 - accuracy: 0.5635 - val_loss: 0.7566 - val_accuracy: 0.3754\n",
      "Epoch 1720/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.4084 - accuracy: 0.5632 - val_loss: 0.7220 - val_accuracy: 0.3667\n",
      "Epoch 1721/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4385 - accuracy: 0.5311 - val_loss: 0.7508 - val_accuracy: 0.3708\n",
      "Epoch 1722/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4035 - accuracy: 0.5617 - val_loss: 0.7585 - val_accuracy: 0.3753\n",
      "Epoch 1723/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.4198 - accuracy: 0.5545 - val_loss: 0.7631 - val_accuracy: 0.3742\n",
      "Epoch 1724/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.4037 - accuracy: 0.5657 - val_loss: 0.7713 - val_accuracy: 0.3744\n",
      "Epoch 1725/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4006 - accuracy: 0.5682 - val_loss: 0.7788 - val_accuracy: 0.3721\n",
      "Epoch 1726/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3960 - accuracy: 0.5715 - val_loss: 0.7641 - val_accuracy: 0.3751\n",
      "Epoch 1727/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3965 - accuracy: 0.5705 - val_loss: 0.7707 - val_accuracy: 0.3754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1728/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4030 - accuracy: 0.5647 - val_loss: 0.7595 - val_accuracy: 0.3727\n",
      "Epoch 1729/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3962 - accuracy: 0.5734 - val_loss: 0.7761 - val_accuracy: 0.3771\n",
      "Epoch 1730/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.3914 - accuracy: 0.5773 - val_loss: 0.7636 - val_accuracy: 0.3745\n",
      "Epoch 1731/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4085 - accuracy: 0.5643 - val_loss: 0.7563 - val_accuracy: 0.3714\n",
      "Epoch 1732/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3930 - accuracy: 0.5714 - val_loss: 0.7563 - val_accuracy: 0.3790\n",
      "Epoch 1733/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3991 - accuracy: 0.5684 - val_loss: 0.7674 - val_accuracy: 0.3733\n",
      "Epoch 1734/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4008 - accuracy: 0.5673 - val_loss: 0.7744 - val_accuracy: 0.3762\n",
      "Epoch 1735/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3968 - accuracy: 0.5731 - val_loss: 0.7688 - val_accuracy: 0.3797\n",
      "Epoch 1736/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4006 - accuracy: 0.5693 - val_loss: 0.7518 - val_accuracy: 0.3658\n",
      "Epoch 1737/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4223 - accuracy: 0.5482 - val_loss: 0.7490 - val_accuracy: 0.3751\n",
      "Epoch 1738/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4155 - accuracy: 0.5525 - val_loss: 0.7360 - val_accuracy: 0.3700\n",
      "Epoch 1739/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4058 - accuracy: 0.5642 - val_loss: 0.7516 - val_accuracy: 0.3718\n",
      "Epoch 1740/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3983 - accuracy: 0.5706 - val_loss: 0.7501 - val_accuracy: 0.3753\n",
      "Epoch 1741/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4003 - accuracy: 0.5690 - val_loss: 0.7627 - val_accuracy: 0.3726\n",
      "Epoch 1742/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4024 - accuracy: 0.5679 - val_loss: 0.7588 - val_accuracy: 0.3697\n",
      "Epoch 1743/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3990 - accuracy: 0.5694 - val_loss: 0.7597 - val_accuracy: 0.3741\n",
      "Epoch 1744/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3983 - accuracy: 0.5694 - val_loss: 0.7533 - val_accuracy: 0.3792\n",
      "Epoch 1745/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4013 - accuracy: 0.5673 - val_loss: 0.7512 - val_accuracy: 0.3718\n",
      "Epoch 1746/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4000 - accuracy: 0.5681 - val_loss: 0.7563 - val_accuracy: 0.3691\n",
      "Epoch 1747/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4030 - accuracy: 0.5679 - val_loss: 0.7478 - val_accuracy: 0.3759\n",
      "Epoch 1748/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4035 - accuracy: 0.5620 - val_loss: 0.7717 - val_accuracy: 0.3748\n",
      "Epoch 1749/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3969 - accuracy: 0.5716 - val_loss: 0.7598 - val_accuracy: 0.3751\n",
      "Epoch 1750/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3979 - accuracy: 0.5741 - val_loss: 0.7487 - val_accuracy: 0.3730\n",
      "Epoch 1751/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3991 - accuracy: 0.5681 - val_loss: 0.7486 - val_accuracy: 0.3763\n",
      "Epoch 1752/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4028 - accuracy: 0.5639 - val_loss: 0.7246 - val_accuracy: 0.3741\n",
      "Epoch 1753/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4122 - accuracy: 0.5601 - val_loss: 0.7303 - val_accuracy: 0.3667\n",
      "Epoch 1754/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4184 - accuracy: 0.5547 - val_loss: 0.7456 - val_accuracy: 0.3651\n",
      "Epoch 1755/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4252 - accuracy: 0.5497 - val_loss: 0.7369 - val_accuracy: 0.3751\n",
      "Epoch 1756/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4149 - accuracy: 0.5514 - val_loss: 0.7428 - val_accuracy: 0.3768\n",
      "Epoch 1757/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4213 - accuracy: 0.5475 - val_loss: 0.7234 - val_accuracy: 0.3729\n",
      "Epoch 1758/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4477 - accuracy: 0.5253 - val_loss: 0.7285 - val_accuracy: 0.3684\n",
      "Epoch 1759/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4305 - accuracy: 0.5383 - val_loss: 0.7334 - val_accuracy: 0.3784\n",
      "Epoch 1760/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.4408 - accuracy: 0.5310 - val_loss: 0.7197 - val_accuracy: 0.3648\n",
      "Epoch 1761/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4506 - accuracy: 0.5254 - val_loss: 0.7245 - val_accuracy: 0.3736\n",
      "Epoch 1762/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.4280 - accuracy: 0.5420 - val_loss: 0.7454 - val_accuracy: 0.3730\n",
      "Epoch 1763/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4175 - accuracy: 0.5503 - val_loss: 0.7448 - val_accuracy: 0.3685\n",
      "Epoch 1764/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4138 - accuracy: 0.5550 - val_loss: 0.7393 - val_accuracy: 0.3714\n",
      "Epoch 1765/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4191 - accuracy: 0.5469 - val_loss: 0.7547 - val_accuracy: 0.3772\n",
      "Epoch 1766/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4119 - accuracy: 0.5567 - val_loss: 0.7619 - val_accuracy: 0.3711\n",
      "Epoch 1767/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4029 - accuracy: 0.5653 - val_loss: 0.7576 - val_accuracy: 0.3745\n",
      "Epoch 1768/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4156 - accuracy: 0.5551 - val_loss: 0.7449 - val_accuracy: 0.3696\n",
      "Epoch 1769/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4128 - accuracy: 0.5591 - val_loss: 0.7486 - val_accuracy: 0.3706\n",
      "Epoch 1770/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4118 - accuracy: 0.5592 - val_loss: 0.7363 - val_accuracy: 0.3658\n",
      "Epoch 1771/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4319 - accuracy: 0.5398 - val_loss: 0.7522 - val_accuracy: 0.3666\n",
      "Epoch 1772/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4065 - accuracy: 0.5602 - val_loss: 0.7543 - val_accuracy: 0.3759\n",
      "Epoch 1773/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4046 - accuracy: 0.5651 - val_loss: 0.7583 - val_accuracy: 0.3727\n",
      "Epoch 1774/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4059 - accuracy: 0.5624 - val_loss: 0.7462 - val_accuracy: 0.3715\n",
      "Epoch 1775/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4149 - accuracy: 0.5549 - val_loss: 0.7384 - val_accuracy: 0.3705\n",
      "Epoch 1776/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4048 - accuracy: 0.5692 - val_loss: 0.7590 - val_accuracy: 0.3754\n",
      "Epoch 1777/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4030 - accuracy: 0.5682 - val_loss: 0.7512 - val_accuracy: 0.3763\n",
      "Epoch 1778/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3977 - accuracy: 0.5714 - val_loss: 0.7661 - val_accuracy: 0.3675\n",
      "Epoch 1779/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3980 - accuracy: 0.5734 - val_loss: 0.7582 - val_accuracy: 0.3720\n",
      "Epoch 1780/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3955 - accuracy: 0.5749 - val_loss: 0.7725 - val_accuracy: 0.3729\n",
      "Epoch 1781/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4023 - accuracy: 0.5672 - val_loss: 0.7618 - val_accuracy: 0.3718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1782/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4003 - accuracy: 0.5695 - val_loss: 0.7470 - val_accuracy: 0.3718\n",
      "Epoch 1783/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4269 - accuracy: 0.5432 - val_loss: 0.7532 - val_accuracy: 0.3717\n",
      "Epoch 1784/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4208 - accuracy: 0.5508 - val_loss: 0.7398 - val_accuracy: 0.3759\n",
      "Epoch 1785/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4521 - accuracy: 0.5258 - val_loss: 0.7416 - val_accuracy: 0.3759\n",
      "Epoch 1786/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4262 - accuracy: 0.5417 - val_loss: 0.7482 - val_accuracy: 0.3750\n",
      "Epoch 1787/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4084 - accuracy: 0.5598 - val_loss: 0.7553 - val_accuracy: 0.3706\n",
      "Epoch 1788/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4107 - accuracy: 0.5582 - val_loss: 0.7649 - val_accuracy: 0.3672\n",
      "Epoch 1789/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4225 - accuracy: 0.5505 - val_loss: 0.7345 - val_accuracy: 0.3679\n",
      "Epoch 1790/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4031 - accuracy: 0.5651 - val_loss: 0.7591 - val_accuracy: 0.3727\n",
      "Epoch 1791/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4104 - accuracy: 0.5606 - val_loss: 0.7578 - val_accuracy: 0.3700\n",
      "Epoch 1792/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4101 - accuracy: 0.5618 - val_loss: 0.7238 - val_accuracy: 0.3715\n",
      "Epoch 1793/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4272 - accuracy: 0.5429 - val_loss: 0.7451 - val_accuracy: 0.3700\n",
      "Epoch 1794/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4102 - accuracy: 0.5589 - val_loss: 0.7555 - val_accuracy: 0.3769\n",
      "Epoch 1795/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4026 - accuracy: 0.5647 - val_loss: 0.7574 - val_accuracy: 0.3732\n",
      "Epoch 1796/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4010 - accuracy: 0.5666 - val_loss: 0.7679 - val_accuracy: 0.3783\n",
      "Epoch 1797/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4021 - accuracy: 0.5679 - val_loss: 0.7703 - val_accuracy: 0.3781\n",
      "Epoch 1798/2500\n",
      "20056/20056 [==============================] - 1s 52us/step - loss: 0.3969 - accuracy: 0.5698 - val_loss: 0.7677 - val_accuracy: 0.3815\n",
      "Epoch 1799/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4067 - accuracy: 0.5614 - val_loss: 0.7708 - val_accuracy: 0.3763\n",
      "Epoch 1800/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.3945 - accuracy: 0.5737 - val_loss: 0.7714 - val_accuracy: 0.3762\n",
      "Epoch 1801/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3884 - accuracy: 0.5794 - val_loss: 0.7795 - val_accuracy: 0.3790\n",
      "Epoch 1802/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3946 - accuracy: 0.5752 - val_loss: 0.7649 - val_accuracy: 0.3750\n",
      "Epoch 1803/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4001 - accuracy: 0.5704 - val_loss: 0.7707 - val_accuracy: 0.3786\n",
      "Epoch 1804/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3940 - accuracy: 0.5751 - val_loss: 0.7735 - val_accuracy: 0.3823\n",
      "Epoch 1805/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3922 - accuracy: 0.5763 - val_loss: 0.7785 - val_accuracy: 0.3781\n",
      "Epoch 1806/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4022 - accuracy: 0.5707 - val_loss: 0.7713 - val_accuracy: 0.3736\n",
      "Epoch 1807/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3917 - accuracy: 0.5763 - val_loss: 0.7702 - val_accuracy: 0.3772\n",
      "Epoch 1808/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3993 - accuracy: 0.5757 - val_loss: 0.8033 - val_accuracy: 0.3768\n",
      "Epoch 1809/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4001 - accuracy: 0.5701 - val_loss: 0.7776 - val_accuracy: 0.3765\n",
      "Epoch 1810/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.3966 - accuracy: 0.5701 - val_loss: 0.7585 - val_accuracy: 0.3694\n",
      "Epoch 1811/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3963 - accuracy: 0.5716 - val_loss: 0.7767 - val_accuracy: 0.3783\n",
      "Epoch 1812/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3967 - accuracy: 0.5739 - val_loss: 0.7639 - val_accuracy: 0.3744\n",
      "Epoch 1813/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3978 - accuracy: 0.5716 - val_loss: 0.7809 - val_accuracy: 0.3721\n",
      "Epoch 1814/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3975 - accuracy: 0.5722 - val_loss: 0.7660 - val_accuracy: 0.3780\n",
      "Epoch 1815/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4075 - accuracy: 0.5632 - val_loss: 0.7715 - val_accuracy: 0.3803\n",
      "Epoch 1816/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4043 - accuracy: 0.5629 - val_loss: 0.7613 - val_accuracy: 0.3729\n",
      "Epoch 1817/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3956 - accuracy: 0.5706 - val_loss: 0.7657 - val_accuracy: 0.3771\n",
      "Epoch 1818/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.4098 - accuracy: 0.5649 - val_loss: 0.7660 - val_accuracy: 0.3735\n",
      "Epoch 1819/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4059 - accuracy: 0.5662 - val_loss: 0.7716 - val_accuracy: 0.3729\n",
      "Epoch 1820/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3979 - accuracy: 0.5700 - val_loss: 0.7722 - val_accuracy: 0.3727\n",
      "Epoch 1821/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3942 - accuracy: 0.5746 - val_loss: 0.7566 - val_accuracy: 0.3700\n",
      "Epoch 1822/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4228 - accuracy: 0.5507 - val_loss: 0.7375 - val_accuracy: 0.3669\n",
      "Epoch 1823/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4245 - accuracy: 0.5454 - val_loss: 0.7619 - val_accuracy: 0.3708\n",
      "Epoch 1824/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4333 - accuracy: 0.5387 - val_loss: 0.7602 - val_accuracy: 0.3753\n",
      "Epoch 1825/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4141 - accuracy: 0.5569 - val_loss: 0.7484 - val_accuracy: 0.3715\n",
      "Epoch 1826/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4039 - accuracy: 0.5586 - val_loss: 0.7619 - val_accuracy: 0.3714\n",
      "Epoch 1827/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.3940 - accuracy: 0.5766 - val_loss: 0.7719 - val_accuracy: 0.3754\n",
      "Epoch 1828/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.3956 - accuracy: 0.5708 - val_loss: 0.7779 - val_accuracy: 0.3729\n",
      "Epoch 1829/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3957 - accuracy: 0.5731 - val_loss: 0.7644 - val_accuracy: 0.3766\n",
      "Epoch 1830/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4038 - accuracy: 0.5631 - val_loss: 0.7559 - val_accuracy: 0.3793\n",
      "Epoch 1831/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3998 - accuracy: 0.5696 - val_loss: 0.7697 - val_accuracy: 0.3736\n",
      "Epoch 1832/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3986 - accuracy: 0.5677 - val_loss: 0.7459 - val_accuracy: 0.3700\n",
      "Epoch 1833/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3946 - accuracy: 0.5729 - val_loss: 0.7634 - val_accuracy: 0.3720\n",
      "Epoch 1834/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4018 - accuracy: 0.5679 - val_loss: 0.7358 - val_accuracy: 0.3702\n",
      "Epoch 1835/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4190 - accuracy: 0.5526 - val_loss: 0.7679 - val_accuracy: 0.3809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1836/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3971 - accuracy: 0.5717 - val_loss: 0.7695 - val_accuracy: 0.3808\n",
      "Epoch 1837/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3929 - accuracy: 0.5748 - val_loss: 0.7664 - val_accuracy: 0.3812\n",
      "Epoch 1838/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4000 - accuracy: 0.5703 - val_loss: 0.7694 - val_accuracy: 0.3780\n",
      "Epoch 1839/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4137 - accuracy: 0.5548 - val_loss: 0.7434 - val_accuracy: 0.3754\n",
      "Epoch 1840/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.3985 - accuracy: 0.5706 - val_loss: 0.7728 - val_accuracy: 0.3736\n",
      "Epoch 1841/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4119 - accuracy: 0.5588 - val_loss: 0.7447 - val_accuracy: 0.3823\n",
      "Epoch 1842/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4178 - accuracy: 0.5521 - val_loss: 0.7565 - val_accuracy: 0.3778\n",
      "Epoch 1843/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3997 - accuracy: 0.5668 - val_loss: 0.7577 - val_accuracy: 0.3771\n",
      "Epoch 1844/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3983 - accuracy: 0.5692 - val_loss: 0.7735 - val_accuracy: 0.3766\n",
      "Epoch 1845/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3950 - accuracy: 0.5746 - val_loss: 0.7675 - val_accuracy: 0.3772\n",
      "Epoch 1846/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3896 - accuracy: 0.5759 - val_loss: 0.7617 - val_accuracy: 0.3768\n",
      "Epoch 1847/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3995 - accuracy: 0.5707 - val_loss: 0.7585 - val_accuracy: 0.3750\n",
      "Epoch 1848/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4006 - accuracy: 0.5700 - val_loss: 0.7681 - val_accuracy: 0.3774\n",
      "Epoch 1849/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4009 - accuracy: 0.5698 - val_loss: 0.7678 - val_accuracy: 0.3709\n",
      "Epoch 1850/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3939 - accuracy: 0.5741 - val_loss: 0.7710 - val_accuracy: 0.3709\n",
      "Epoch 1851/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4074 - accuracy: 0.5641 - val_loss: 0.7460 - val_accuracy: 0.3757\n",
      "Epoch 1852/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3972 - accuracy: 0.5700 - val_loss: 0.7692 - val_accuracy: 0.3772\n",
      "Epoch 1853/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.4010 - accuracy: 0.5676 - val_loss: 0.7628 - val_accuracy: 0.3765\n",
      "Epoch 1854/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4084 - accuracy: 0.5621 - val_loss: 0.7468 - val_accuracy: 0.3714\n",
      "Epoch 1855/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4040 - accuracy: 0.5663 - val_loss: 0.7736 - val_accuracy: 0.3763\n",
      "Epoch 1856/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4026 - accuracy: 0.5656 - val_loss: 0.7609 - val_accuracy: 0.3754\n",
      "Epoch 1857/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3946 - accuracy: 0.5730 - val_loss: 0.7673 - val_accuracy: 0.3733\n",
      "Epoch 1858/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.3937 - accuracy: 0.5760 - val_loss: 0.7835 - val_accuracy: 0.3765\n",
      "Epoch 1859/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3930 - accuracy: 0.5752 - val_loss: 0.7856 - val_accuracy: 0.3771\n",
      "Epoch 1860/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3938 - accuracy: 0.5775 - val_loss: 0.7673 - val_accuracy: 0.3771\n",
      "Epoch 1861/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3879 - accuracy: 0.5840 - val_loss: 0.7585 - val_accuracy: 0.3765\n",
      "Epoch 1862/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4060 - accuracy: 0.5663 - val_loss: 0.7782 - val_accuracy: 0.3756\n",
      "Epoch 1863/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3902 - accuracy: 0.5788 - val_loss: 0.7658 - val_accuracy: 0.3763\n",
      "Epoch 1864/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4056 - accuracy: 0.5649 - val_loss: 0.7711 - val_accuracy: 0.3783\n",
      "Epoch 1865/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4059 - accuracy: 0.5642 - val_loss: 0.7595 - val_accuracy: 0.3766\n",
      "Epoch 1866/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4084 - accuracy: 0.5640 - val_loss: 0.7542 - val_accuracy: 0.3793\n",
      "Epoch 1867/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4121 - accuracy: 0.5592 - val_loss: 0.7395 - val_accuracy: 0.3658\n",
      "Epoch 1868/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4039 - accuracy: 0.5658 - val_loss: 0.7503 - val_accuracy: 0.3775\n",
      "Epoch 1869/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4045 - accuracy: 0.5656 - val_loss: 0.7607 - val_accuracy: 0.3762\n",
      "Epoch 1870/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3944 - accuracy: 0.5716 - val_loss: 0.7709 - val_accuracy: 0.3778\n",
      "Epoch 1871/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3981 - accuracy: 0.5734 - val_loss: 0.7496 - val_accuracy: 0.3694\n",
      "Epoch 1872/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3942 - accuracy: 0.5774 - val_loss: 0.7564 - val_accuracy: 0.3769\n",
      "Epoch 1873/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3952 - accuracy: 0.5753 - val_loss: 0.7880 - val_accuracy: 0.3793\n",
      "Epoch 1874/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3977 - accuracy: 0.5717 - val_loss: 0.7493 - val_accuracy: 0.3696\n",
      "Epoch 1875/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4003 - accuracy: 0.5700 - val_loss: 0.7728 - val_accuracy: 0.3772\n",
      "Epoch 1876/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4098 - accuracy: 0.5638 - val_loss: 0.7647 - val_accuracy: 0.3792\n",
      "Epoch 1877/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3960 - accuracy: 0.5710 - val_loss: 0.7774 - val_accuracy: 0.3783\n",
      "Epoch 1878/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4002 - accuracy: 0.5688 - val_loss: 0.7477 - val_accuracy: 0.3703\n",
      "Epoch 1879/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4157 - accuracy: 0.5561 - val_loss: 0.7548 - val_accuracy: 0.3757\n",
      "Epoch 1880/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4123 - accuracy: 0.5596 - val_loss: 0.7660 - val_accuracy: 0.3787\n",
      "Epoch 1881/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3957 - accuracy: 0.5719 - val_loss: 0.7626 - val_accuracy: 0.3790\n",
      "Epoch 1882/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.4046 - accuracy: 0.5719 - val_loss: 0.7582 - val_accuracy: 0.3793\n",
      "Epoch 1883/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4084 - accuracy: 0.5628 - val_loss: 0.7659 - val_accuracy: 0.3711\n",
      "Epoch 1884/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.3951 - accuracy: 0.5753 - val_loss: 0.7773 - val_accuracy: 0.3715\n",
      "Epoch 1885/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.4122 - accuracy: 0.5638 - val_loss: 0.7351 - val_accuracy: 0.3712\n",
      "Epoch 1886/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4105 - accuracy: 0.5596 - val_loss: 0.7478 - val_accuracy: 0.3726\n",
      "Epoch 1887/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4127 - accuracy: 0.5573 - val_loss: 0.7605 - val_accuracy: 0.3711\n",
      "Epoch 1888/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4061 - accuracy: 0.5645 - val_loss: 0.7470 - val_accuracy: 0.3757\n",
      "Epoch 1889/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4419 - accuracy: 0.5368 - val_loss: 0.7454 - val_accuracy: 0.3799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1890/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4051 - accuracy: 0.5641 - val_loss: 0.7701 - val_accuracy: 0.3766\n",
      "Epoch 1891/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.3913 - accuracy: 0.5795 - val_loss: 0.7601 - val_accuracy: 0.3730\n",
      "Epoch 1892/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3899 - accuracy: 0.5789 - val_loss: 0.7586 - val_accuracy: 0.3786\n",
      "Epoch 1893/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4004 - accuracy: 0.5718 - val_loss: 0.7519 - val_accuracy: 0.3736\n",
      "Epoch 1894/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3940 - accuracy: 0.5768 - val_loss: 0.7759 - val_accuracy: 0.3760\n",
      "Epoch 1895/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3970 - accuracy: 0.5720 - val_loss: 0.7793 - val_accuracy: 0.3792\n",
      "Epoch 1896/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3868 - accuracy: 0.5798 - val_loss: 0.7712 - val_accuracy: 0.3774\n",
      "Epoch 1897/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.4023 - accuracy: 0.5728 - val_loss: 0.7568 - val_accuracy: 0.3684\n",
      "Epoch 1898/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4022 - accuracy: 0.5687 - val_loss: 0.7564 - val_accuracy: 0.3702\n",
      "Epoch 1899/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4018 - accuracy: 0.5691 - val_loss: 0.7664 - val_accuracy: 0.3778\n",
      "Epoch 1900/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4012 - accuracy: 0.5697 - val_loss: 0.7567 - val_accuracy: 0.3759\n",
      "Epoch 1901/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3980 - accuracy: 0.5709 - val_loss: 0.7646 - val_accuracy: 0.3783\n",
      "Epoch 1902/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3984 - accuracy: 0.5700 - val_loss: 0.7617 - val_accuracy: 0.3750\n",
      "Epoch 1903/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3935 - accuracy: 0.5752 - val_loss: 0.7772 - val_accuracy: 0.3771\n",
      "Epoch 1904/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3995 - accuracy: 0.5724 - val_loss: 0.7682 - val_accuracy: 0.3780\n",
      "Epoch 1905/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3940 - accuracy: 0.5741 - val_loss: 0.7669 - val_accuracy: 0.3789\n",
      "Epoch 1906/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4038 - accuracy: 0.5694 - val_loss: 0.7568 - val_accuracy: 0.3759\n",
      "Epoch 1907/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4006 - accuracy: 0.5664 - val_loss: 0.7714 - val_accuracy: 0.3723\n",
      "Epoch 1908/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3954 - accuracy: 0.5699 - val_loss: 0.7679 - val_accuracy: 0.3751\n",
      "Epoch 1909/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3856 - accuracy: 0.5800 - val_loss: 0.7808 - val_accuracy: 0.3765\n",
      "Epoch 1910/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3866 - accuracy: 0.5810 - val_loss: 0.7589 - val_accuracy: 0.3777\n",
      "Epoch 1911/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3886 - accuracy: 0.5778 - val_loss: 0.7530 - val_accuracy: 0.3763\n",
      "Epoch 1912/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3936 - accuracy: 0.5719 - val_loss: 0.7577 - val_accuracy: 0.3769\n",
      "Epoch 1913/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3971 - accuracy: 0.5708 - val_loss: 0.7625 - val_accuracy: 0.3748\n",
      "Epoch 1914/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3946 - accuracy: 0.5730 - val_loss: 0.7759 - val_accuracy: 0.3768\n",
      "Epoch 1915/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.3850 - accuracy: 0.5842 - val_loss: 0.7658 - val_accuracy: 0.3735\n",
      "Epoch 1916/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.4005 - accuracy: 0.5708 - val_loss: 0.7584 - val_accuracy: 0.3730\n",
      "Epoch 1917/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.4027 - accuracy: 0.5700 - val_loss: 0.7639 - val_accuracy: 0.3687\n",
      "Epoch 1918/2500\n",
      "20056/20056 [==============================] - 2s 78us/step - loss: 0.3886 - accuracy: 0.5769 - val_loss: 0.7645 - val_accuracy: 0.3732\n",
      "Epoch 1919/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3851 - accuracy: 0.5805 - val_loss: 0.7612 - val_accuracy: 0.3735\n",
      "Epoch 1920/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.3896 - accuracy: 0.5795 - val_loss: 0.7650 - val_accuracy: 0.3793\n",
      "Epoch 1921/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.3910 - accuracy: 0.5791 - val_loss: 0.7714 - val_accuracy: 0.3745\n",
      "Epoch 1922/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.3992 - accuracy: 0.5709 - val_loss: 0.7445 - val_accuracy: 0.3768\n",
      "Epoch 1923/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3991 - accuracy: 0.5681 - val_loss: 0.7619 - val_accuracy: 0.3750\n",
      "Epoch 1924/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.4081 - accuracy: 0.5634 - val_loss: 0.7342 - val_accuracy: 0.3694\n",
      "Epoch 1925/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.4052 - accuracy: 0.5665 - val_loss: 0.7562 - val_accuracy: 0.3759\n",
      "Epoch 1926/2500\n",
      "20056/20056 [==============================] - 2s 77us/step - loss: 0.4034 - accuracy: 0.5682 - val_loss: 0.7479 - val_accuracy: 0.3703\n",
      "Epoch 1927/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4099 - accuracy: 0.5636 - val_loss: 0.7764 - val_accuracy: 0.3777\n",
      "Epoch 1928/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4202 - accuracy: 0.5540 - val_loss: 0.7593 - val_accuracy: 0.3774\n",
      "Epoch 1929/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.3990 - accuracy: 0.5745 - val_loss: 0.7682 - val_accuracy: 0.3771\n",
      "Epoch 1930/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.3963 - accuracy: 0.5768 - val_loss: 0.7536 - val_accuracy: 0.3753\n",
      "Epoch 1931/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4000 - accuracy: 0.5754 - val_loss: 0.7454 - val_accuracy: 0.3760\n",
      "Epoch 1932/2500\n",
      "20056/20056 [==============================] - 2s 79us/step - loss: 0.4081 - accuracy: 0.5643 - val_loss: 0.7532 - val_accuracy: 0.3693\n",
      "Epoch 1933/2500\n",
      "20056/20056 [==============================] - 2s 81us/step - loss: 0.4029 - accuracy: 0.5693 - val_loss: 0.7679 - val_accuracy: 0.3715\n",
      "Epoch 1934/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.3908 - accuracy: 0.5812 - val_loss: 0.7625 - val_accuracy: 0.3715\n",
      "Epoch 1935/2500\n",
      "20056/20056 [==============================] - 2s 77us/step - loss: 0.4081 - accuracy: 0.5593 - val_loss: 0.7598 - val_accuracy: 0.3763\n",
      "Epoch 1936/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3912 - accuracy: 0.5752 - val_loss: 0.7669 - val_accuracy: 0.3792\n",
      "Epoch 1937/2500\n",
      "20056/20056 [==============================] - 2s 81us/step - loss: 0.3836 - accuracy: 0.5872 - val_loss: 0.7810 - val_accuracy: 0.3735\n",
      "Epoch 1938/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.3811 - accuracy: 0.5880 - val_loss: 0.7853 - val_accuracy: 0.3741\n",
      "Epoch 1939/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3872 - accuracy: 0.5845 - val_loss: 0.7681 - val_accuracy: 0.3747\n",
      "Epoch 1940/2500\n",
      "20056/20056 [==============================] - 2s 81us/step - loss: 0.3864 - accuracy: 0.5856 - val_loss: 0.7847 - val_accuracy: 0.3790\n",
      "Epoch 1941/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.3860 - accuracy: 0.5861 - val_loss: 0.7798 - val_accuracy: 0.3729\n",
      "Epoch 1942/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.3918 - accuracy: 0.5786 - val_loss: 0.7656 - val_accuracy: 0.3705\n",
      "Epoch 1943/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3870 - accuracy: 0.5824 - val_loss: 0.7587 - val_accuracy: 0.3765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1944/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3891 - accuracy: 0.5796 - val_loss: 0.7800 - val_accuracy: 0.3754\n",
      "Epoch 1945/2500\n",
      "20056/20056 [==============================] - 2s 75us/step - loss: 0.3909 - accuracy: 0.5807 - val_loss: 0.7953 - val_accuracy: 0.3781\n",
      "Epoch 1946/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3908 - accuracy: 0.5789 - val_loss: 0.7769 - val_accuracy: 0.3759\n",
      "Epoch 1947/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3956 - accuracy: 0.5732 - val_loss: 0.7690 - val_accuracy: 0.3736\n",
      "Epoch 1948/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3915 - accuracy: 0.5772 - val_loss: 0.7740 - val_accuracy: 0.3748\n",
      "Epoch 1949/2500\n",
      "20056/20056 [==============================] - 2s 83us/step - loss: 0.3931 - accuracy: 0.5761 - val_loss: 0.7783 - val_accuracy: 0.3797\n",
      "Epoch 1950/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3887 - accuracy: 0.5837 - val_loss: 0.7791 - val_accuracy: 0.3711\n",
      "Epoch 1951/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.3854 - accuracy: 0.5856 - val_loss: 0.7852 - val_accuracy: 0.3803\n",
      "Epoch 1952/2500\n",
      "20056/20056 [==============================] - 2s 77us/step - loss: 0.3935 - accuracy: 0.5777 - val_loss: 0.7514 - val_accuracy: 0.3723\n",
      "Epoch 1953/2500\n",
      "20056/20056 [==============================] - 2s 84us/step - loss: 0.4008 - accuracy: 0.5739 - val_loss: 0.7640 - val_accuracy: 0.3724\n",
      "Epoch 1954/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3952 - accuracy: 0.5770 - val_loss: 0.7597 - val_accuracy: 0.3729\n",
      "Epoch 1955/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3957 - accuracy: 0.5738 - val_loss: 0.7611 - val_accuracy: 0.3789\n",
      "Epoch 1956/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3984 - accuracy: 0.5721 - val_loss: 0.7441 - val_accuracy: 0.3709\n",
      "Epoch 1957/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.3994 - accuracy: 0.5710 - val_loss: 0.7521 - val_accuracy: 0.3797\n",
      "Epoch 1958/2500\n",
      "20056/20056 [==============================] - 2s 79us/step - loss: 0.3891 - accuracy: 0.5764 - val_loss: 0.7788 - val_accuracy: 0.3765\n",
      "Epoch 1959/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3911 - accuracy: 0.5792 - val_loss: 0.7628 - val_accuracy: 0.3747\n",
      "Epoch 1960/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3875 - accuracy: 0.5819 - val_loss: 0.7831 - val_accuracy: 0.3766\n",
      "Epoch 1961/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.4036 - accuracy: 0.5691 - val_loss: 0.7639 - val_accuracy: 0.3730\n",
      "Epoch 1962/2500\n",
      "20056/20056 [==============================] - 2s 78us/step - loss: 0.3886 - accuracy: 0.5796 - val_loss: 0.7666 - val_accuracy: 0.3777\n",
      "Epoch 1963/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.3877 - accuracy: 0.5822 - val_loss: 0.7778 - val_accuracy: 0.3748\n",
      "Epoch 1964/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.3959 - accuracy: 0.5765 - val_loss: 0.7572 - val_accuracy: 0.3735\n",
      "Epoch 1965/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.3926 - accuracy: 0.5802 - val_loss: 0.7788 - val_accuracy: 0.3778\n",
      "Epoch 1966/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3894 - accuracy: 0.5775 - val_loss: 0.7657 - val_accuracy: 0.3715\n",
      "Epoch 1967/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3925 - accuracy: 0.5787 - val_loss: 0.7865 - val_accuracy: 0.3784\n",
      "Epoch 1968/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.3975 - accuracy: 0.5759 - val_loss: 0.7798 - val_accuracy: 0.3711\n",
      "Epoch 1969/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3953 - accuracy: 0.5767 - val_loss: 0.7782 - val_accuracy: 0.3800\n",
      "Epoch 1970/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3970 - accuracy: 0.5751 - val_loss: 0.7756 - val_accuracy: 0.3730\n",
      "Epoch 1971/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.3979 - accuracy: 0.5726 - val_loss: 0.7849 - val_accuracy: 0.3705\n",
      "Epoch 1972/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4013 - accuracy: 0.5667 - val_loss: 0.7850 - val_accuracy: 0.3787\n",
      "Epoch 1973/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3924 - accuracy: 0.5768 - val_loss: 0.7641 - val_accuracy: 0.3771\n",
      "Epoch 1974/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4050 - accuracy: 0.5700 - val_loss: 0.7798 - val_accuracy: 0.3794\n",
      "Epoch 1975/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4076 - accuracy: 0.5633 - val_loss: 0.7638 - val_accuracy: 0.3797\n",
      "Epoch 1976/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3855 - accuracy: 0.5805 - val_loss: 0.7678 - val_accuracy: 0.3738\n",
      "Epoch 1977/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3813 - accuracy: 0.5863 - val_loss: 0.7939 - val_accuracy: 0.3742\n",
      "Epoch 1978/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3897 - accuracy: 0.5817 - val_loss: 0.7813 - val_accuracy: 0.3760\n",
      "Epoch 1979/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4070 - accuracy: 0.5691 - val_loss: 0.7570 - val_accuracy: 0.3768\n",
      "Epoch 1980/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4030 - accuracy: 0.5669 - val_loss: 0.7487 - val_accuracy: 0.3786\n",
      "Epoch 1981/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3960 - accuracy: 0.5758 - val_loss: 0.7654 - val_accuracy: 0.3774\n",
      "Epoch 1982/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3905 - accuracy: 0.5778 - val_loss: 0.7629 - val_accuracy: 0.3777\n",
      "Epoch 1983/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3872 - accuracy: 0.5830 - val_loss: 0.7810 - val_accuracy: 0.3751\n",
      "Epoch 1984/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3886 - accuracy: 0.5805 - val_loss: 0.7794 - val_accuracy: 0.3772\n",
      "Epoch 1985/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3842 - accuracy: 0.5856 - val_loss: 0.7574 - val_accuracy: 0.3812\n",
      "Epoch 1986/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3852 - accuracy: 0.5828 - val_loss: 0.7665 - val_accuracy: 0.3757\n",
      "Epoch 1987/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3931 - accuracy: 0.5754 - val_loss: 0.7858 - val_accuracy: 0.3805\n",
      "Epoch 1988/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3982 - accuracy: 0.5730 - val_loss: 0.7645 - val_accuracy: 0.3793\n",
      "Epoch 1989/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3922 - accuracy: 0.5801 - val_loss: 0.7864 - val_accuracy: 0.3784\n",
      "Epoch 1990/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3898 - accuracy: 0.5787 - val_loss: 0.7615 - val_accuracy: 0.3759\n",
      "Epoch 1991/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3980 - accuracy: 0.5724 - val_loss: 0.7656 - val_accuracy: 0.3706\n",
      "Epoch 1992/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.4173 - accuracy: 0.5544 - val_loss: 0.7653 - val_accuracy: 0.3727\n",
      "Epoch 1993/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3975 - accuracy: 0.5725 - val_loss: 0.7606 - val_accuracy: 0.3729\n",
      "Epoch 1994/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4039 - accuracy: 0.5690 - val_loss: 0.7608 - val_accuracy: 0.3777\n",
      "Epoch 1995/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.3988 - accuracy: 0.5725 - val_loss: 0.7759 - val_accuracy: 0.3794\n",
      "Epoch 1996/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.3873 - accuracy: 0.5801 - val_loss: 0.7740 - val_accuracy: 0.3717\n",
      "Epoch 1997/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3912 - accuracy: 0.5795 - val_loss: 0.7838 - val_accuracy: 0.3771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1998/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.3924 - accuracy: 0.5736 - val_loss: 0.7773 - val_accuracy: 0.3760\n",
      "Epoch 1999/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.3976 - accuracy: 0.5725 - val_loss: 0.7523 - val_accuracy: 0.3778\n",
      "Epoch 2000/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.3859 - accuracy: 0.5829 - val_loss: 0.7813 - val_accuracy: 0.3803\n",
      "Epoch 2001/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3886 - accuracy: 0.5811 - val_loss: 0.7734 - val_accuracy: 0.3744\n",
      "Epoch 2002/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.3935 - accuracy: 0.5783 - val_loss: 0.7830 - val_accuracy: 0.3741\n",
      "Epoch 2003/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3902 - accuracy: 0.5793 - val_loss: 0.7792 - val_accuracy: 0.3786\n",
      "Epoch 2004/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.3864 - accuracy: 0.5827 - val_loss: 0.7764 - val_accuracy: 0.3815\n",
      "Epoch 2005/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.3867 - accuracy: 0.5857 - val_loss: 0.7580 - val_accuracy: 0.3744\n",
      "Epoch 2006/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.3863 - accuracy: 0.5814 - val_loss: 0.7899 - val_accuracy: 0.3809\n",
      "Epoch 2007/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.4632 - accuracy: 0.5199 - val_loss: 0.7351 - val_accuracy: 0.3726\n",
      "Epoch 2008/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.4374 - accuracy: 0.5368 - val_loss: 0.7488 - val_accuracy: 0.3777\n",
      "Epoch 2009/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.4185 - accuracy: 0.5536 - val_loss: 0.7384 - val_accuracy: 0.3711\n",
      "Epoch 2010/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.4180 - accuracy: 0.5549 - val_loss: 0.7495 - val_accuracy: 0.3744\n",
      "Epoch 2011/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.4007 - accuracy: 0.5699 - val_loss: 0.7617 - val_accuracy: 0.3739\n",
      "Epoch 2012/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.3942 - accuracy: 0.5763 - val_loss: 0.7709 - val_accuracy: 0.3753\n",
      "Epoch 2013/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3931 - accuracy: 0.5764 - val_loss: 0.7715 - val_accuracy: 0.3762\n",
      "Epoch 2014/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3906 - accuracy: 0.5756 - val_loss: 0.7588 - val_accuracy: 0.3750\n",
      "Epoch 2015/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.3907 - accuracy: 0.5775 - val_loss: 0.7720 - val_accuracy: 0.3772\n",
      "Epoch 2016/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3858 - accuracy: 0.5815 - val_loss: 0.7685 - val_accuracy: 0.3756\n",
      "Epoch 2017/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3829 - accuracy: 0.5891 - val_loss: 0.7602 - val_accuracy: 0.3706\n",
      "Epoch 2018/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3849 - accuracy: 0.5826 - val_loss: 0.7710 - val_accuracy: 0.3684\n",
      "Epoch 2019/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.3923 - accuracy: 0.5759 - val_loss: 0.7814 - val_accuracy: 0.3769\n",
      "Epoch 2020/2500\n",
      "20056/20056 [==============================] - 2s 81us/step - loss: 0.3867 - accuracy: 0.5819 - val_loss: 0.7748 - val_accuracy: 0.3797\n",
      "Epoch 2021/2500\n",
      "20056/20056 [==============================] - 2s 87us/step - loss: 0.3886 - accuracy: 0.5795 - val_loss: 0.7812 - val_accuracy: 0.3829\n",
      "Epoch 2022/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.3910 - accuracy: 0.5780 - val_loss: 0.7731 - val_accuracy: 0.3745\n",
      "Epoch 2023/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4229 - accuracy: 0.5489 - val_loss: 0.7622 - val_accuracy: 0.3792\n",
      "Epoch 2024/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3998 - accuracy: 0.5674 - val_loss: 0.7696 - val_accuracy: 0.3757\n",
      "Epoch 2025/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.3934 - accuracy: 0.5773 - val_loss: 0.7735 - val_accuracy: 0.3708\n",
      "Epoch 2026/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.3888 - accuracy: 0.5782 - val_loss: 0.7874 - val_accuracy: 0.3774\n",
      "Epoch 2027/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3878 - accuracy: 0.5798 - val_loss: 0.7757 - val_accuracy: 0.3730\n",
      "Epoch 2028/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.3860 - accuracy: 0.5857 - val_loss: 0.7778 - val_accuracy: 0.3818\n",
      "Epoch 2029/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.3856 - accuracy: 0.5824 - val_loss: 0.7641 - val_accuracy: 0.3769\n",
      "Epoch 2030/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3886 - accuracy: 0.5845 - val_loss: 0.7670 - val_accuracy: 0.3778\n",
      "Epoch 2031/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3993 - accuracy: 0.5726 - val_loss: 0.7652 - val_accuracy: 0.3717\n",
      "Epoch 2032/2500\n",
      "20056/20056 [==============================] - 2s 86us/step - loss: 0.3950 - accuracy: 0.5736 - val_loss: 0.7652 - val_accuracy: 0.3747\n",
      "Epoch 2033/2500\n",
      "20056/20056 [==============================] - 2s 77us/step - loss: 0.3992 - accuracy: 0.5727 - val_loss: 0.7657 - val_accuracy: 0.3730\n",
      "Epoch 2034/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.3945 - accuracy: 0.5759 - val_loss: 0.7709 - val_accuracy: 0.3717\n",
      "Epoch 2035/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.3929 - accuracy: 0.5775 - val_loss: 0.7615 - val_accuracy: 0.3726\n",
      "Epoch 2036/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4037 - accuracy: 0.5677 - val_loss: 0.7699 - val_accuracy: 0.3714\n",
      "Epoch 2037/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.4114 - accuracy: 0.5612 - val_loss: 0.7617 - val_accuracy: 0.3672\n",
      "Epoch 2038/2500\n",
      "20056/20056 [==============================] - 2s 85us/step - loss: 0.3941 - accuracy: 0.5735 - val_loss: 0.7868 - val_accuracy: 0.3794\n",
      "Epoch 2039/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3973 - accuracy: 0.5740 - val_loss: 0.7730 - val_accuracy: 0.3771\n",
      "Epoch 2040/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.3998 - accuracy: 0.5737 - val_loss: 0.7627 - val_accuracy: 0.3745\n",
      "Epoch 2041/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4154 - accuracy: 0.5620 - val_loss: 0.7563 - val_accuracy: 0.3794\n",
      "Epoch 2042/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4039 - accuracy: 0.5704 - val_loss: 0.7718 - val_accuracy: 0.3766\n",
      "Epoch 2043/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.3899 - accuracy: 0.5775 - val_loss: 0.7724 - val_accuracy: 0.3736\n",
      "Epoch 2044/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.3913 - accuracy: 0.5821 - val_loss: 0.7608 - val_accuracy: 0.3748\n",
      "Epoch 2045/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.3881 - accuracy: 0.5813 - val_loss: 0.7653 - val_accuracy: 0.3736\n",
      "Epoch 2046/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.3835 - accuracy: 0.5863 - val_loss: 0.7768 - val_accuracy: 0.3729\n",
      "Epoch 2047/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.3857 - accuracy: 0.5845 - val_loss: 0.7739 - val_accuracy: 0.3753\n",
      "Epoch 2048/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.3857 - accuracy: 0.5856 - val_loss: 0.7583 - val_accuracy: 0.3717\n",
      "Epoch 2049/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4130 - accuracy: 0.5625 - val_loss: 0.7573 - val_accuracy: 0.3676\n",
      "Epoch 2050/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.3892 - accuracy: 0.5804 - val_loss: 0.7641 - val_accuracy: 0.3756\n",
      "Epoch 2051/2500\n",
      "20056/20056 [==============================] - 2s 77us/step - loss: 0.4043 - accuracy: 0.5656 - val_loss: 0.7565 - val_accuracy: 0.3622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2052/2500\n",
      "20056/20056 [==============================] - 2s 79us/step - loss: 0.4032 - accuracy: 0.5668 - val_loss: 0.7465 - val_accuracy: 0.3673\n",
      "Epoch 2053/2500\n",
      "20056/20056 [==============================] - 2s 78us/step - loss: 0.3961 - accuracy: 0.5674 - val_loss: 0.7779 - val_accuracy: 0.3757\n",
      "Epoch 2054/2500\n",
      "20056/20056 [==============================] - 2s 80us/step - loss: 0.4472 - accuracy: 0.5346 - val_loss: 0.7209 - val_accuracy: 0.3661\n",
      "Epoch 2055/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.4402 - accuracy: 0.5383 - val_loss: 0.7476 - val_accuracy: 0.3682\n",
      "Epoch 2056/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.4129 - accuracy: 0.5627 - val_loss: 0.7531 - val_accuracy: 0.3732\n",
      "Epoch 2057/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.4172 - accuracy: 0.5541 - val_loss: 0.7599 - val_accuracy: 0.3727\n",
      "Epoch 2058/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3959 - accuracy: 0.5706 - val_loss: 0.7688 - val_accuracy: 0.3759\n",
      "Epoch 2059/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3927 - accuracy: 0.5764 - val_loss: 0.7781 - val_accuracy: 0.3780\n",
      "Epoch 2060/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.3932 - accuracy: 0.5742 - val_loss: 0.7744 - val_accuracy: 0.3750\n",
      "Epoch 2061/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.3839 - accuracy: 0.5831 - val_loss: 0.7718 - val_accuracy: 0.3763\n",
      "Epoch 2062/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4533 - accuracy: 0.5284 - val_loss: 0.7410 - val_accuracy: 0.3651\n",
      "Epoch 2063/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4046 - accuracy: 0.5646 - val_loss: 0.7590 - val_accuracy: 0.3747\n",
      "Epoch 2064/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.4067 - accuracy: 0.5668 - val_loss: 0.7659 - val_accuracy: 0.3697\n",
      "Epoch 2065/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3884 - accuracy: 0.5795 - val_loss: 0.7791 - val_accuracy: 0.3762\n",
      "Epoch 2066/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3832 - accuracy: 0.5880 - val_loss: 0.7545 - val_accuracy: 0.3709\n",
      "Epoch 2067/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3972 - accuracy: 0.5730 - val_loss: 0.7656 - val_accuracy: 0.3775\n",
      "Epoch 2068/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.3885 - accuracy: 0.5793 - val_loss: 0.7775 - val_accuracy: 0.3800\n",
      "Epoch 2069/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3908 - accuracy: 0.5758 - val_loss: 0.7606 - val_accuracy: 0.3700\n",
      "Epoch 2070/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3927 - accuracy: 0.5741 - val_loss: 0.7606 - val_accuracy: 0.3759\n",
      "Epoch 2071/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.4127 - accuracy: 0.5635 - val_loss: 0.7396 - val_accuracy: 0.3736\n",
      "Epoch 2072/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4236 - accuracy: 0.5525 - val_loss: 0.7467 - val_accuracy: 0.3732\n",
      "Epoch 2073/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4018 - accuracy: 0.5710 - val_loss: 0.7672 - val_accuracy: 0.3742\n",
      "Epoch 2074/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3859 - accuracy: 0.5839 - val_loss: 0.7786 - val_accuracy: 0.3739\n",
      "Epoch 2075/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.3942 - accuracy: 0.5806 - val_loss: 0.7744 - val_accuracy: 0.3742\n",
      "Epoch 2076/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3859 - accuracy: 0.5854 - val_loss: 0.7724 - val_accuracy: 0.3721\n",
      "Epoch 2077/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3822 - accuracy: 0.5870 - val_loss: 0.7800 - val_accuracy: 0.3714\n",
      "Epoch 2078/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.3856 - accuracy: 0.5809 - val_loss: 0.7880 - val_accuracy: 0.3793\n",
      "Epoch 2079/2500\n",
      "20056/20056 [==============================] - 2s 77us/step - loss: 0.3943 - accuracy: 0.5767 - val_loss: 0.7664 - val_accuracy: 0.3778\n",
      "Epoch 2080/2500\n",
      "20056/20056 [==============================] - 2s 85us/step - loss: 0.3875 - accuracy: 0.5810 - val_loss: 0.7815 - val_accuracy: 0.3792\n",
      "Epoch 2081/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3838 - accuracy: 0.5865 - val_loss: 0.7620 - val_accuracy: 0.3736\n",
      "Epoch 2082/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.3892 - accuracy: 0.5800 - val_loss: 0.7708 - val_accuracy: 0.3772\n",
      "Epoch 2083/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.3863 - accuracy: 0.5848 - val_loss: 0.7604 - val_accuracy: 0.3682\n",
      "Epoch 2084/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4025 - accuracy: 0.5664 - val_loss: 0.7791 - val_accuracy: 0.3781\n",
      "Epoch 2085/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.3926 - accuracy: 0.5778 - val_loss: 0.7568 - val_accuracy: 0.3780\n",
      "Epoch 2086/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.3965 - accuracy: 0.5733 - val_loss: 0.7598 - val_accuracy: 0.3742\n",
      "Epoch 2087/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.3946 - accuracy: 0.5760 - val_loss: 0.7678 - val_accuracy: 0.3727\n",
      "Epoch 2088/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.3871 - accuracy: 0.5812 - val_loss: 0.7763 - val_accuracy: 0.3836\n",
      "Epoch 2089/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.3844 - accuracy: 0.5847 - val_loss: 0.7807 - val_accuracy: 0.3762\n",
      "Epoch 2090/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3835 - accuracy: 0.5838 - val_loss: 0.7633 - val_accuracy: 0.3706\n",
      "Epoch 2091/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3876 - accuracy: 0.5789 - val_loss: 0.7634 - val_accuracy: 0.3736\n",
      "Epoch 2092/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3895 - accuracy: 0.5786 - val_loss: 0.7823 - val_accuracy: 0.3757\n",
      "Epoch 2093/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.3908 - accuracy: 0.5761 - val_loss: 0.7631 - val_accuracy: 0.3747\n",
      "Epoch 2094/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3863 - accuracy: 0.5821 - val_loss: 0.7681 - val_accuracy: 0.3784\n",
      "Epoch 2095/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.3854 - accuracy: 0.5847 - val_loss: 0.7672 - val_accuracy: 0.3769\n",
      "Epoch 2096/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.3908 - accuracy: 0.5811 - val_loss: 0.7728 - val_accuracy: 0.3706\n",
      "Epoch 2097/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3902 - accuracy: 0.5816 - val_loss: 0.7716 - val_accuracy: 0.3805\n",
      "Epoch 2098/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3887 - accuracy: 0.5840 - val_loss: 0.7788 - val_accuracy: 0.3772\n",
      "Epoch 2099/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3950 - accuracy: 0.5758 - val_loss: 0.7738 - val_accuracy: 0.3699\n",
      "Epoch 2100/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3844 - accuracy: 0.5861 - val_loss: 0.7685 - val_accuracy: 0.3763\n",
      "Epoch 2101/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3840 - accuracy: 0.5864 - val_loss: 0.7753 - val_accuracy: 0.3765\n",
      "Epoch 2102/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.3897 - accuracy: 0.5825 - val_loss: 0.7770 - val_accuracy: 0.3771\n",
      "Epoch 2103/2500\n",
      "20056/20056 [==============================] - 2s 83us/step - loss: 0.3960 - accuracy: 0.5796 - val_loss: 0.7866 - val_accuracy: 0.3760\n",
      "Epoch 2104/2500\n",
      "20056/20056 [==============================] - 2s 88us/step - loss: 0.3968 - accuracy: 0.5737 - val_loss: 0.7642 - val_accuracy: 0.3777\n",
      "Epoch 2105/2500\n",
      "20056/20056 [==============================] - 2s 80us/step - loss: 0.4078 - accuracy: 0.5657 - val_loss: 0.7690 - val_accuracy: 0.3775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2106/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.4007 - accuracy: 0.5727 - val_loss: 0.7534 - val_accuracy: 0.3729\n",
      "Epoch 2107/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3927 - accuracy: 0.5789 - val_loss: 0.7568 - val_accuracy: 0.3670\n",
      "Epoch 2108/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.4093 - accuracy: 0.5620 - val_loss: 0.7585 - val_accuracy: 0.3723\n",
      "Epoch 2109/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3912 - accuracy: 0.5788 - val_loss: 0.7584 - val_accuracy: 0.3760\n",
      "Epoch 2110/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3854 - accuracy: 0.5823 - val_loss: 0.7774 - val_accuracy: 0.3765\n",
      "Epoch 2111/2500\n",
      "20056/20056 [==============================] - 1s 64us/step - loss: 0.3897 - accuracy: 0.5783 - val_loss: 0.7702 - val_accuracy: 0.3762\n",
      "Epoch 2112/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.3886 - accuracy: 0.5794 - val_loss: 0.7704 - val_accuracy: 0.3759\n",
      "Epoch 2113/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.4225 - accuracy: 0.5572 - val_loss: 0.7361 - val_accuracy: 0.3673\n",
      "Epoch 2114/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4329 - accuracy: 0.5446 - val_loss: 0.7504 - val_accuracy: 0.3780\n",
      "Epoch 2115/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.4123 - accuracy: 0.5590 - val_loss: 0.7772 - val_accuracy: 0.3765\n",
      "Epoch 2116/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.3942 - accuracy: 0.5773 - val_loss: 0.7698 - val_accuracy: 0.3768\n",
      "Epoch 2117/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3924 - accuracy: 0.5784 - val_loss: 0.7686 - val_accuracy: 0.3772\n",
      "Epoch 2118/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.4006 - accuracy: 0.5693 - val_loss: 0.7863 - val_accuracy: 0.3775\n",
      "Epoch 2119/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.3923 - accuracy: 0.5755 - val_loss: 0.7694 - val_accuracy: 0.3750\n",
      "Epoch 2120/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3911 - accuracy: 0.5795 - val_loss: 0.7668 - val_accuracy: 0.3766\n",
      "Epoch 2121/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3909 - accuracy: 0.5817 - val_loss: 0.7699 - val_accuracy: 0.3766\n",
      "Epoch 2122/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3871 - accuracy: 0.5833 - val_loss: 0.7679 - val_accuracy: 0.3780\n",
      "Epoch 2123/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.4149 - accuracy: 0.5590 - val_loss: 0.7518 - val_accuracy: 0.3766\n",
      "Epoch 2124/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.3906 - accuracy: 0.5800 - val_loss: 0.7544 - val_accuracy: 0.3762\n",
      "Epoch 2125/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3838 - accuracy: 0.5831 - val_loss: 0.7711 - val_accuracy: 0.3712\n",
      "Epoch 2126/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.3933 - accuracy: 0.5790 - val_loss: 0.7564 - val_accuracy: 0.3705\n",
      "Epoch 2127/2500\n",
      "20056/20056 [==============================] - 2s 81us/step - loss: 0.3941 - accuracy: 0.5730 - val_loss: 0.7541 - val_accuracy: 0.3751\n",
      "Epoch 2128/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3989 - accuracy: 0.5716 - val_loss: 0.7666 - val_accuracy: 0.3730\n",
      "Epoch 2129/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3900 - accuracy: 0.5804 - val_loss: 0.7666 - val_accuracy: 0.3744\n",
      "Epoch 2130/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3869 - accuracy: 0.5838 - val_loss: 0.7663 - val_accuracy: 0.3751\n",
      "Epoch 2131/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.3863 - accuracy: 0.5855 - val_loss: 0.7648 - val_accuracy: 0.3733\n",
      "Epoch 2132/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3850 - accuracy: 0.5848 - val_loss: 0.7717 - val_accuracy: 0.3765\n",
      "Epoch 2133/2500\n",
      "20056/20056 [==============================] - 2s 84us/step - loss: 0.3828 - accuracy: 0.5868 - val_loss: 0.7835 - val_accuracy: 0.3774\n",
      "Epoch 2134/2500\n",
      "20056/20056 [==============================] - 2s 85us/step - loss: 0.4065 - accuracy: 0.5688 - val_loss: 0.7302 - val_accuracy: 0.3762\n",
      "Epoch 2135/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3955 - accuracy: 0.5751 - val_loss: 0.7735 - val_accuracy: 0.3778\n",
      "Epoch 2136/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3950 - accuracy: 0.5764 - val_loss: 0.7784 - val_accuracy: 0.3768\n",
      "Epoch 2137/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3912 - accuracy: 0.5746 - val_loss: 0.7720 - val_accuracy: 0.3778\n",
      "Epoch 2138/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.3817 - accuracy: 0.5861 - val_loss: 0.7780 - val_accuracy: 0.3777\n",
      "Epoch 2139/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.3852 - accuracy: 0.5851 - val_loss: 0.7733 - val_accuracy: 0.3727\n",
      "Epoch 2140/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3834 - accuracy: 0.5871 - val_loss: 0.7712 - val_accuracy: 0.3729\n",
      "Epoch 2141/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.3894 - accuracy: 0.5814 - val_loss: 0.7731 - val_accuracy: 0.3772\n",
      "Epoch 2142/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3898 - accuracy: 0.5794 - val_loss: 0.7603 - val_accuracy: 0.3756\n",
      "Epoch 2143/2500\n",
      "20056/20056 [==============================] - 2s 91us/step - loss: 0.3820 - accuracy: 0.5849 - val_loss: 0.7774 - val_accuracy: 0.3792\n",
      "Epoch 2144/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.3859 - accuracy: 0.5852 - val_loss: 0.7966 - val_accuracy: 0.3729\n",
      "Epoch 2145/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3880 - accuracy: 0.5845 - val_loss: 0.7696 - val_accuracy: 0.3771\n",
      "Epoch 2146/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3896 - accuracy: 0.5789 - val_loss: 0.7787 - val_accuracy: 0.3789\n",
      "Epoch 2147/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.4058 - accuracy: 0.5700 - val_loss: 0.7749 - val_accuracy: 0.3772\n",
      "Epoch 2148/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.3930 - accuracy: 0.5746 - val_loss: 0.7828 - val_accuracy: 0.3799\n",
      "Epoch 2149/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3873 - accuracy: 0.5798 - val_loss: 0.7758 - val_accuracy: 0.3733\n",
      "Epoch 2150/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3869 - accuracy: 0.5837 - val_loss: 0.7608 - val_accuracy: 0.3735\n",
      "Epoch 2151/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.3895 - accuracy: 0.5781 - val_loss: 0.7860 - val_accuracy: 0.3765\n",
      "Epoch 2152/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.3829 - accuracy: 0.5850 - val_loss: 0.7840 - val_accuracy: 0.3757\n",
      "Epoch 2153/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3926 - accuracy: 0.5773 - val_loss: 0.7638 - val_accuracy: 0.3757\n",
      "Epoch 2154/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3823 - accuracy: 0.5841 - val_loss: 0.7871 - val_accuracy: 0.3781\n",
      "Epoch 2155/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.3921 - accuracy: 0.5801 - val_loss: 0.7601 - val_accuracy: 0.3714\n",
      "Epoch 2156/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3954 - accuracy: 0.5769 - val_loss: 0.7609 - val_accuracy: 0.3643\n",
      "Epoch 2157/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3932 - accuracy: 0.5787 - val_loss: 0.7773 - val_accuracy: 0.3730\n",
      "Epoch 2158/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3956 - accuracy: 0.5754 - val_loss: 0.7630 - val_accuracy: 0.3763\n",
      "Epoch 2159/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.4022 - accuracy: 0.5709 - val_loss: 0.7657 - val_accuracy: 0.3705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2160/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.3959 - accuracy: 0.5756 - val_loss: 0.7654 - val_accuracy: 0.3724\n",
      "Epoch 2161/2500\n",
      "20056/20056 [==============================] - 2s 89us/step - loss: 0.3909 - accuracy: 0.5772 - val_loss: 0.7631 - val_accuracy: 0.3729\n",
      "Epoch 2162/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.3822 - accuracy: 0.5883 - val_loss: 0.7636 - val_accuracy: 0.3729\n",
      "Epoch 2163/2500\n",
      "20056/20056 [==============================] - 2s 86us/step - loss: 0.3801 - accuracy: 0.5840 - val_loss: 0.7743 - val_accuracy: 0.3712\n",
      "Epoch 2164/2500\n",
      "20056/20056 [==============================] - 2s 77us/step - loss: 0.3807 - accuracy: 0.5890 - val_loss: 0.7719 - val_accuracy: 0.3763\n",
      "Epoch 2165/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4343 - accuracy: 0.5442 - val_loss: 0.7496 - val_accuracy: 0.3753\n",
      "Epoch 2166/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3985 - accuracy: 0.5727 - val_loss: 0.7692 - val_accuracy: 0.3799\n",
      "Epoch 2167/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3841 - accuracy: 0.5843 - val_loss: 0.7797 - val_accuracy: 0.3783\n",
      "Epoch 2168/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.4033 - accuracy: 0.5709 - val_loss: 0.7655 - val_accuracy: 0.3718\n",
      "Epoch 2169/2500\n",
      "20056/20056 [==============================] - 2s 80us/step - loss: 0.3804 - accuracy: 0.5895 - val_loss: 0.7783 - val_accuracy: 0.3711\n",
      "Epoch 2170/2500\n",
      "20056/20056 [==============================] - 2s 84us/step - loss: 0.3727 - accuracy: 0.5936 - val_loss: 0.7845 - val_accuracy: 0.3756\n",
      "Epoch 2171/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3808 - accuracy: 0.5886 - val_loss: 0.7658 - val_accuracy: 0.3792\n",
      "Epoch 2172/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.4148 - accuracy: 0.5598 - val_loss: 0.7609 - val_accuracy: 0.3763\n",
      "Epoch 2173/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.4003 - accuracy: 0.5741 - val_loss: 0.7560 - val_accuracy: 0.3750\n",
      "Epoch 2174/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3907 - accuracy: 0.5802 - val_loss: 0.7680 - val_accuracy: 0.3745\n",
      "Epoch 2175/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3888 - accuracy: 0.5813 - val_loss: 0.7748 - val_accuracy: 0.3759\n",
      "Epoch 2176/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3883 - accuracy: 0.5819 - val_loss: 0.7657 - val_accuracy: 0.3771\n",
      "Epoch 2177/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3856 - accuracy: 0.5858 - val_loss: 0.7868 - val_accuracy: 0.3808\n",
      "Epoch 2178/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3807 - accuracy: 0.5928 - val_loss: 0.7819 - val_accuracy: 0.3827\n",
      "Epoch 2179/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.3861 - accuracy: 0.5835 - val_loss: 0.7834 - val_accuracy: 0.3748\n",
      "Epoch 2180/2500\n",
      "20056/20056 [==============================] - 2s 84us/step - loss: 0.3805 - accuracy: 0.5916 - val_loss: 0.7821 - val_accuracy: 0.3760\n",
      "Epoch 2181/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.3836 - accuracy: 0.5856 - val_loss: 0.7651 - val_accuracy: 0.3769\n",
      "Epoch 2182/2500\n",
      "20056/20056 [==============================] - 2s 88us/step - loss: 0.4059 - accuracy: 0.5663 - val_loss: 0.7791 - val_accuracy: 0.3750\n",
      "Epoch 2183/2500\n",
      "20056/20056 [==============================] - 2s 85us/step - loss: 0.3860 - accuracy: 0.5809 - val_loss: 0.7739 - val_accuracy: 0.3744\n",
      "Epoch 2184/2500\n",
      "20056/20056 [==============================] - 2s 92us/step - loss: 0.3861 - accuracy: 0.5819 - val_loss: 0.7899 - val_accuracy: 0.3750\n",
      "Epoch 2185/2500\n",
      "20056/20056 [==============================] - 2s 77us/step - loss: 0.3838 - accuracy: 0.5851 - val_loss: 0.7820 - val_accuracy: 0.3765\n",
      "Epoch 2186/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3763 - accuracy: 0.5943 - val_loss: 0.7716 - val_accuracy: 0.3775\n",
      "Epoch 2187/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3943 - accuracy: 0.5835 - val_loss: 0.7733 - val_accuracy: 0.3797\n",
      "Epoch 2188/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3857 - accuracy: 0.5861 - val_loss: 0.7772 - val_accuracy: 0.3799\n",
      "Epoch 2189/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.3840 - accuracy: 0.5854 - val_loss: 0.7869 - val_accuracy: 0.3735\n",
      "Epoch 2190/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3846 - accuracy: 0.5855 - val_loss: 0.7802 - val_accuracy: 0.3778\n",
      "Epoch 2191/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3936 - accuracy: 0.5779 - val_loss: 0.7752 - val_accuracy: 0.3756\n",
      "Epoch 2192/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.3814 - accuracy: 0.5896 - val_loss: 0.7733 - val_accuracy: 0.3765\n",
      "Epoch 2193/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3841 - accuracy: 0.5829 - val_loss: 0.7689 - val_accuracy: 0.3727\n",
      "Epoch 2194/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3811 - accuracy: 0.5857 - val_loss: 0.7767 - val_accuracy: 0.3745\n",
      "Epoch 2195/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.3702 - accuracy: 0.5938 - val_loss: 0.7908 - val_accuracy: 0.3824\n",
      "Epoch 2196/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3811 - accuracy: 0.5912 - val_loss: 0.7835 - val_accuracy: 0.3786\n",
      "Epoch 2197/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3865 - accuracy: 0.5876 - val_loss: 0.7927 - val_accuracy: 0.3760\n",
      "Epoch 2198/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3818 - accuracy: 0.5876 - val_loss: 0.7834 - val_accuracy: 0.3786\n",
      "Epoch 2199/2500\n",
      "20056/20056 [==============================] - 2s 75us/step - loss: 0.3867 - accuracy: 0.5842 - val_loss: 0.7721 - val_accuracy: 0.3792\n",
      "Epoch 2200/2500\n",
      "20056/20056 [==============================] - 1s 75us/step - loss: 0.3840 - accuracy: 0.5869 - val_loss: 0.7737 - val_accuracy: 0.3800\n",
      "Epoch 2201/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3801 - accuracy: 0.5863 - val_loss: 0.7906 - val_accuracy: 0.3799\n",
      "Epoch 2202/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3797 - accuracy: 0.5902 - val_loss: 0.7686 - val_accuracy: 0.3747\n",
      "Epoch 2203/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3779 - accuracy: 0.5905 - val_loss: 0.7920 - val_accuracy: 0.3735\n",
      "Epoch 2204/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3812 - accuracy: 0.5903 - val_loss: 0.7824 - val_accuracy: 0.3772\n",
      "Epoch 2205/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3861 - accuracy: 0.5863 - val_loss: 0.7735 - val_accuracy: 0.3751\n",
      "Epoch 2206/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3810 - accuracy: 0.5868 - val_loss: 0.7759 - val_accuracy: 0.3786\n",
      "Epoch 2207/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3772 - accuracy: 0.5897 - val_loss: 0.7870 - val_accuracy: 0.3769\n",
      "Epoch 2208/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3735 - accuracy: 0.5930 - val_loss: 0.7794 - val_accuracy: 0.3783\n",
      "Epoch 2209/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.3858 - accuracy: 0.5820 - val_loss: 0.7655 - val_accuracy: 0.3733\n",
      "Epoch 2210/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3952 - accuracy: 0.5806 - val_loss: 0.7726 - val_accuracy: 0.3714\n",
      "Epoch 2211/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3863 - accuracy: 0.5846 - val_loss: 0.7823 - val_accuracy: 0.3768\n",
      "Epoch 2212/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3835 - accuracy: 0.5853 - val_loss: 0.7685 - val_accuracy: 0.3762\n",
      "Epoch 2213/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3886 - accuracy: 0.5797 - val_loss: 0.7906 - val_accuracy: 0.3800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2214/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3892 - accuracy: 0.5818 - val_loss: 0.7603 - val_accuracy: 0.3715\n",
      "Epoch 2215/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3887 - accuracy: 0.5797 - val_loss: 0.7799 - val_accuracy: 0.3748\n",
      "Epoch 2216/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3975 - accuracy: 0.5715 - val_loss: 0.7770 - val_accuracy: 0.3726\n",
      "Epoch 2217/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3832 - accuracy: 0.5883 - val_loss: 0.7810 - val_accuracy: 0.3738\n",
      "Epoch 2218/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3753 - accuracy: 0.5905 - val_loss: 0.7829 - val_accuracy: 0.3784\n",
      "Epoch 2219/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3930 - accuracy: 0.5765 - val_loss: 0.7738 - val_accuracy: 0.3757\n",
      "Epoch 2220/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3874 - accuracy: 0.5786 - val_loss: 0.7557 - val_accuracy: 0.3747\n",
      "Epoch 2221/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.3952 - accuracy: 0.5758 - val_loss: 0.7728 - val_accuracy: 0.3751\n",
      "Epoch 2222/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3844 - accuracy: 0.5843 - val_loss: 0.7671 - val_accuracy: 0.3733\n",
      "Epoch 2223/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3941 - accuracy: 0.5799 - val_loss: 0.7729 - val_accuracy: 0.3744\n",
      "Epoch 2224/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3820 - accuracy: 0.5847 - val_loss: 0.7655 - val_accuracy: 0.3768\n",
      "Epoch 2225/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3942 - accuracy: 0.5770 - val_loss: 0.7639 - val_accuracy: 0.3760\n",
      "Epoch 2226/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3835 - accuracy: 0.5833 - val_loss: 0.7752 - val_accuracy: 0.3781\n",
      "Epoch 2227/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3811 - accuracy: 0.5860 - val_loss: 0.7831 - val_accuracy: 0.3729\n",
      "Epoch 2228/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.3856 - accuracy: 0.5839 - val_loss: 0.7782 - val_accuracy: 0.3786\n",
      "Epoch 2229/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3928 - accuracy: 0.5788 - val_loss: 0.7730 - val_accuracy: 0.3777\n",
      "Epoch 2230/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3877 - accuracy: 0.5808 - val_loss: 0.7631 - val_accuracy: 0.3751\n",
      "Epoch 2231/2500\n",
      "20056/20056 [==============================] - 2s 77us/step - loss: 0.3834 - accuracy: 0.5870 - val_loss: 0.7663 - val_accuracy: 0.3721\n",
      "Epoch 2232/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.3797 - accuracy: 0.5910 - val_loss: 0.7841 - val_accuracy: 0.3727\n",
      "Epoch 2233/2500\n",
      "20056/20056 [==============================] - 2s 85us/step - loss: 0.3826 - accuracy: 0.5871 - val_loss: 0.7802 - val_accuracy: 0.3783\n",
      "Epoch 2234/2500\n",
      "20056/20056 [==============================] - 2s 92us/step - loss: 0.3812 - accuracy: 0.5883 - val_loss: 0.7690 - val_accuracy: 0.3735\n",
      "Epoch 2235/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.3807 - accuracy: 0.5861 - val_loss: 0.7867 - val_accuracy: 0.3742\n",
      "Epoch 2236/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3776 - accuracy: 0.5917 - val_loss: 0.7831 - val_accuracy: 0.3775\n",
      "Epoch 2237/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3840 - accuracy: 0.5883 - val_loss: 0.7577 - val_accuracy: 0.3775\n",
      "Epoch 2238/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3928 - accuracy: 0.5801 - val_loss: 0.7768 - val_accuracy: 0.3781\n",
      "Epoch 2239/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3827 - accuracy: 0.5861 - val_loss: 0.7862 - val_accuracy: 0.3745\n",
      "Epoch 2240/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3882 - accuracy: 0.5878 - val_loss: 0.7873 - val_accuracy: 0.3660\n",
      "Epoch 2241/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3935 - accuracy: 0.5782 - val_loss: 0.7747 - val_accuracy: 0.3735\n",
      "Epoch 2242/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3907 - accuracy: 0.5787 - val_loss: 0.7779 - val_accuracy: 0.3742\n",
      "Epoch 2243/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.3877 - accuracy: 0.5837 - val_loss: 0.7736 - val_accuracy: 0.3809\n",
      "Epoch 2244/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.3829 - accuracy: 0.5869 - val_loss: 0.7742 - val_accuracy: 0.3709\n",
      "Epoch 2245/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.3749 - accuracy: 0.5901 - val_loss: 0.7930 - val_accuracy: 0.3730\n",
      "Epoch 2246/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3732 - accuracy: 0.5943 - val_loss: 0.7846 - val_accuracy: 0.3792\n",
      "Epoch 2247/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3815 - accuracy: 0.5881 - val_loss: 0.7841 - val_accuracy: 0.3794\n",
      "Epoch 2248/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3812 - accuracy: 0.5903 - val_loss: 0.7704 - val_accuracy: 0.3714\n",
      "Epoch 2249/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.4000 - accuracy: 0.5714 - val_loss: 0.7720 - val_accuracy: 0.3784\n",
      "Epoch 2250/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3820 - accuracy: 0.5884 - val_loss: 0.7731 - val_accuracy: 0.3772\n",
      "Epoch 2251/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3754 - accuracy: 0.5961 - val_loss: 0.7874 - val_accuracy: 0.3824\n",
      "Epoch 2252/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.3758 - accuracy: 0.5941 - val_loss: 0.7829 - val_accuracy: 0.3741\n",
      "Epoch 2253/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.3783 - accuracy: 0.5899 - val_loss: 0.8011 - val_accuracy: 0.3780\n",
      "Epoch 2254/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3796 - accuracy: 0.5891 - val_loss: 0.7920 - val_accuracy: 0.3793\n",
      "Epoch 2255/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3786 - accuracy: 0.5905 - val_loss: 0.7872 - val_accuracy: 0.3786\n",
      "Epoch 2256/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3689 - accuracy: 0.6005 - val_loss: 0.7936 - val_accuracy: 0.3780\n",
      "Epoch 2257/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3866 - accuracy: 0.5876 - val_loss: 0.7888 - val_accuracy: 0.3786\n",
      "Epoch 2258/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3815 - accuracy: 0.5893 - val_loss: 0.7753 - val_accuracy: 0.3778\n",
      "Epoch 2259/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.4017 - accuracy: 0.5732 - val_loss: 0.7759 - val_accuracy: 0.3763\n",
      "Epoch 2260/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3831 - accuracy: 0.5821 - val_loss: 0.7843 - val_accuracy: 0.3717\n",
      "Epoch 2261/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3832 - accuracy: 0.5856 - val_loss: 0.7810 - val_accuracy: 0.3726\n",
      "Epoch 2262/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3798 - accuracy: 0.5901 - val_loss: 0.7847 - val_accuracy: 0.3769\n",
      "Epoch 2263/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3886 - accuracy: 0.5792 - val_loss: 0.7839 - val_accuracy: 0.3732\n",
      "Epoch 2264/2500\n",
      "20056/20056 [==============================] - 2s 82us/step - loss: 0.3805 - accuracy: 0.5868 - val_loss: 0.7795 - val_accuracy: 0.3760\n",
      "Epoch 2265/2500\n",
      "20056/20056 [==============================] - 2s 77us/step - loss: 0.3779 - accuracy: 0.5901 - val_loss: 0.7827 - val_accuracy: 0.3786\n",
      "Epoch 2266/2500\n",
      "20056/20056 [==============================] - 2s 79us/step - loss: 0.3845 - accuracy: 0.5883 - val_loss: 0.7814 - val_accuracy: 0.3792\n",
      "Epoch 2267/2500\n",
      "20056/20056 [==============================] - 2s 83us/step - loss: 0.4019 - accuracy: 0.5766 - val_loss: 0.7687 - val_accuracy: 0.3787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2268/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3915 - accuracy: 0.5852 - val_loss: 0.7540 - val_accuracy: 0.3790\n",
      "Epoch 2269/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.3933 - accuracy: 0.5786 - val_loss: 0.7527 - val_accuracy: 0.3812\n",
      "Epoch 2270/2500\n",
      "20056/20056 [==============================] - 2s 84us/step - loss: 0.3956 - accuracy: 0.5768 - val_loss: 0.7721 - val_accuracy: 0.3847\n",
      "Epoch 2271/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3929 - accuracy: 0.5799 - val_loss: 0.7648 - val_accuracy: 0.3821\n",
      "Epoch 2272/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4043 - accuracy: 0.5682 - val_loss: 0.7734 - val_accuracy: 0.3757\n",
      "Epoch 2273/2500\n",
      "20056/20056 [==============================] - 2s 75us/step - loss: 0.3764 - accuracy: 0.5928 - val_loss: 0.7759 - val_accuracy: 0.3823\n",
      "Epoch 2274/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3856 - accuracy: 0.5872 - val_loss: 0.7721 - val_accuracy: 0.3793\n",
      "Epoch 2275/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3894 - accuracy: 0.5803 - val_loss: 0.7815 - val_accuracy: 0.3738\n",
      "Epoch 2276/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.3818 - accuracy: 0.5900 - val_loss: 0.7763 - val_accuracy: 0.3735\n",
      "Epoch 2277/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3885 - accuracy: 0.5837 - val_loss: 0.7828 - val_accuracy: 0.3762\n",
      "Epoch 2278/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.4002 - accuracy: 0.5738 - val_loss: 0.7795 - val_accuracy: 0.3780\n",
      "Epoch 2279/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.4259 - accuracy: 0.5556 - val_loss: 0.7599 - val_accuracy: 0.3684\n",
      "Epoch 2280/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.4006 - accuracy: 0.5689 - val_loss: 0.7738 - val_accuracy: 0.3775\n",
      "Epoch 2281/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3850 - accuracy: 0.5854 - val_loss: 0.7488 - val_accuracy: 0.3781\n",
      "Epoch 2282/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.4046 - accuracy: 0.5670 - val_loss: 0.7652 - val_accuracy: 0.3721\n",
      "Epoch 2283/2500\n",
      "20056/20056 [==============================] - 2s 99us/step - loss: 0.4033 - accuracy: 0.5698 - val_loss: 0.7817 - val_accuracy: 0.3778\n",
      "Epoch 2284/2500\n",
      "20056/20056 [==============================] - 2s 88us/step - loss: 0.3947 - accuracy: 0.5759 - val_loss: 0.7609 - val_accuracy: 0.3756\n",
      "Epoch 2285/2500\n",
      "20056/20056 [==============================] - 2s 76us/step - loss: 0.3822 - accuracy: 0.5871 - val_loss: 0.7969 - val_accuracy: 0.3697\n",
      "Epoch 2286/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.3941 - accuracy: 0.5815 - val_loss: 0.7799 - val_accuracy: 0.3742\n",
      "Epoch 2287/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3853 - accuracy: 0.5847 - val_loss: 0.7567 - val_accuracy: 0.3696\n",
      "Epoch 2288/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3835 - accuracy: 0.5877 - val_loss: 0.7457 - val_accuracy: 0.3681\n",
      "Epoch 2289/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3822 - accuracy: 0.5880 - val_loss: 0.7672 - val_accuracy: 0.3714\n",
      "Epoch 2290/2500\n",
      "20056/20056 [==============================] - 2s 77us/step - loss: 0.3860 - accuracy: 0.5856 - val_loss: 0.7563 - val_accuracy: 0.3709\n",
      "Epoch 2291/2500\n",
      "20056/20056 [==============================] - 2s 96us/step - loss: 0.3902 - accuracy: 0.5799 - val_loss: 0.7783 - val_accuracy: 0.3726\n",
      "Epoch 2292/2500\n",
      "20056/20056 [==============================] - 2s 83us/step - loss: 0.3812 - accuracy: 0.5912 - val_loss: 0.7882 - val_accuracy: 0.3751\n",
      "Epoch 2293/2500\n",
      "20056/20056 [==============================] - 2s 95us/step - loss: 0.3737 - accuracy: 0.5980 - val_loss: 0.7866 - val_accuracy: 0.3744\n",
      "Epoch 2294/2500\n",
      "20056/20056 [==============================] - 2s 91us/step - loss: 0.3807 - accuracy: 0.5891 - val_loss: 0.7795 - val_accuracy: 0.3730\n",
      "Epoch 2295/2500\n",
      "20056/20056 [==============================] - 2s 80us/step - loss: 0.3745 - accuracy: 0.5962 - val_loss: 0.7807 - val_accuracy: 0.3721\n",
      "Epoch 2296/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.4000 - accuracy: 0.5735 - val_loss: 0.7763 - val_accuracy: 0.3726\n",
      "Epoch 2297/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3884 - accuracy: 0.5828 - val_loss: 0.7779 - val_accuracy: 0.3720\n",
      "Epoch 2298/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3803 - accuracy: 0.5867 - val_loss: 0.7744 - val_accuracy: 0.3777\n",
      "Epoch 2299/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3778 - accuracy: 0.5934 - val_loss: 0.7822 - val_accuracy: 0.3711\n",
      "Epoch 2300/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3912 - accuracy: 0.5837 - val_loss: 0.7366 - val_accuracy: 0.3634\n",
      "Epoch 2301/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3992 - accuracy: 0.5753 - val_loss: 0.7690 - val_accuracy: 0.3824\n",
      "Epoch 2302/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.3781 - accuracy: 0.5936 - val_loss: 0.7745 - val_accuracy: 0.3768\n",
      "Epoch 2303/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.3763 - accuracy: 0.5924 - val_loss: 0.7914 - val_accuracy: 0.3783\n",
      "Epoch 2304/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.3765 - accuracy: 0.5927 - val_loss: 0.7768 - val_accuracy: 0.3780\n",
      "Epoch 2305/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.3789 - accuracy: 0.5903 - val_loss: 0.7778 - val_accuracy: 0.3735\n",
      "Epoch 2306/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3804 - accuracy: 0.5917 - val_loss: 0.7806 - val_accuracy: 0.3765\n",
      "Epoch 2307/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3853 - accuracy: 0.5854 - val_loss: 0.7812 - val_accuracy: 0.3739\n",
      "Epoch 2308/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3810 - accuracy: 0.5912 - val_loss: 0.7788 - val_accuracy: 0.3814\n",
      "Epoch 2309/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3833 - accuracy: 0.5872 - val_loss: 0.7747 - val_accuracy: 0.3775\n",
      "Epoch 2310/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3840 - accuracy: 0.5881 - val_loss: 0.7701 - val_accuracy: 0.3681\n",
      "Epoch 2311/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3813 - accuracy: 0.5891 - val_loss: 0.7594 - val_accuracy: 0.3738\n",
      "Epoch 2312/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3884 - accuracy: 0.5828 - val_loss: 0.7714 - val_accuracy: 0.3768\n",
      "Epoch 2313/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3781 - accuracy: 0.5891 - val_loss: 0.7869 - val_accuracy: 0.3744\n",
      "Epoch 2314/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3956 - accuracy: 0.5800 - val_loss: 0.7643 - val_accuracy: 0.3732\n",
      "Epoch 2315/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3842 - accuracy: 0.5850 - val_loss: 0.7763 - val_accuracy: 0.3727\n",
      "Epoch 2316/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3872 - accuracy: 0.5833 - val_loss: 0.7764 - val_accuracy: 0.3780\n",
      "Epoch 2317/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.3838 - accuracy: 0.5869 - val_loss: 0.7769 - val_accuracy: 0.3730\n",
      "Epoch 2318/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.3822 - accuracy: 0.5873 - val_loss: 0.7823 - val_accuracy: 0.3738\n",
      "Epoch 2319/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.3730 - accuracy: 0.5934 - val_loss: 0.7827 - val_accuracy: 0.3715\n",
      "Epoch 2320/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.3787 - accuracy: 0.5927 - val_loss: 0.7673 - val_accuracy: 0.3762\n",
      "Epoch 2321/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.3797 - accuracy: 0.5895 - val_loss: 0.7765 - val_accuracy: 0.3750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2322/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3917 - accuracy: 0.5851 - val_loss: 0.7783 - val_accuracy: 0.3748\n",
      "Epoch 2323/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3841 - accuracy: 0.5902 - val_loss: 0.7723 - val_accuracy: 0.3792\n",
      "Epoch 2324/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.3840 - accuracy: 0.5902 - val_loss: 0.7610 - val_accuracy: 0.3714\n",
      "Epoch 2325/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.3900 - accuracy: 0.5814 - val_loss: 0.7504 - val_accuracy: 0.3769\n",
      "Epoch 2326/2500\n",
      "20056/20056 [==============================] - 1s 70us/step - loss: 0.3765 - accuracy: 0.5913 - val_loss: 0.7732 - val_accuracy: 0.3762\n",
      "Epoch 2327/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3740 - accuracy: 0.5941 - val_loss: 0.7904 - val_accuracy: 0.3732\n",
      "Epoch 2328/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3834 - accuracy: 0.5903 - val_loss: 0.7895 - val_accuracy: 0.3742\n",
      "Epoch 2329/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3799 - accuracy: 0.5930 - val_loss: 0.7747 - val_accuracy: 0.3787\n",
      "Epoch 2330/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3761 - accuracy: 0.5910 - val_loss: 0.7731 - val_accuracy: 0.3787\n",
      "Epoch 2331/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3761 - accuracy: 0.5952 - val_loss: 0.7646 - val_accuracy: 0.3827\n",
      "Epoch 2332/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3725 - accuracy: 0.5965 - val_loss: 0.7833 - val_accuracy: 0.3799\n",
      "Epoch 2333/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3793 - accuracy: 0.5946 - val_loss: 0.7816 - val_accuracy: 0.3781\n",
      "Epoch 2334/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3752 - accuracy: 0.5955 - val_loss: 0.7876 - val_accuracy: 0.3808\n",
      "Epoch 2335/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.3807 - accuracy: 0.5934 - val_loss: 0.7697 - val_accuracy: 0.3706\n",
      "Epoch 2336/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3769 - accuracy: 0.5978 - val_loss: 0.7769 - val_accuracy: 0.3799\n",
      "Epoch 2337/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3718 - accuracy: 0.5958 - val_loss: 0.7654 - val_accuracy: 0.3706\n",
      "Epoch 2338/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3808 - accuracy: 0.5945 - val_loss: 0.7661 - val_accuracy: 0.3745\n",
      "Epoch 2339/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3885 - accuracy: 0.5886 - val_loss: 0.7646 - val_accuracy: 0.3747\n",
      "Epoch 2340/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.4086 - accuracy: 0.5694 - val_loss: 0.7489 - val_accuracy: 0.3714\n",
      "Epoch 2341/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3938 - accuracy: 0.5739 - val_loss: 0.7765 - val_accuracy: 0.3775\n",
      "Epoch 2342/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3886 - accuracy: 0.5827 - val_loss: 0.7689 - val_accuracy: 0.3789\n",
      "Epoch 2343/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3789 - accuracy: 0.5911 - val_loss: 0.7562 - val_accuracy: 0.3715\n",
      "Epoch 2344/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.4017 - accuracy: 0.5759 - val_loss: 0.7793 - val_accuracy: 0.3741\n",
      "Epoch 2345/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3979 - accuracy: 0.5751 - val_loss: 0.7593 - val_accuracy: 0.3747\n",
      "Epoch 2346/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3862 - accuracy: 0.5849 - val_loss: 0.7563 - val_accuracy: 0.3724\n",
      "Epoch 2347/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3758 - accuracy: 0.5916 - val_loss: 0.7708 - val_accuracy: 0.3753\n",
      "Epoch 2348/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3845 - accuracy: 0.5829 - val_loss: 0.7886 - val_accuracy: 0.3762\n",
      "Epoch 2349/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.4092 - accuracy: 0.5668 - val_loss: 0.7635 - val_accuracy: 0.3756\n",
      "Epoch 2350/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3783 - accuracy: 0.5904 - val_loss: 0.7655 - val_accuracy: 0.3688\n",
      "Epoch 2351/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.3836 - accuracy: 0.5848 - val_loss: 0.7680 - val_accuracy: 0.3693\n",
      "Epoch 2352/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3819 - accuracy: 0.5862 - val_loss: 0.7592 - val_accuracy: 0.3706\n",
      "Epoch 2353/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3869 - accuracy: 0.5828 - val_loss: 0.7701 - val_accuracy: 0.3679\n",
      "Epoch 2354/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3832 - accuracy: 0.5874 - val_loss: 0.7696 - val_accuracy: 0.3664\n",
      "Epoch 2355/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3884 - accuracy: 0.5791 - val_loss: 0.7663 - val_accuracy: 0.3765\n",
      "Epoch 2356/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3864 - accuracy: 0.5830 - val_loss: 0.7708 - val_accuracy: 0.3709\n",
      "Epoch 2357/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.3801 - accuracy: 0.5877 - val_loss: 0.7755 - val_accuracy: 0.3754\n",
      "Epoch 2358/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.3841 - accuracy: 0.5863 - val_loss: 0.7703 - val_accuracy: 0.3794\n",
      "Epoch 2359/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3839 - accuracy: 0.5874 - val_loss: 0.7721 - val_accuracy: 0.3775\n",
      "Epoch 2360/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3929 - accuracy: 0.5810 - val_loss: 0.7589 - val_accuracy: 0.3769\n",
      "Epoch 2361/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.3803 - accuracy: 0.5848 - val_loss: 0.7643 - val_accuracy: 0.3738\n",
      "Epoch 2362/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3765 - accuracy: 0.5917 - val_loss: 0.7696 - val_accuracy: 0.3780\n",
      "Epoch 2363/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3848 - accuracy: 0.5865 - val_loss: 0.7657 - val_accuracy: 0.3757\n",
      "Epoch 2364/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3818 - accuracy: 0.5894 - val_loss: 0.7769 - val_accuracy: 0.3772\n",
      "Epoch 2365/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.3808 - accuracy: 0.5929 - val_loss: 0.7572 - val_accuracy: 0.3808\n",
      "Epoch 2366/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3923 - accuracy: 0.5791 - val_loss: 0.7639 - val_accuracy: 0.3759\n",
      "Epoch 2367/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3905 - accuracy: 0.5813 - val_loss: 0.7623 - val_accuracy: 0.3778\n",
      "Epoch 2368/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.4027 - accuracy: 0.5713 - val_loss: 0.7644 - val_accuracy: 0.3726\n",
      "Epoch 2369/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.3832 - accuracy: 0.5869 - val_loss: 0.7722 - val_accuracy: 0.3720\n",
      "Epoch 2370/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3822 - accuracy: 0.5873 - val_loss: 0.7655 - val_accuracy: 0.3762\n",
      "Epoch 2371/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3811 - accuracy: 0.5929 - val_loss: 0.7667 - val_accuracy: 0.3729\n",
      "Epoch 2372/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3904 - accuracy: 0.5791 - val_loss: 0.7725 - val_accuracy: 0.3753\n",
      "Epoch 2373/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.3852 - accuracy: 0.5862 - val_loss: 0.7934 - val_accuracy: 0.3756\n",
      "Epoch 2374/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.3791 - accuracy: 0.5900 - val_loss: 0.7639 - val_accuracy: 0.3793\n",
      "Epoch 2375/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3737 - accuracy: 0.5941 - val_loss: 0.7798 - val_accuracy: 0.3753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2376/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3750 - accuracy: 0.5945 - val_loss: 0.7908 - val_accuracy: 0.3732\n",
      "Epoch 2377/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3775 - accuracy: 0.5934 - val_loss: 0.7817 - val_accuracy: 0.3762\n",
      "Epoch 2378/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3901 - accuracy: 0.5809 - val_loss: 0.7574 - val_accuracy: 0.3748\n",
      "Epoch 2379/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3846 - accuracy: 0.5862 - val_loss: 0.7782 - val_accuracy: 0.3784\n",
      "Epoch 2380/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3791 - accuracy: 0.5909 - val_loss: 0.7759 - val_accuracy: 0.3733\n",
      "Epoch 2381/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3831 - accuracy: 0.5889 - val_loss: 0.7773 - val_accuracy: 0.3826\n",
      "Epoch 2382/2500\n",
      "20056/20056 [==============================] - 1s 66us/step - loss: 0.3775 - accuracy: 0.5908 - val_loss: 0.7690 - val_accuracy: 0.3835\n",
      "Epoch 2383/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.3758 - accuracy: 0.5952 - val_loss: 0.7702 - val_accuracy: 0.3802\n",
      "Epoch 2384/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3850 - accuracy: 0.5844 - val_loss: 0.7641 - val_accuracy: 0.3800\n",
      "Epoch 2385/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.3876 - accuracy: 0.5850 - val_loss: 0.7545 - val_accuracy: 0.3721\n",
      "Epoch 2386/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3994 - accuracy: 0.5750 - val_loss: 0.7621 - val_accuracy: 0.3760\n",
      "Epoch 2387/2500\n",
      "20056/20056 [==============================] - 1s 68us/step - loss: 0.3885 - accuracy: 0.5832 - val_loss: 0.7748 - val_accuracy: 0.3772\n",
      "Epoch 2388/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.3774 - accuracy: 0.5915 - val_loss: 0.7744 - val_accuracy: 0.3812\n",
      "Epoch 2389/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3753 - accuracy: 0.5908 - val_loss: 0.7745 - val_accuracy: 0.3789\n",
      "Epoch 2390/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3975 - accuracy: 0.5753 - val_loss: 0.7717 - val_accuracy: 0.3789\n",
      "Epoch 2391/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3910 - accuracy: 0.5801 - val_loss: 0.7716 - val_accuracy: 0.3732\n",
      "Epoch 2392/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3839 - accuracy: 0.5894 - val_loss: 0.7740 - val_accuracy: 0.3790\n",
      "Epoch 2393/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3808 - accuracy: 0.5901 - val_loss: 0.7851 - val_accuracy: 0.3772\n",
      "Epoch 2394/2500\n",
      "20056/20056 [==============================] - 1s 53us/step - loss: 0.4671 - accuracy: 0.5210 - val_loss: 0.7212 - val_accuracy: 0.3602\n",
      "Epoch 2395/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.4305 - accuracy: 0.5429 - val_loss: 0.7482 - val_accuracy: 0.3784\n",
      "Epoch 2396/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3997 - accuracy: 0.5715 - val_loss: 0.7601 - val_accuracy: 0.3771\n",
      "Epoch 2397/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3898 - accuracy: 0.5818 - val_loss: 0.7636 - val_accuracy: 0.3775\n",
      "Epoch 2398/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3812 - accuracy: 0.5906 - val_loss: 0.7854 - val_accuracy: 0.3774\n",
      "Epoch 2399/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3819 - accuracy: 0.5888 - val_loss: 0.7855 - val_accuracy: 0.3756\n",
      "Epoch 2400/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3772 - accuracy: 0.5941 - val_loss: 0.7575 - val_accuracy: 0.3830\n",
      "Epoch 2401/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3781 - accuracy: 0.5934 - val_loss: 0.7808 - val_accuracy: 0.3775\n",
      "Epoch 2402/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3912 - accuracy: 0.5824 - val_loss: 0.7761 - val_accuracy: 0.3796\n",
      "Epoch 2403/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3769 - accuracy: 0.5932 - val_loss: 0.7820 - val_accuracy: 0.3787\n",
      "Epoch 2404/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.3760 - accuracy: 0.5956 - val_loss: 0.7812 - val_accuracy: 0.3800\n",
      "Epoch 2405/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3740 - accuracy: 0.5986 - val_loss: 0.7819 - val_accuracy: 0.3765\n",
      "Epoch 2406/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3787 - accuracy: 0.5924 - val_loss: 0.7711 - val_accuracy: 0.3682\n",
      "Epoch 2407/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3734 - accuracy: 0.5977 - val_loss: 0.7780 - val_accuracy: 0.3786\n",
      "Epoch 2408/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3798 - accuracy: 0.5950 - val_loss: 0.7871 - val_accuracy: 0.3786\n",
      "Epoch 2409/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3798 - accuracy: 0.5935 - val_loss: 0.7731 - val_accuracy: 0.3768\n",
      "Epoch 2410/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3776 - accuracy: 0.5952 - val_loss: 0.7680 - val_accuracy: 0.3765\n",
      "Epoch 2411/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3790 - accuracy: 0.5894 - val_loss: 0.7755 - val_accuracy: 0.3815\n",
      "Epoch 2412/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3783 - accuracy: 0.5913 - val_loss: 0.7804 - val_accuracy: 0.3784\n",
      "Epoch 2413/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.4118 - accuracy: 0.5675 - val_loss: 0.7626 - val_accuracy: 0.3720\n",
      "Epoch 2414/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3775 - accuracy: 0.5921 - val_loss: 0.7817 - val_accuracy: 0.3736\n",
      "Epoch 2415/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3742 - accuracy: 0.6000 - val_loss: 0.7744 - val_accuracy: 0.3723\n",
      "Epoch 2416/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3701 - accuracy: 0.6032 - val_loss: 0.7876 - val_accuracy: 0.3742\n",
      "Epoch 2417/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3801 - accuracy: 0.5918 - val_loss: 0.7802 - val_accuracy: 0.3718\n",
      "Epoch 2418/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3801 - accuracy: 0.5890 - val_loss: 0.7760 - val_accuracy: 0.3814\n",
      "Epoch 2419/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.3769 - accuracy: 0.5915 - val_loss: 0.7802 - val_accuracy: 0.3765\n",
      "Epoch 2420/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3809 - accuracy: 0.5898 - val_loss: 0.7970 - val_accuracy: 0.3732\n",
      "Epoch 2421/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.3900 - accuracy: 0.5849 - val_loss: 0.7749 - val_accuracy: 0.3789\n",
      "Epoch 2422/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.4086 - accuracy: 0.5656 - val_loss: 0.7781 - val_accuracy: 0.3708\n",
      "Epoch 2423/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.3802 - accuracy: 0.5890 - val_loss: 0.7903 - val_accuracy: 0.3720\n",
      "Epoch 2424/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3842 - accuracy: 0.5832 - val_loss: 0.7869 - val_accuracy: 0.3756\n",
      "Epoch 2425/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3848 - accuracy: 0.5805 - val_loss: 0.7836 - val_accuracy: 0.3777\n",
      "Epoch 2426/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3788 - accuracy: 0.5897 - val_loss: 0.7933 - val_accuracy: 0.3786\n",
      "Epoch 2427/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3972 - accuracy: 0.5786 - val_loss: 0.7851 - val_accuracy: 0.3771\n",
      "Epoch 2428/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3788 - accuracy: 0.5914 - val_loss: 0.7994 - val_accuracy: 0.3741\n",
      "Epoch 2429/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3868 - accuracy: 0.5836 - val_loss: 0.7847 - val_accuracy: 0.3772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2430/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3825 - accuracy: 0.5873 - val_loss: 0.7825 - val_accuracy: 0.3751\n",
      "Epoch 2431/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3770 - accuracy: 0.5947 - val_loss: 0.7783 - val_accuracy: 0.3753\n",
      "Epoch 2432/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.3728 - accuracy: 0.5971 - val_loss: 0.7874 - val_accuracy: 0.3747\n",
      "Epoch 2433/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3733 - accuracy: 0.5954 - val_loss: 0.7755 - val_accuracy: 0.3769\n",
      "Epoch 2434/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3722 - accuracy: 0.5986 - val_loss: 0.7785 - val_accuracy: 0.3754\n",
      "Epoch 2435/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3701 - accuracy: 0.6011 - val_loss: 0.7833 - val_accuracy: 0.3741\n",
      "Epoch 2436/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3738 - accuracy: 0.5958 - val_loss: 0.7811 - val_accuracy: 0.3780\n",
      "Epoch 2437/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3765 - accuracy: 0.5905 - val_loss: 0.7814 - val_accuracy: 0.3715\n",
      "Epoch 2438/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3767 - accuracy: 0.5925 - val_loss: 0.7783 - val_accuracy: 0.3739\n",
      "Epoch 2439/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3722 - accuracy: 0.5980 - val_loss: 0.7835 - val_accuracy: 0.3803\n",
      "Epoch 2440/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3760 - accuracy: 0.5954 - val_loss: 0.7794 - val_accuracy: 0.3766\n",
      "Epoch 2441/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3758 - accuracy: 0.5942 - val_loss: 0.7830 - val_accuracy: 0.3768\n",
      "Epoch 2442/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3774 - accuracy: 0.5942 - val_loss: 0.7766 - val_accuracy: 0.3799\n",
      "Epoch 2443/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3776 - accuracy: 0.5921 - val_loss: 0.7729 - val_accuracy: 0.3763\n",
      "Epoch 2444/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3841 - accuracy: 0.5855 - val_loss: 0.7861 - val_accuracy: 0.3775\n",
      "Epoch 2445/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3735 - accuracy: 0.5964 - val_loss: 0.7906 - val_accuracy: 0.3780\n",
      "Epoch 2446/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3753 - accuracy: 0.5954 - val_loss: 0.7788 - val_accuracy: 0.3709\n",
      "Epoch 2447/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3707 - accuracy: 0.6012 - val_loss: 0.7842 - val_accuracy: 0.3772\n",
      "Epoch 2448/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3930 - accuracy: 0.5822 - val_loss: 0.7839 - val_accuracy: 0.3756\n",
      "Epoch 2449/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3850 - accuracy: 0.5876 - val_loss: 0.7505 - val_accuracy: 0.3760\n",
      "Epoch 2450/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3783 - accuracy: 0.5937 - val_loss: 0.7764 - val_accuracy: 0.3771\n",
      "Epoch 2451/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3747 - accuracy: 0.5957 - val_loss: 0.7788 - val_accuracy: 0.3772\n",
      "Epoch 2452/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3846 - accuracy: 0.5872 - val_loss: 0.7941 - val_accuracy: 0.3730\n",
      "Epoch 2453/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3841 - accuracy: 0.5904 - val_loss: 0.7784 - val_accuracy: 0.3799\n",
      "Epoch 2454/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3760 - accuracy: 0.5932 - val_loss: 0.7716 - val_accuracy: 0.3760\n",
      "Epoch 2455/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3819 - accuracy: 0.5885 - val_loss: 0.7749 - val_accuracy: 0.3744\n",
      "Epoch 2456/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3814 - accuracy: 0.5892 - val_loss: 0.7846 - val_accuracy: 0.3750\n",
      "Epoch 2457/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3757 - accuracy: 0.5924 - val_loss: 0.7928 - val_accuracy: 0.3765\n",
      "Epoch 2458/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3738 - accuracy: 0.5984 - val_loss: 0.7725 - val_accuracy: 0.3766\n",
      "Epoch 2459/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3677 - accuracy: 0.5999 - val_loss: 0.7798 - val_accuracy: 0.3762\n",
      "Epoch 2460/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3721 - accuracy: 0.5994 - val_loss: 0.7897 - val_accuracy: 0.3792\n",
      "Epoch 2461/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3840 - accuracy: 0.5908 - val_loss: 0.7867 - val_accuracy: 0.3789\n",
      "Epoch 2462/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3754 - accuracy: 0.5925 - val_loss: 0.7831 - val_accuracy: 0.3778\n",
      "Epoch 2463/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3759 - accuracy: 0.5937 - val_loss: 0.7800 - val_accuracy: 0.3751\n",
      "Epoch 2464/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.3733 - accuracy: 0.5969 - val_loss: 0.7779 - val_accuracy: 0.3756\n",
      "Epoch 2465/2500\n",
      "20056/20056 [==============================] - 2s 81us/step - loss: 0.3755 - accuracy: 0.5952 - val_loss: 0.7671 - val_accuracy: 0.3780\n",
      "Epoch 2466/2500\n",
      "20056/20056 [==============================] - 2s 78us/step - loss: 0.3818 - accuracy: 0.5850 - val_loss: 0.7818 - val_accuracy: 0.3748\n",
      "Epoch 2467/2500\n",
      "20056/20056 [==============================] - 1s 72us/step - loss: 0.3789 - accuracy: 0.5934 - val_loss: 0.7808 - val_accuracy: 0.3726\n",
      "Epoch 2468/2500\n",
      "20056/20056 [==============================] - 1s 69us/step - loss: 0.3855 - accuracy: 0.5833 - val_loss: 0.7694 - val_accuracy: 0.3745\n",
      "Epoch 2469/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.3921 - accuracy: 0.5800 - val_loss: 0.7955 - val_accuracy: 0.3762\n",
      "Epoch 2470/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.3860 - accuracy: 0.5889 - val_loss: 0.7740 - val_accuracy: 0.3783\n",
      "Epoch 2471/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3755 - accuracy: 0.5960 - val_loss: 0.7726 - val_accuracy: 0.3760\n",
      "Epoch 2472/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3735 - accuracy: 0.5954 - val_loss: 0.7892 - val_accuracy: 0.3774\n",
      "Epoch 2473/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3851 - accuracy: 0.5859 - val_loss: 0.7708 - val_accuracy: 0.3697\n",
      "Epoch 2474/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3711 - accuracy: 0.5941 - val_loss: 0.7893 - val_accuracy: 0.3772\n",
      "Epoch 2475/2500\n",
      "20056/20056 [==============================] - 1s 55us/step - loss: 0.3789 - accuracy: 0.5904 - val_loss: 0.7756 - val_accuracy: 0.3756\n",
      "Epoch 2476/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3799 - accuracy: 0.5924 - val_loss: 0.7798 - val_accuracy: 0.3762\n",
      "Epoch 2477/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3751 - accuracy: 0.5928 - val_loss: 0.7909 - val_accuracy: 0.3786\n",
      "Epoch 2478/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.3806 - accuracy: 0.5898 - val_loss: 0.7775 - val_accuracy: 0.3763\n",
      "Epoch 2479/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.3716 - accuracy: 0.5943 - val_loss: 0.7888 - val_accuracy: 0.3748\n",
      "Epoch 2480/2500\n",
      "20056/20056 [==============================] - 1s 71us/step - loss: 0.3697 - accuracy: 0.5979 - val_loss: 0.7988 - val_accuracy: 0.3769\n",
      "Epoch 2481/2500\n",
      "20056/20056 [==============================] - 1s 62us/step - loss: 0.3863 - accuracy: 0.5877 - val_loss: 0.7630 - val_accuracy: 0.3771\n",
      "Epoch 2482/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.3724 - accuracy: 0.5961 - val_loss: 0.7871 - val_accuracy: 0.3787\n",
      "Epoch 2483/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.3686 - accuracy: 0.6043 - val_loss: 0.7843 - val_accuracy: 0.3815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2484/2500\n",
      "20056/20056 [==============================] - 1s 73us/step - loss: 0.3710 - accuracy: 0.5976 - val_loss: 0.7931 - val_accuracy: 0.3769\n",
      "Epoch 2485/2500\n",
      "20056/20056 [==============================] - 1s 59us/step - loss: 0.3832 - accuracy: 0.5899 - val_loss: 0.7548 - val_accuracy: 0.3759\n",
      "Epoch 2486/2500\n",
      "20056/20056 [==============================] - 1s 61us/step - loss: 0.3896 - accuracy: 0.5874 - val_loss: 0.7916 - val_accuracy: 0.3792\n",
      "Epoch 2487/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3692 - accuracy: 0.6008 - val_loss: 0.7779 - val_accuracy: 0.3763\n",
      "Epoch 2488/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.3758 - accuracy: 0.5968 - val_loss: 0.7809 - val_accuracy: 0.3744\n",
      "Epoch 2489/2500\n",
      "20056/20056 [==============================] - 1s 67us/step - loss: 0.3843 - accuracy: 0.5926 - val_loss: 0.7674 - val_accuracy: 0.3793\n",
      "Epoch 2490/2500\n",
      "20056/20056 [==============================] - 1s 63us/step - loss: 0.3824 - accuracy: 0.5925 - val_loss: 0.7833 - val_accuracy: 0.3717\n",
      "Epoch 2491/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3723 - accuracy: 0.6009 - val_loss: 0.7738 - val_accuracy: 0.3724\n",
      "Epoch 2492/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3710 - accuracy: 0.5994 - val_loss: 0.7707 - val_accuracy: 0.3775\n",
      "Epoch 2493/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3884 - accuracy: 0.5877 - val_loss: 0.7440 - val_accuracy: 0.3757\n",
      "Epoch 2494/2500\n",
      "20056/20056 [==============================] - 1s 58us/step - loss: 0.3786 - accuracy: 0.5901 - val_loss: 0.7780 - val_accuracy: 0.3780\n",
      "Epoch 2495/2500\n",
      "20056/20056 [==============================] - 1s 74us/step - loss: 0.3935 - accuracy: 0.5807 - val_loss: 0.7741 - val_accuracy: 0.3753\n",
      "Epoch 2496/2500\n",
      "20056/20056 [==============================] - 1s 56us/step - loss: 0.3832 - accuracy: 0.5852 - val_loss: 0.7652 - val_accuracy: 0.3763\n",
      "Epoch 2497/2500\n",
      "20056/20056 [==============================] - 1s 54us/step - loss: 0.3938 - accuracy: 0.5795 - val_loss: 0.7642 - val_accuracy: 0.3771\n",
      "Epoch 2498/2500\n",
      "20056/20056 [==============================] - 1s 57us/step - loss: 0.3810 - accuracy: 0.5910 - val_loss: 0.7944 - val_accuracy: 0.3794\n",
      "Epoch 2499/2500\n",
      "20056/20056 [==============================] - 1s 65us/step - loss: 0.3736 - accuracy: 0.5965 - val_loss: 0.7700 - val_accuracy: 0.3789\n",
      "Epoch 2500/2500\n",
      "20056/20056 [==============================] - 1s 60us/step - loss: 0.3776 - accuracy: 0.5958 - val_loss: 0.7874 - val_accuracy: 0.3805\n"
     ]
    }
   ],
   "source": [
    "res = model.fit(X_train, y_train, epochs=2500, batch_size=128, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_training_results(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss and accuracy may show signs of overfitting but the validation accuracy appears to have reached its limit. More epochs or more layers/nodes will most likely not solve the issue. \n",
    "\n",
    "A Neural Network most likely will not beat the results from other classification models such as XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
